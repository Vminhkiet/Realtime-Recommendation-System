version: '3.9'

# x-airflow-common: &airflow-common
#   image: airflow-base:2.9.0-custom
#   build:
#     context: .
#     dockerfile: ./infra/airflow/Dockerfile
#   environment: &airflow-common-env
#     AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#     AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
#     AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-db/airflow
#     AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0

#     # ðŸ” FERNET KEY (Láº¤Y Tá»ª .env)
#     AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}

#     AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
#     AIRFLOW__CORE__LOAD_EXAMPLES: "false"
#     AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

#     TZ: Asia/Ho_Chi_Minh
#     AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Ho_Chi_Minh

#   volumes:
#     - ./dags:/opt/airflow/dags
#     - ./logs:/opt/airflow/logs
#     - ./plugins:/opt/airflow/plugins
#     - ./src:/opt/airflow/src
#     - ./data:/opt/airflow/data
#     - /var/run/docker.sock:/var/run/docker.sock

#   networks:
#     - spark-kafka-net

services:
  # =========================================
  # 1. Háº  Táº¦NG MESSAGING & GOVERNANCE
  # =========================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - spark-kafka-net

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - spark-kafka-net

  schema-registry:
    image: confluentinc/cp-schema-registry:7.3.0
    container_name: schema-registry
    depends_on:
      - kafka
    ports:
      - "9081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    networks:
      - spark-kafka-net

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8090:8080"
    depends_on:
      - kafka
      - schema-registry
    environment:
      KAFKA_CLUSTERS_0_NAME: local-kafka
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
    networks:
      - spark-kafka-net

  # =========================================
  # 2. Háº  Táº¦NG INTEGRATION (KAFKA CONNECT - CUSTOM BUILD)
  # =========================================
  kafka-connect:
    build:
      context: ./infra/kafka-connector
      dockerfile: Dockerfile
    container_name: kafka-connect
    depends_on:
      - kafka
      - schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_GROUP_ID: compose-connect-group

      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status

      # ðŸ”¥ Báº®T BUá»˜C THÃŠM 3 DÃ’NG NÃ€Y
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1

      # Avro converter
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"

      CONNECT_REST_PORT: 8083

      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    networks:
      - spark-kafka-net

  # =========================================
  # 3. Háº  Táº¦NG LÆ¯U TRá»® (STORAGE LAYER)
  # =========================================
  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - ./data/redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - spark-kafka-net

  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./data/lake:/data
    networks:
      - spark-kafka-net

  mongo:
    image: mongo:4.4.6
    container_name: mongo
    ports:
      - "27017:27017"
    volumes:
      - ./data/mongo_data:/data/db
    networks:
      - spark-kafka-net

  timescaledb:
    image: timescale/timescaledb:latest-pg14
    container_name: timescaledb
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=ecommerce_logs
    volumes:
      - ./data/timescale_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/01_init.sql
    networks:
      - spark-kafka-net

  mlflow-db:
    image: postgres:14
    container_name: mlflow-db
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
      POSTGRES_DB: mlflow
    volumes:
      - ./data/mlflow_pg:/var/lib/postgresql/data
    networks:
      - spark-kafka-net

  mlflow:
    build: ./infra/mlflow
    container_name: mlflow
    depends_on:
      - mlflow-db
      - minio
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    command: >
      mlflow server
      --backend-store-uri postgresql://mlflow:mlflow@mlflow-db:5432/mlflow
      --default-artifact-root s3://mlflow
      --host 0.0.0.0
      --port 5000
    ports:
      - "5000:5000"
    networks:
      - spark-kafka-net

  # =========================================
  # 4. SPARK CLUSTER
  # =========================================
  spark-master:
    build: ./base
    container_name: spark-master
    user: root
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - ./src:/home/spark/work/src
      - ./data:/home/spark/work/data
      - ./data/ivy_cache:/root/.ivy2
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 5G
    networks:
      - spark-kafka-net

  spark-worker:
    build: ./base
    user: root
    depends_on:
      - spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_WORKER_MEMORY: 2G
      SPARK_WORKER_CORES: 1
    volumes:
      - ./src:/home/spark/work/src
      - ./data:/home/spark/work/data
    networks:
      - spark-kafka-net

  # =========================================
  # 5. SERVING LAYER
  # =========================================
  dashboard:
    build: ./base  # (Hoáº·c ./infra/spark-base tÃ¹y vÃ o thÆ° má»¥c báº¡n táº¡o)
    container_name: dashboard
    user: root
    ports:
      - "8502:8501"
    volumes:
      - ./src:/home/spark/work/src
      - ./data:/home/spark/work/data
    working_dir: /home/spark/work
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - DB_HOST=timescaledb
      - KAFKA_SERVER=kafka:29092
      - MONGO_URI=mongodb://mongo:27017/
      - TIMESCALE_URI=postgresql://postgres:password@timescaledb:5432/ecommerce_logs
    # --- Sá»¬A DÃ’NG NÃ€Y ---
    command: >
      bash -c "pip install streamlit pandas matplotlib && 
      streamlit run src/dashboard/app.py --server.port 8501 --server.address 0.0.0.0"
    networks:
      - spark-kafka-net

  tf-serving:
    image: tensorflow/serving:latest
    container_name: tf-serving
    ports:
      - "8500:8500"
      - "8501:8501"
    volumes:
      - ./data/model_registry:/models/sasrec
      - ./infra/tf-serving/batching_config.txt:/models/batching_config.txt
    environment:
      - MODEL_NAME=sasrec
    command: >
      --model_name=sasrec
      --model_base_path=/models/sasrec
      --enable_batching=true
      --batching_parameters_file=/models/batching_config.txt
    networks:
      - spark-kafka-net
  # =========================================
  # 6. AIRFLOW ORCHESTRATION
  # =========================================
  # airflow-db:
  #   image: postgres:13
  #   container_name: airflow-db
  #   environment:
  #     POSTGRES_USER: airflow
  #     POSTGRES_PASSWORD: airflow
  #     POSTGRES_DB: airflow
  #   volumes:
  #     - airflow-db-volume:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD", "pg_isready", "-U", "airflow"]
  #     interval: 10s
  #     retries: 5
  #   networks:
  #     - spark-kafka-net

  # # ================= AIRFLOW INIT =================
  # airflow-init:
  #   <<: *airflow-common
  #   container_name: airflow-init
  #   entrypoint: /bin/bash
  #   command:
  #     - -c
  #     - |
  #       airflow db migrate

  #       airflow users create \
  #         --username ${AIRFLOW_ADMIN_USER} \
  #         --password ${AIRFLOW_ADMIN_PASSWORD} \
  #         --firstname Airflow \
  #         --lastname Admin \
  #         --role Admin \
  #         --email admin@example.com
  #   depends_on:
  #     airflow-db:
  #       condition: service_healthy
  #     redis:
  #       condition: service_healthy

  # # ================= AIRFLOW CORE =================
  # airflow-webserver:
  #   <<: *airflow-common
  #   container_name: airflow-webserver
  #   command: webserver
  #   ports:
  #     - "8080:8080"
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 30s
  #     retries: 5

  # airflow-scheduler:
  #   <<: *airflow-common
  #   container_name: airflow-scheduler
  #   command: scheduler
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-worker:
  #   <<: *airflow-common
  #   container_name: airflow-worker
  #   command: celery worker
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully

  # airflow-triggerer:
  #   <<: *airflow-common
  #   container_name: airflow-triggerer
  #   command: triggerer
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully

  # # =========================================
  # # MONITORING (GRAFANA)
  # # =========================================
  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: grafana
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_USER=admin
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
  #     - GF_USERS_ALLOW_SIGN_UP=false
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   networks:
  #     - spark-kafka-net
networks:
  spark-kafka-net:
    driver: bridge

# volumes:
#   airflow-db-volume:
#   grafana_data: