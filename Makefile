# ==============================================================================
# üõ†Ô∏è C·∫§U H√åNH BI·∫æN M√îI TR∆Ø·ªúNG (CONFIGURATION)
# ==============================================================================

# S·ª≠ d·ª•ng 'docker compose' (V2) thay v√¨ 'docker-compose' (V1 c≈©) ƒë·ªÉ tr√°nh l·ªói
DOCKER_COMPOSE = docker-compose
SPARK_MASTER   = spark-master
CONNECT_HOST   = localhost
CONNECT_PORT   = 8083
PYTHON = .venv/bin/python
PIP = .venv/bin/pip
PREPARE_SCRIPT = src/ai_core/prepare_model.py
# ƒê·ªãnh nghƒ©a m√†u s·∫Øc ƒë·ªÉ log d·ªÖ nh√¨n h∆°n
GREEN  := $(shell tput -Txterm setaf 2)
YELLOW := $(shell tput -Txterm setaf 3)
RESET  := $(shell tput -Txterm sgr0)

.PHONY: help up down restart build logs ps setup process train stream sim inspect test-ai clean

# ==============================================================================
# üöÄ 1. V·∫¨N H√ÄNH H·∫† T·∫¶NG (INFRASTRUCTURE OPERATIONS)
# ==============================================================================

## Hi·ªÉn th·ªã danh s√°ch l·ªánh
help:
	@echo ''
	@echo '${YELLOW}Usage:${RESET} make ${GREEN}<target>${RESET}'
	@echo ''
	@echo 'Targets:'
	@awk '/^[a-zA-Z\-\_0-9]+:/ { \
		helpMessage = match(lastLine, /^## (.*)/); \
		if (helpMessage) { \
			helpCommand = substr($$1, 0, index($$1, ":")-1); \
			helpMessage = substr(lastLine, RSTART + 3, RLENGTH); \
			printf "  ${GREEN}%-20s${RESET} %s\n", helpCommand, helpMessage; \
		} \
	} \
	{ lastLine = $$0 }' $(MAKEFILE_LIST)

## B·∫≠t to√†n b·ªô h·ªá th·ªëng (Background mode)
up:
	@echo "${YELLOW}Starting infrastructure...${RESET}"
	$(DOCKER_COMPOSE) up -d

## T·∫Øt to√†n b·ªô h·ªá th·ªëng
down:
	@echo "${YELLOW}Stopping infrastructure...${RESET}"
	$(DOCKER_COMPOSE) -f src/serving/docker-compose.yml down
	$(DOCKER_COMPOSE) down

## Kh·ªüi ƒë·ªông l·∫°i h·ªá th·ªëng
restart: down up

## Build l·∫°i images (Ch·∫°y khi s·ª≠a Dockerfile ho·∫∑c requirements.txt)
build:
	docker build -t spark-base ./base
	docker build -t kafka-connect ./infra/kafka-connector
	docker build -t inference-service ./base/serving
	$(DOCKER_COMPOSE) up -d

## Xem danh s√°ch container ƒëang ch·∫°y
ps:
	docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Ports}}"

## Xem logs (V√≠ d·ª•: make logs s=spark-master)
logs:
	$(DOCKER_COMPOSE) logs -f $(s)

# ==============================================================================
# üß† 2. QUY TR√åNH HU·∫§N LUY·ªÜN AI (AI PIPELINE)
# ==============================================================================


## B2. X·ª≠ l√Ω d·ªØ li·ªáu (Raw JSON -> Dataset.pkl)
process_beauty:
	@echo "${YELLOW}Running Data Processing...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/processing/streaming/spark_process_beauty.py
process_game:
	@echo "${YELLOW}Running Data Processing...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/processing/streaming/spark_process_game.py
## B3. Hu·∫•n luy·ªán Model (Dataset.pkl -> Model.keras)
train:
	@echo "${YELLOW}Running Model Training...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/train.py

train_game:
	@echo "${YELLOW}Running Model Training...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/train_game.py

## B4. Test th·ª≠ Model sau khi train
test-ai:
	@echo "${YELLOW}Testing Trained Model...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/test_model.py

eval-ai:
	@echo "üìä Running Full Evaluation..."
	# C√†i tqdm cho ƒë·∫πp (n·∫øu ch∆∞a c√≥), sau ƒë√≥ ch·∫°y evaluate
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/evaluate.py

eval-game-ai:
	@echo "üìä Running Full Evaluation..."
	# C√†i tqdm cho ƒë·∫πp (n·∫øu ch∆∞a c√≥), sau ƒë√≥ ch·∫°y evaluate
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/evaluate_game.py

eval-metric:
	@echo "üìä Running Full Evaluation..."
	# C√†i tqdm cho ƒë·∫πp (n·∫øu ch∆∞a c√≥), sau ƒë√≥ ch·∫°y evaluate
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/evaluate_metrics.py


prepare-model:
	@echo "${YELLOW}Converting Keras model to SavedModel format...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 $(PREPARE_SCRIPT)
	@echo "${YELLOW}Converting Keras model to SavedModel format...${RESET}"
	@echo "${GREEN}Model prepared successfully in data/model_registry/1/${RESET}"

# L·ªánh t·ªïng h·ª£p: Chuy·ªÉn ƒë·ªïi model v√† kh·ªüi ƒë·ªông l·∫°i TF Serving
reload-tf: prepare-model
	@echo "${YELLOW}Restarting TF Serving to load new model version...${RESET}"
	docker-compose restart tf-serving
	@echo "${GREEN}TF Serving is reloading. Check logs with: docker logs -f tf-serving${RESET}"
# ==============================================================================
# ‚öôÔ∏è 3. SETUP D·ªÆ LI·ªÜU & K·∫æT N·ªêI (DATA SETUP - RUN ONCE)
# ==============================================================================

# Setup to√†n b·ªô (Metadata -> TimescaleDB -> Connectors)
setup:
	@echo "${YELLOW}--- 1. Importing Metadata to MongoDB ---${RESET}"
	# N·∫°p th√¥ng tin s·∫£n ph·∫©m (T√™n, Gi√°, ·∫¢nh) v√†o MongoDB
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo.py

# 	@echo "\n${YELLOW}--- 2. Creating TimescaleDB Hypertable ---${RESET}"
# 	# T·∫°o b·∫£ng l∆∞u log h√†nh vi ng∆∞·ªùi d√πng trong TimescaleDB
# 	docker exec -i timescaledb psql -U postgres -d ecommerce_logs -c "\
# 		CREATE TABLE IF NOT EXISTS user_activity ( \
# 			time TIMESTAMPTZ NOT NULL, \
# 			user_id TEXT, \
# 			item_id TEXT, \
# 			action_type TEXT, \
# 			device TEXT \
# 		); \
# 		SELECT create_hypertable('user_activity', 'time', if_not_exists => TRUE);" || true

# 	@echo "\n${YELLOW}--- 3. Registering Kafka Connectors ---${RESET}"
# 	# ƒêƒÉng k√Ω Connector (JSON vi·∫øt 1 d√≤ng ƒë·ªÉ tr√°nh l·ªói Makefile)
# 	@echo "Waiting for Kafka Connect to be ready..."
# 	@sleep 5
# 	@curl -s -X POST http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors \
# 		-H "Content-Type: application/json" \
# 		-d '{"name": "timescale-sink", "config": {"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", "tasks.max": "1", "topics": "user_clicks", "connection.url": "jdbc:postgresql://timescaledb:5432/ecommerce_logs", "connection.user": "postgres", "connection.password": "password", "auto.create": "true", "insert.mode": "insert"}}' || echo "Connector might already exist."
# 	@echo "\n${GREEN}Setup Completed!${RESET}"

init-timescale:
	sudo rm -rf ./data/timescale_data

setup-timescale-sink:
	@echo "‚ôªÔ∏è  G·ª° b·ªè Connector c≈© ƒë·ªÉ reset b·ªô nh·ªõ ƒë·ªám..."
	@curl -s -X DELETE http://localhost:8083/connectors/sink-timescale-interactions || true
	@sleep 2
	@echo "üöÄ ƒêang thi·∫øt l·∫≠p JDBC Sink Connector v·ªõi auto.evolve=false..."
	@curl -s -X POST -H "Content-Type: application/json" \
		--data @connectors/sink_timescale.json \
		http://localhost:8083/connectors
	@echo "\n‚úÖ ƒê√£ g·ª≠i y√™u c·∫ßu. ƒê·ª£i 5s ƒë·ªÉ Task kh·ªüi ƒë·ªông..."
	@sleep 5
	@curl -s http://localhost:8083/connectors/sink-timescale-interactions/status | jq

setup-minio-sink:
	@echo "‚ôªÔ∏è  ƒêang g·ª° b·ªè Connector c≈©..."
	# 1. X√≥a Connector c≈© (n·∫øu c√≥)
	@curl -s -X DELETE http://localhost:8083/connectors/sink-minio-processed-parquet || true
	
	@echo "‚è≥ ƒê·ª£i 3 gi√¢y..."
	@sleep 3
	
	@echo "üöÄ ƒêang deploy Connector t·ª´ file: connectors/sink_minio.json"
	# 2. T·∫°o m·ªõi v·ªõi ƒë√∫ng ƒë∆∞·ªùng d·∫´n file b·∫°n y√™u c·∫ßu
	@curl -s -X POST http://localhost:8083/connectors \
		-H "Content-Type: application/json" \
		-d @connectors/sink_minio_fake.json
	
	@echo "\n‚úÖ Setup Completed! Ki·ªÉm tra tr·∫°ng th√°i:"
	@sleep 1
	@curl -s http://localhost:8083/connectors/sink-minio-processed-parquet/status | jq

setup-mongo:
	@echo "${YELLOW}--- 1. Importing Metadata to MongoDB ---${RESET}"
	# N·∫°p th√¥ng tin s·∫£n ph·∫©m (T√™n, Gi√°, ·∫¢nh) v√†o MongoDB
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo.py
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo_meta.py

# 	@echo "\n${YELLOW}--- 2. Creating TimescaleDB Hypertable ---${RESET}"
# 	# T·∫°o b·∫£ng l∆∞u log h√†nh vi ng∆∞·ªùi d√πng trong TimescaleDB
# 	docker exec -i timescaledb psql -U postgres -d ecommerce_logs -c "\
# 		CREATE TABLE IF NOT EXISTS user_activity ( \
# 			time TIMESTAMPTZ NOT NULL, \
# 			user_id TEXT, \
# 			item_id TEXT, \
# 			action_type TEXT, \
# 			device TEXT \
# 		); \
# 		SELECT create_hypertable('user_activity', 'time', if_not_exists => TRUE);" || true

# 	@echo "\n${YELLOW}--- 3. Registering Kafka Connectors (AVRO MODE) ---${RESET}"
# 	# X√≥a connector c≈© n·∫øu c√≥ ƒë·ªÉ tr√°nh xung ƒë·ªôt
# 	@curl -s -X DELETE http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors/timescale-sink || true
# 	@curl -s -X DELETE http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors/timescale-sink-avro || true
	
# 	@echo "Waiting for Kafka Connect to be ready..."
# 	@sleep 5
# 	# [QUAN TR·ªåNG] JSON b√™n d∆∞·ªõi ƒë√£ ƒë∆∞·ª£c vi·∫øt th√†nh 1 d√≤ng ƒë·ªÉ tr√°nh l·ªói Makefile
# 	@curl -s -X POST http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors \
# 		-H "Content-Type: application/json" \
# 		-d '{"name": "timescale-sink-avro", "config": {"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", "tasks.max": "1", "topics": "user_clicks", "connection.url": "jdbc:postgresql://timescaledb:5432/ecommerce_logs", "connection.user": "postgres", "connection.password": "password", "auto.create": "true", "insert.mode": "insert", "key.converter": "org.apache.kafka.connect.storage.StringConverter", "value.converter": "io.confluent.connect.avro.AvroConverter", "value.converter.schema.registry.url": "http://schema-registry:8081"}}' || echo "Connector setup failed."
# 	@echo "\n${GREEN}Setup Completed!${RESET}"

# ==============================================================================
# üåä 4. CH·∫†Y DEMO REAL-TIME (RUNTIME)
# ==============================================================================

## Terminal 1: Ch·∫°y Simulator (Gi·∫£ l·∫≠p ng∆∞·ªùi d√πng click)
sim:
	@echo "${YELLOW}Running Simulation inside Docker...${RESET}"
	# Ch·∫°y producer v·ªõi bi·∫øn m√¥i tr∆∞·ªùng Kafka n·ªôi b·ªô
# 	docker exec -it -w /home/spark/work -e KAFKA_SERVER=kafka:29092 $(SPARK_MASTER) python3 src/simulation/main_producer.py
	docker exec -it spark-master pip install confluent-kafka fastavro requests Faker authlib
	docker exec -it -w /home/spark/work \
		-e KAFKA_BOOTSTRAP=kafka:29092 \
		-e SCHEMA_REGISTRY_URL=http://schema-registry:8081 \
		spark-master python3 src/simulation/avro_producer.py

## Terminal 2: Ch·∫°y Spark Streaming (AI Inference Real-time)
stream:
	@echo "${YELLOW}Submitting Spark Streaming Job...${RESET}"
	docker exec -it -e PYTHONPATH=/home/spark/work -w /home/spark/work spark-master spark-submit \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-avro_2.12:3.5.1 \
		/home/spark/work/src/serving/run_inference.py

streaming:
	docker exec -it spark-master spark-submit \
		--master spark://spark-master:7077 \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
		/home/spark/work/src/processing/streaming/feature_engineering.py
ETL:
	docker exec -it -w /home/spark/work spark-master bash -c "\
		pip install boto3 && \
		spark-submit \
		--packages org.apache.spark:spark-hadoop-cloud_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
		src/processing/batch_etl_minio.py"

UPLOAD:
	docker exec -it -w /home/spark/work spark-master bash -c "\
		pip install boto3 && \
		spark-submit \
		--packages org.apache.spark:spark-hadoop-cloud_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
		src/utils/upload_local_to_minio.py"

ETL-TRAIN:
	docker exec -it -w /home/spark/work spark-master bash -c "\
		pip install boto3 && \
		spark-submit \
		--packages org.apache.spark:spark-hadoop-cloud_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
		src/processing/batch/batch_etl_train.py"

auto-train:
	@echo "${YELLOW}üöÄ Starting Automated Training Pipeline...${RESET}"
	# ƒê·∫£m b·∫£o c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt trong container
	#docker exec -u 0 spark-master pip install tensorflow keras-hub s3fs pymongo pandas
	# Th·ª±c thi script hu·∫•n luy·ªán l·∫•y d·ªØ li·ªáu t·ª´ Data Lake (MinIO)
	docker exec -it -w /home/spark/work spark-master spark-submit \
        --packages org.apache.spark:spark-hadoop-cloud_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
        src/utils/debug_view.py
	@echo "${GREEN}‚úÖ Training completed. TF Serving will hot-reload the new model version.${RESET}"

up-serving:
	@echo "${YELLOW}Starting Inference Service...${RESET}"
	docker-compose -f src/serving/docker-compose.yml up -d

down-serving:
	@echo "${RED}Stopping Inference Service...${RESET}"
	docker-compose -f src/serving/docker-compose.yml down
## D·ªçn d·∫πp d·ªØ li·ªáu r√°c (C·∫®N TH·∫¨N: X√≥a s·∫°ch Database)
clean-data: down
	@echo "${YELLOW}Cleaning all data volumes...${RESET}"
	sudo rm -rf data/mongo_data/* data/timescale_data/* data/redis_data/*
	@echo "${GREEN}All data cleaned!${RESET}"