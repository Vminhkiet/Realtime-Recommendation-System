# ==============================================================================
# üõ†Ô∏è BI·∫æN M√îI TR∆Ø·ªúNG
# ==============================================================================

DOCKER_COMPOSE = docker-compose
SPARK_MASTER   = spark-master
CONNECT_HOST   = localhost
CONNECT_PORT   = 8083

# M√†u s·∫Øc
GREEN  := $(shell tput -Txterm setaf 2)
YELLOW := $(shell tput -Txterm setaf 3)
RESET  := $(shell tput -Txterm sgr0)

.PHONY: help up down setup stream sim ps

# ==============================================================================
# üöÄ L·ªÜNH V·∫¨N H√ÄNH (OPERATIONS)
# ==============================================================================

## 1. B·∫≠t h·ªá th·ªëng
up:
	@echo "$(YELLOW)Starting infrastructure...$(RESET)"
	$(DOCKER_COMPOSE) up -d

## 2. T·∫Øt h·ªá th·ªëng
down:
	@echo "$(YELLOW)Stopping infrastructure...$(RESET)"
	$(DOCKER_COMPOSE) down

## 3. Xem danh s√°ch container
ps:
	docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Ports}}"

# ==============================================================================
# ‚öôÔ∏è KH·ªûI T·∫†O (SETUP - CH·∫†Y 1 L·∫¶N)
# ==============================================================================

## 4. Setup to√†n b·ªô (Mongo + Timescale + Connect)
setup:
	@echo "$(YELLOW)--- 1. Importing Metadata to MongoDB ---$(RESET)"
	# [S·ª¨A ·ªû ƒê√ÇY] ƒê·ªïi 'python' th√†nh 'python3'
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo.py

	# @echo "\n$(YELLOW)--- 2. Creating TimescaleDB Hypertable ---$(RESET)"
	# docker exec -i timescaledb psql -U postgres -d ecommerce_logs -c "\
	# CREATE TABLE IF NOT EXISTS user_activity (\
	#     time TIMESTAMPTZ NOT NULL,\
	#     user_id TEXT,\
	#     item_id TEXT,\
	#     action_type TEXT,\
	#     device TEXT\
	# ); \
	# SELECT create_hypertable('user_activity', 'time', if_not_exists => TRUE);"

	# @echo "\n$(GREEN)--- 3. Registering Kafka Connectors ---$(RESET)"
	# @curl -s -X POST http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors \
	# -H "Content-Type: application/json" \
	# -d '{"name": "timescale-sink", "config": {"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", "tasks.max": "1", "topics": "logs_analytics", "connection.url": "jdbc:postgresql://timescaledb:5432/ecommerce_logs", "connection.user": "postgres", "connection.password": "password", "auto.create": "true", "insert.mode": "insert"}}' > /dev/null

	# @echo "$(GREEN)Setup Completed!$(RESET)"

# ==============================================================================
# üèÉ CH·∫†Y ·ª®NG D·ª§NG (RUNTIME)
# ==============================================================================

## 5. Ch·∫°y Spark Streaming (M·ªü Terminal 2)
stream:
	@echo "$(YELLOW)Submitting Spark Job...$(RESET)"
	docker exec -it $(SPARK_MASTER) spark-submit \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
		--py-files /home/spark/work/src/processing/streaming/utils.py,/home/spark/work/src/ai_core/model.py \
		/home/spark/work/src/processing/streaming/inference.py

## 6. Ch·∫°y Simulator (M·ªü Terminal 1)
sim:
	@echo "$(YELLOW)Running Simulation inside Docker...$(RESET)"
	docker exec -it $(SPARK_MASTER) python src/simulation/main_producer.py
