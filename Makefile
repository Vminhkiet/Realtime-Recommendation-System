# ==============================================================================
# üõ†Ô∏è C·∫§U H√åNH BI·∫æN M√îI TR∆Ø·ªúNG (CONFIGURATION)
# ==============================================================================

# S·ª≠ d·ª•ng 'docker compose' (V2) thay v√¨ 'docker-compose' (V1 c≈©) ƒë·ªÉ tr√°nh l·ªói
DOCKER_COMPOSE = docker-compose
SPARK_MASTER   = spark-master
CONNECT_HOST   = localhost
CONNECT_PORT   = 8083

# ƒê·ªãnh nghƒ©a m√†u s·∫Øc ƒë·ªÉ log d·ªÖ nh√¨n h∆°n
GREEN  := $(shell tput -Txterm setaf 2)
YELLOW := $(shell tput -Txterm setaf 3)
RESET  := $(shell tput -Txterm sgr0)

.PHONY: help up down restart build logs ps setup process train stream sim inspect test-ai clean

# ==============================================================================
# üöÄ 1. V·∫¨N H√ÄNH H·∫† T·∫¶NG (INFRASTRUCTURE OPERATIONS)
# ==============================================================================

## Hi·ªÉn th·ªã danh s√°ch l·ªánh
help:
	@echo ''
	@echo '${YELLOW}Usage:${RESET} make ${GREEN}<target>${RESET}'
	@echo ''
	@echo 'Targets:'
	@awk '/^[a-zA-Z\-\_0-9]+:/ { \
		helpMessage = match(lastLine, /^## (.*)/); \
		if (helpMessage) { \
			helpCommand = substr($$1, 0, index($$1, ":")-1); \
			helpMessage = substr(lastLine, RSTART + 3, RLENGTH); \
			printf "  ${GREEN}%-20s${RESET} %s\n", helpCommand, helpMessage; \
		} \
	} \
	{ lastLine = $$0 }' $(MAKEFILE_LIST)

## B·∫≠t to√†n b·ªô h·ªá th·ªëng (Background mode)
up:
	@echo "${YELLOW}Starting infrastructure...${RESET}"
	$(DOCKER_COMPOSE) up -d

## T·∫Øt to√†n b·ªô h·ªá th·ªëng
down:
	@echo "${YELLOW}Stopping infrastructure...${RESET}"
	$(DOCKER_COMPOSE) down

## Kh·ªüi ƒë·ªông l·∫°i h·ªá th·ªëng
restart: down up

## Build l·∫°i images (Ch·∫°y khi s·ª≠a Dockerfile ho·∫∑c requirements.txt)
build:
	docker build -t spark-base ./base
	docker build -t kafka-connect ./infra/kafka-connector
	$(DOCKER_COMPOSE) up -d

## Xem danh s√°ch container ƒëang ch·∫°y
ps:
	docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Ports}}"

## Xem logs (V√≠ d·ª•: make logs s=spark-master)
logs:
	$(DOCKER_COMPOSE) logs -f $(s)

# ==============================================================================
# üß† 2. QUY TR√åNH HU·∫§N LUY·ªÜN AI (AI PIPELINE)
# ==============================================================================

## B1. Ki·ªÉm tra d·ªØ li·ªáu th√¥ (Inspect Raw Data)
inspect:
	@echo "${YELLOW}Inspecting Raw Data...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/inspect_data.py

## B2. X·ª≠ l√Ω d·ªØ li·ªáu (Raw JSON -> Dataset.pkl)
process:
	@echo "${YELLOW}Running Data Processing...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/data_process.py

## B3. Hu·∫•n luy·ªán Model (Dataset.pkl -> Model.keras)
train:
	@echo "${YELLOW}Running Model Training...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/train.py

## B4. Test th·ª≠ Model sau khi train
test-ai:
	@echo "${YELLOW}Testing Trained Model...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/test_model.py

# ==============================================================================
# ‚öôÔ∏è 3. SETUP D·ªÆ LI·ªÜU & K·∫æT N·ªêI (DATA SETUP - RUN ONCE)
# ==============================================================================

## Setup to√†n b·ªô (Metadata -> TimescaleDB -> Connectors)
setup:
	@echo "${YELLOW}--- 1. Importing Metadata to MongoDB ---${RESET}"
	# N·∫°p th√¥ng tin s·∫£n ph·∫©m (T√™n, Gi√°, ·∫¢nh) v√†o MongoDB
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo.py

	@echo "\n${YELLOW}--- 2. Creating TimescaleDB Hypertable ---${RESET}"
	# T·∫°o b·∫£ng l∆∞u log h√†nh vi ng∆∞·ªùi d√πng trong TimescaleDB
	docker exec -i timescaledb psql -U postgres -d ecommerce_logs -c "\
		CREATE TABLE IF NOT EXISTS user_activity ( \
			time TIMESTAMPTZ NOT NULL, \
			user_id TEXT, \
			item_id TEXT, \
			action_type TEXT, \
			device TEXT \
		); \
		SELECT create_hypertable('user_activity', 'time', if_not_exists => TRUE);" || true

	@echo "\n${YELLOW}--- 3. Registering Kafka Connectors ---${RESET}"
	# ƒêƒÉng k√Ω Connector (JSON vi·∫øt 1 d√≤ng ƒë·ªÉ tr√°nh l·ªói Makefile)
	@echo "Waiting for Kafka Connect to be ready..."
	@sleep 5
	@curl -s -X POST http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors \
		-H "Content-Type: application/json" \
		-d '{"name": "timescale-sink", "config": {"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", "tasks.max": "1", "topics": "user_clicks", "connection.url": "jdbc:postgresql://timescaledb:5432/ecommerce_logs", "connection.user": "postgres", "connection.password": "password", "auto.create": "true", "insert.mode": "insert"}}' || echo "Connector might already exist."
	@echo "\n${GREEN}Setup Completed!${RESET}"

# ==============================================================================
# üåä 4. CH·∫†Y DEMO REAL-TIME (RUNTIME)
# ==============================================================================

## Terminal 1: Ch·∫°y Simulator (Gi·∫£ l·∫≠p ng∆∞·ªùi d√πng click)
sim:
	@echo "${YELLOW}Running Simulation inside Docker...${RESET}"
	# Ch·∫°y producer v·ªõi bi·∫øn m√¥i tr∆∞·ªùng Kafka n·ªôi b·ªô
	docker exec -it -w /home/spark/work -e KAFKA_SERVER=kafka:29092 $(SPARK_MASTER) python3 src/simulation/main_producer.py

## Terminal 2: Ch·∫°y Spark Streaming (AI Inference Real-time)
stream:
	@echo "${YELLOW}Submitting Spark Streaming Job...${RESET}"
	# Submit job Spark ƒë·ªÉ ƒë·ªçc Kafka v√† g·ªçi Model AI
	docker exec -it -w /home/spark/work $(SPARK_MASTER) spark-submit \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
		--py-files /home/spark/work/src/processing/streaming/utils.py,/home/spark/work/src/ai_core/model.py \
		/home/spark/work/src/processing/streaming/inference.py

## D·ªçn d·∫πp d·ªØ li·ªáu r√°c (C·∫®N TH·∫¨N: X√≥a s·∫°ch Database)
clean-data: down
	@echo "${YELLOW}Cleaning all data volumes...${RESET}"
	sudo rm -rf data/mongo_data/* data/timescale_data/* data/redis_data/*
	@echo "${GREEN}All data cleaned!${RESET}"