# ==============================================================================
# üõ†Ô∏è C·∫§U H√åNH BI·∫æN M√îI TR∆Ø·ªúNG (CONFIGURATION)
# ==============================================================================

# S·ª≠ d·ª•ng 'docker compose' (V2) thay v√¨ 'docker-compose' (V1 c≈©) ƒë·ªÉ tr√°nh l·ªói
DOCKER_COMPOSE = docker-compose
SPARK_MASTER   = spark-master
CONNECT_HOST   = localhost
CONNECT_PORT   = 8083

# ƒê·ªãnh nghƒ©a m√†u s·∫Øc ƒë·ªÉ log d·ªÖ nh√¨n h∆°n
GREEN  := $(shell tput -Txterm setaf 2)
YELLOW := $(shell tput -Txterm setaf 3)
RESET  := $(shell tput -Txterm sgr0)

.PHONY: help up down restart build logs ps setup process train stream sim inspect test-ai clean

# ==============================================================================
# üöÄ 1. V·∫¨N H√ÄNH H·∫† T·∫¶NG (INFRASTRUCTURE OPERATIONS)
# ==============================================================================

## Hi·ªÉn th·ªã danh s√°ch l·ªánh
help:
	@echo ''
	@echo '${YELLOW}Usage:${RESET} make ${GREEN}<target>${RESET}'
	@echo ''
	@echo 'Targets:'
	@awk '/^[a-zA-Z\-\_0-9]+:/ { \
		helpMessage = match(lastLine, /^## (.*)/); \
		if (helpMessage) { \
			helpCommand = substr($$1, 0, index($$1, ":")-1); \
			helpMessage = substr(lastLine, RSTART + 3, RLENGTH); \
			printf "  ${GREEN}%-20s${RESET} %s\n", helpCommand, helpMessage; \
		} \
	} \
	{ lastLine = $$0 }' $(MAKEFILE_LIST)

## B·∫≠t to√†n b·ªô h·ªá th·ªëng (Background mode)
up:
	@echo "${YELLOW}Starting infrastructure...${RESET}"
	$(DOCKER_COMPOSE) up -d

## T·∫Øt to√†n b·ªô h·ªá th·ªëng
down:
	@echo "${YELLOW}Stopping infrastructure...${RESET}"
	$(DOCKER_COMPOSE) down

## Kh·ªüi ƒë·ªông l·∫°i h·ªá th·ªëng
restart: down up

## Build l·∫°i images (Ch·∫°y khi s·ª≠a Dockerfile ho·∫∑c requirements.txt)
build:
	docker build -t spark-base ./base
	docker build -t kafka-connect ./infra/kafka-connector
	$(DOCKER_COMPOSE) up -d

## Xem danh s√°ch container ƒëang ch·∫°y
ps:
	docker ps --format "table {{.ID}}\t{{.Names}}\t{{.Status}}\t{{.Ports}}"

## Xem logs (V√≠ d·ª•: make logs s=spark-master)
logs:
	$(DOCKER_COMPOSE) logs -f $(s)

# ==============================================================================
# üß† 2. QUY TR√åNH HU·∫§N LUY·ªÜN AI (AI PIPELINE)
# ==============================================================================


## B2. X·ª≠ l√Ω d·ªØ li·ªáu (Raw JSON -> Dataset.pkl)
process_beauty:
	@echo "${YELLOW}Running Data Processing...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/processing/streaming/spark_process_beauty.py
process_game:
	@echo "${YELLOW}Running Data Processing...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/processing/streaming/spark_process_game.py
## B3. Hu·∫•n luy·ªán Model (Dataset.pkl -> Model.keras)
train:
	@echo "${YELLOW}Running Model Training...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/train.py

train_game:
	@echo "${YELLOW}Running Model Training...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/train_game.py

## B4. Test th·ª≠ Model sau khi train
test-ai:
	@echo "${YELLOW}Testing Trained Model...${RESET}"
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/test_model.py

eval-ai:
	@echo "üìä Running Full Evaluation..."
	# C√†i tqdm cho ƒë·∫πp (n·∫øu ch∆∞a c√≥), sau ƒë√≥ ch·∫°y evaluate
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/evaluate.py

eval-game-ai:
	@echo "üìä Running Full Evaluation..."
	# C√†i tqdm cho ƒë·∫πp (n·∫øu ch∆∞a c√≥), sau ƒë√≥ ch·∫°y evaluate
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/evaluate_game.py

eval-metric:
	@echo "üìä Running Full Evaluation..."
	# C√†i tqdm cho ƒë·∫πp (n·∫øu ch∆∞a c√≥), sau ƒë√≥ ch·∫°y evaluate
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/ai_core/evaluate_metrics.py
# ==============================================================================
# ‚öôÔ∏è 3. SETUP D·ªÆ LI·ªÜU & K·∫æT N·ªêI (DATA SETUP - RUN ONCE)
# ==============================================================================

## Setup to√†n b·ªô (Metadata -> TimescaleDB -> Connectors)
# setup:
# 	@echo "${YELLOW}--- 1. Importing Metadata to MongoDB ---${RESET}"
# 	# N·∫°p th√¥ng tin s·∫£n ph·∫©m (T√™n, Gi√°, ·∫¢nh) v√†o MongoDB
# 	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo.py

# 	@echo "\n${YELLOW}--- 2. Creating TimescaleDB Hypertable ---${RESET}"
# 	# T·∫°o b·∫£ng l∆∞u log h√†nh vi ng∆∞·ªùi d√πng trong TimescaleDB
# 	docker exec -i timescaledb psql -U postgres -d ecommerce_logs -c "\
# 		CREATE TABLE IF NOT EXISTS user_activity ( \
# 			time TIMESTAMPTZ NOT NULL, \
# 			user_id TEXT, \
# 			item_id TEXT, \
# 			action_type TEXT, \
# 			device TEXT \
# 		); \
# 		SELECT create_hypertable('user_activity', 'time', if_not_exists => TRUE);" || true

# 	@echo "\n${YELLOW}--- 3. Registering Kafka Connectors ---${RESET}"
# 	# ƒêƒÉng k√Ω Connector (JSON vi·∫øt 1 d√≤ng ƒë·ªÉ tr√°nh l·ªói Makefile)
# 	@echo "Waiting for Kafka Connect to be ready..."
# 	@sleep 5
# 	@curl -s -X POST http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors \
# 		-H "Content-Type: application/json" \
# 		-d '{"name": "timescale-sink", "config": {"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", "tasks.max": "1", "topics": "user_clicks", "connection.url": "jdbc:postgresql://timescaledb:5432/ecommerce_logs", "connection.user": "postgres", "connection.password": "password", "auto.create": "true", "insert.mode": "insert"}}' || echo "Connector might already exist."
# 	@echo "\n${GREEN}Setup Completed!${RESET}"

setup-minio-sink:
	@echo "‚ôªÔ∏è  ƒêang g·ª° b·ªè Connector c≈©..."
	# 1. X√≥a Connector c≈© (n·∫øu c√≥)
	@curl -s -X DELETE http://localhost:8083/connectors/sink-minio-processed-parquet || true
	
	@echo "‚è≥ ƒê·ª£i 3 gi√¢y..."
	@sleep 3
	
	@echo "üöÄ ƒêang deploy Connector t·ª´ file: connectors/sink_minio.json"
	# 2. T·∫°o m·ªõi v·ªõi ƒë√∫ng ƒë∆∞·ªùng d·∫´n file b·∫°n y√™u c·∫ßu
	@curl -s -X POST http://localhost:8083/connectors \
		-H "Content-Type: application/json" \
		-d @connectors/sink_minio.json
	
	@echo "\n‚úÖ Setup Completed! Ki·ªÉm tra tr·∫°ng th√°i:"
	@sleep 1
	@curl -s http://localhost:8083/connectors/sink-minio-processed-parquet/status | jq

setup:
	@echo "${YELLOW}--- 1. Importing Metadata to MongoDB ---${RESET}"
	# N·∫°p th√¥ng tin s·∫£n ph·∫©m (T√™n, Gi√°, ·∫¢nh) v√†o MongoDB
	#docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo.py
	docker exec -it -w /home/spark/work $(SPARK_MASTER) python3 src/utils/init_mongo_meta.py

	@echo "\n${YELLOW}--- 2. Creating TimescaleDB Hypertable ---${RESET}"
	# T·∫°o b·∫£ng l∆∞u log h√†nh vi ng∆∞·ªùi d√πng trong TimescaleDB
	docker exec -i timescaledb psql -U postgres -d ecommerce_logs -c "\
		CREATE TABLE IF NOT EXISTS user_activity ( \
			time TIMESTAMPTZ NOT NULL, \
			user_id TEXT, \
			item_id TEXT, \
			action_type TEXT, \
			device TEXT \
		); \
		SELECT create_hypertable('user_activity', 'time', if_not_exists => TRUE);" || true

	@echo "\n${YELLOW}--- 3. Registering Kafka Connectors (AVRO MODE) ---${RESET}"
	# X√≥a connector c≈© n·∫øu c√≥ ƒë·ªÉ tr√°nh xung ƒë·ªôt
	@curl -s -X DELETE http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors/timescale-sink || true
	@curl -s -X DELETE http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors/timescale-sink-avro || true
	
	@echo "Waiting for Kafka Connect to be ready..."
	@sleep 5
	# [QUAN TR·ªåNG] JSON b√™n d∆∞·ªõi ƒë√£ ƒë∆∞·ª£c vi·∫øt th√†nh 1 d√≤ng ƒë·ªÉ tr√°nh l·ªói Makefile
	@curl -s -X POST http://$(CONNECT_HOST):$(CONNECT_PORT)/connectors \
		-H "Content-Type: application/json" \
		-d '{"name": "timescale-sink-avro", "config": {"connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector", "tasks.max": "1", "topics": "user_clicks", "connection.url": "jdbc:postgresql://timescaledb:5432/ecommerce_logs", "connection.user": "postgres", "connection.password": "password", "auto.create": "true", "insert.mode": "insert", "key.converter": "org.apache.kafka.connect.storage.StringConverter", "value.converter": "io.confluent.connect.avro.AvroConverter", "value.converter.schema.registry.url": "http://schema-registry:8081"}}' || echo "Connector setup failed."
	@echo "\n${GREEN}Setup Completed!${RESET}"

# ==============================================================================
# üåä 4. CH·∫†Y DEMO REAL-TIME (RUNTIME)
# ==============================================================================

## Terminal 1: Ch·∫°y Simulator (Gi·∫£ l·∫≠p ng∆∞·ªùi d√πng click)
sim:
	@echo "${YELLOW}Running Simulation inside Docker...${RESET}"
	# Ch·∫°y producer v·ªõi bi·∫øn m√¥i tr∆∞·ªùng Kafka n·ªôi b·ªô
# 	docker exec -it -w /home/spark/work -e KAFKA_SERVER=kafka:29092 $(SPARK_MASTER) python3 src/simulation/main_producer.py
	docker exec -it spark-master pip install confluent-kafka fastavro requests Faker authlib
	docker exec -it -w /home/spark/work \
		-e KAFKA_BOOTSTRAP=kafka:29092 \
		-e SCHEMA_REGISTRY_URL=http://schema-registry:8081 \
		spark-master python3 src/simulation/avro_producer.py

## Terminal 2: Ch·∫°y Spark Streaming (AI Inference Real-time)
stream:
	@echo "${YELLOW}Submitting Spark Streaming Job...${RESET}"
	docker exec -it -e PYTHONPATH=/home/spark/work -w /home/spark/work spark-master spark-submit \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-avro_2.12:3.5.1 \
		/home/spark/work/src/serving/run_inference.py

streaming:
	docker exec -it spark-master spark-submit \
		--master spark://spark-master:7077 \
		--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
		/home/spark/work/src/processing/streaming/feature_engineering.py
ETL:
	docker exec -it -w /home/spark/work spark-master bash -c "\
		pip install boto3 && \
		spark-submit \
		--packages org.apache.spark:spark-hadoop-cloud_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
		src/processing/batch_etl_minio.py"
## D·ªçn d·∫πp d·ªØ li·ªáu r√°c (C·∫®N TH·∫¨N: X√≥a s·∫°ch Database)
clean-data: down
	@echo "${YELLOW}Cleaning all data volumes...${RESET}"
	sudo rm -rf data/mongo_data/* data/timescale_data/* data/redis_data/*
	@echo "${GREEN}All data cleaned!${RESET}"