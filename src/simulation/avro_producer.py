# import time
# import json
# import random
# import os
# import uuid
# from datetime import datetime

# # ThÆ° viá»‡n Confluent Kafka há»— trá»£ Avro
# from confluent_kafka import SerializingProducer
# from confluent_kafka.serialization import StringSerializer
# from confluent_kafka.schema_registry import SchemaRegistryClient
# from confluent_kafka.schema_registry.avro import AvroSerializer

# # ==========================================
# # Cáº¤U HÃŒNH (CONFIG)
# # ==========================================
# KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP', 'localhost:9092')
# SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:9081')
# TOPIC = 'raw_clicks_avro'  # TÃªn topic cho data Avro

# # Path load dá»¯ liá»‡u giáº£ láº­p
# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))
# VALID_USERS_PATH = os.path.join(PROJECT_ROOT, 'src/simulation/users.json')
# ITEM_MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')

# # ==========================================
# # Äá»ŠNH NGHÄ¨A AVRO SCHEMA
# # ==========================================
# KEY_SCHEMA_STR = """
# {
#   "type": "string"
# }
# """

# VALUE_SCHEMA_STR = """
# {
#   "namespace": "ecommerce.tracking",
#   "type": "record",
#   "name": "UserClick",
#   "fields": [
#     {"name": "event_id", "type": "string"},
#     {"name": "session_id", "type": "string"},
#     {"name": "user_id", "type": "string"},
#     {"name": "item_id", "type": "string"},
#     {"name": "event_type", "type": "string"},
#     {"name": "rating_original", "type": "float"},
#     {"name": "timestamp", "type": "string"},
#     {"name": "context", "type": {
#         "type": "record",
#         "name": "ContextData",
#         "fields": [
#             {"name": "device", "type": "string"},
#             {"name": "location", "type": "string"},
#             {"name": "ip", "type": "string"}
#         ]
#     }}
#   ]
# }
# """

# # ==========================================
# # LOAD DATA HELPER
# # ==========================================
# def load_valid_data():
#     print(f"ğŸ“‚ Äang Ä‘á»c User tá»«: {VALID_USERS_PATH}")
    
#     if not os.path.exists(VALID_USERS_PATH):
#         print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y valid_users.json.")
#         exit()

#     with open(VALID_USERS_PATH, 'r') as f:
#         users = json.load(f)

#     with open(ITEM_MAP_PATH, 'r') as f:
#         item_map = json.load(f)
#         # item_map format: {"ItemID_Str": Index_Int} -> Láº¥y keys lÃ  ItemID
#         items = list(item_map.values())

#     print(f"âœ… ÄÃ£ load: {len(users)} Users vÃ  {len(items)} Items.")
#     return users, items

# # ==========================================
# # CALLBACK FUNCTION
# # ==========================================
# def delivery_report(err, msg):
#     """HÃ m nÃ y Ä‘Æ°á»£c gá»i khi Kafka xÃ¡c nháº­n Ä‘Ã£ nháº­n tin nháº¯n"""
#     if err is not None:
#         print(f"âŒ Delivery failed for User record {msg.key()}: {err}")
#     # else:
#     #     print(f"âœ… Record {msg.key()} produced to {msg.topic()} [{msg.partition()}]")

# # ==========================================
# # MAIN LOOP
# # ==========================================
# def main():
#     # 1. Load Data
#     valid_users, valid_items = load_valid_data()
    
#     # 2. Setup Schema Registry Client
#     schema_registry_conf = {'url': SCHEMA_REGISTRY_URL}
#     schema_registry_client = SchemaRegistryClient(schema_registry_conf)

#     # 3. Setup Avro Serializer
#     avro_serializer = AvroSerializer(
#         schema_registry_client,
#         VALUE_SCHEMA_STR
#     )

#     # 4. Setup Producer Configuration
#     producer_conf = {
#         'bootstrap.servers': KAFKA_BOOTSTRAP,
#         'key.serializer': StringSerializer('utf_8'),
#         'value.serializer': avro_serializer
#     }

#     producer = SerializingProducer(producer_conf)

#     print(f"ğŸš€ Báº¯t Ä‘áº§u báº¯n Avro vÃ o topic: {TOPIC}...")

#     try:
#         while True:
#             # --- Táº¡o dá»¯ liá»‡u giáº£ láº­p ---
#             user_id = str(random.choice(valid_users))
#             item_id = str(random.choice(valid_items))
#             print(item_id)
            
#             event_type = random.choice(['click', 'view', 'add_to_cart', 'purchase'])
#             device = random.choice(['mobile', 'desktop', 'tablet'])
#             location = random.choice(['Hanoi', 'HCM', 'Danang', 'Cantho'])
            
#             # Táº¡o Object Python khá»›p hoÃ n toÃ n vá»›i Schema Avro
#             value_obj = {
#                 "event_id": str(uuid.uuid4()),
#                 "session_id": str(uuid.uuid4()),
#                 "user_id": user_id,
#                 "item_id": item_id,
#                 "event_type": event_type,
#                 "rating_original": round(random.uniform(1.0, 5.0), 1),
#                 "timestamp": datetime.now().isoformat(),
#                 # Nested Record (Context)
#                 "context": {
#                     "device": device,
#                     "location": location,
#                     "ip": f"192.168.1.{random.randint(1, 255)}"
#                 }
#             }

#             print(f"ğŸ“¤ Sending Avro: User={user_id} -> Item={item_id} | Type={event_type}")

#             # --- Gá»­i tin nháº¯n ---
#             # Producer nÃ y sáº½ tá»± Ä‘á»™ng:
#             # 1. ÄÄƒng kÃ½ Schema lÃªn Registry (náº¿u chÆ°a cÃ³).
#             # 2. Láº¥y Schema ID.
#             # 3. ChÃ¨n 5-byte header + data binary.
#             producer.produce(
#                 topic=TOPIC,
#                 key=user_id, # DÃ¹ng UserID lÃ m Key Ä‘á»ƒ partition Ä‘Ãºng
#                 value=value_obj,
#                 on_delivery=delivery_report
#             )

#             # Poll Ä‘á»ƒ trigger callback delivery_report
#             producer.poll(0)
            
#             time.sleep(0.002) # Báº¯n 1 tin/giÃ¢y

#     except KeyboardInterrupt:
#         print("\nğŸ›‘ Dá»«ng simulation.")
#     except Exception as e:
#         print(f"âŒ Lá»—i Producer: {e}")
#     finally:
#         print("â³ Äang flush dá»¯ liá»‡u cÃ²n sÃ³t láº¡i...")
#         producer.flush()

# if __name__ == "__main__":
#     main()

# import time
# import json
# import random
# import os
# import uuid
# import argparse
# from datetime import datetime, timedelta

# # ThÆ° viá»‡n Confluent Kafka
# from confluent_kafka import SerializingProducer
# from confluent_kafka.serialization import StringSerializer
# from confluent_kafka.schema_registry import SchemaRegistryClient
# from confluent_kafka.schema_registry.avro import AvroSerializer

# # ==========================================
# # Cáº¤U HÃŒNH (CONFIG)
# # ==========================================
# KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP', 'localhost:9092')
# SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:9081')
# TOPIC = 'raw_clicks_avro'

# # Path
# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))
# VALID_USERS_PATH = os.path.join(PROJECT_ROOT, 'src/simulation/users.json')
# ITEM_MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')

# # ==========================================
# # SCHEMA (Giá»¯ nguyÃªn)
# # ==========================================
# VALUE_SCHEMA_STR = """
# {
#   "namespace": "ecommerce.tracking",
#   "type": "record",
#   "name": "UserClick",
#   "fields": [
#     {"name": "event_id", "type": "string"},
#     {"name": "session_id", "type": "string"},
#     {"name": "user_id", "type": "string"},
#     {"name": "item_id", "type": "string"},
#     {"name": "event_type", "type": "string"},
#     {"name": "rating_original", "type": "float"},
#     {"name": "timestamp", "type": "string"},
#     {"name": "context", "type": {
#         "type": "record",
#         "name": "ContextData",
#         "fields": [
#             {"name": "device", "type": "string"},
#             {"name": "location", "type": "string"},
#             {"name": "ip", "type": "string"}
#         ]
#     }}
#   ]
# }
# """

# def load_valid_data():
#     if not os.path.exists(VALID_USERS_PATH):
#         # Fallback náº¿u chÆ°a cÃ³ file user
#         print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file users.json, táº¡o user giáº£ láº­p táº¡m thá»i.")
#         return [f"User_{i}" for i in range(100)], [f"Item_{i}" for i in range(1000)]

#     with open(VALID_USERS_PATH, 'r') as f:
#         users = json.load(f)
#     with open(ITEM_MAP_PATH, 'r') as f:
#         item_map = json.load(f)
#         # Láº¥y Key (ASIN/ID gá»‘c) thay vÃ¬ Value
#         items = list(item_map.keys()) 
#     return users, items

# def delivery_report(err, msg):
#     if err is not None:
#         print(f"âŒ Message delivery failed: {err}")

# # ==========================================
# # MAIN SIMULATION LOGIC
# # ==========================================
# def main():
#     # 1. Cáº¥u hÃ¬nh tham sá»‘ cháº¡y
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--days", type=int, default=14, help="Sá»‘ ngÃ y muá»‘n giáº£ láº­p (VD: 14 ngÃ y)")
#     parser.add_argument("--msgs-per-day", type=int, default=2000, help="Sá»‘ message má»—i ngÃ y")
#     args = parser.parse_args()

#     # 2. Setup Kafka
#     users, items = load_valid_data()
#     schema_registry_client = SchemaRegistryClient({'url': SCHEMA_REGISTRY_URL})
#     avro_serializer = AvroSerializer(schema_registry_client, VALUE_SCHEMA_STR)
    
#     producer_conf = {
#         'bootstrap.servers': KAFKA_BOOTSTRAP,
#         'key.serializer': StringSerializer('utf_8'),
#         'value.serializer': avro_serializer,
#         'queue.buffering.max.messages': 50000 # TÄƒng buffer Ä‘á»ƒ báº¯n nhanh hÆ¡n
#     }
#     producer = SerializingProducer(producer_conf)

#     # 3. TÃ­nh toÃ¡n thá»i gian báº¯t Ä‘áº§u (LÃ¹i láº¡i N ngÃ y so vá»›i hiá»‡n táº¡i)
#     # VÃ­ dá»¥: HÃ´m nay 23/12, lÃ¹i 14 ngÃ y -> Báº¯t Ä‘áº§u tá»« 09/12
#     start_date = datetime.now() - timedelta(days=args.days)
    
#     print(f"ğŸš€ Báº®T Äáº¦U GIáº¢ Láº¬P: {args.days} ngÃ y | {args.msgs_per_day} msg/ngÃ y")
#     print(f"ğŸ“… Thá»i gian dá»¯ liá»‡u (Event Time): Tá»« {start_date.strftime('%Y-%m-%d')} Ä‘áº¿n {datetime.now().strftime('%Y-%m-%d')}")
#     print("-" * 50)

#     total_sent = 0

#     try:
#         # --- VÃ’NG Láº¶P NGÃ€Y (Tá»« ngÃ y xÆ°a -> HÃ´m nay) ---
#         for day_offset in range(args.days):
#             current_day = start_date + timedelta(days=day_offset)
#             print(f"ğŸ“… Äang sinh dá»¯ liá»‡u cho ngÃ y: {current_day.strftime('%Y-%m-%d')} ...")

#             # --- VÃ’NG Láº¶P MESSAGE TRONG NGÃ€Y ---
#             for _ in range(args.msgs_per_day):
#                 # Random thá»i gian trong vÃ²ng 24h cá»§a ngÃ y hÃ´m Ä‘Ã³
#                 random_second = random.randint(0, 86399)
#                 event_time = current_day.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(seconds=random_second)

#                 user_id = str(random.choice(users))
#                 item_id = str(random.choice(items))
#                 event_type = random.choice(['click', 'click', 'click', 'view', 'purchase']) # Æ¯u tiÃªn click nhiá»u hÆ¡n

#                 value_obj = {
#                     "event_id": str(uuid.uuid4()),
#                     "session_id": str(uuid.uuid4()),
#                     "user_id": user_id,
#                     "item_id": item_id,
#                     "event_type": event_type,
#                     "rating_original": round(random.uniform(1.0, 5.0), 1),
#                     "timestamp": event_time.isoformat(), # <--- DÃ¹ng thá»i gian giáº£ láº­p
#                     "context": {
#                         "device": random.choice(['mobile', 'desktop']),
#                         "location": random.choice(['Hanoi', 'HCM']),
#                         "ip": "127.0.0.1"
#                     }
#                 }

#                 producer.produce(
#                     topic=TOPIC,
#                     key=user_id,
#                     value=value_obj,
#                     on_delivery=delivery_report
#                 )
                
#                 total_sent += 1
                
#                 # Cá»© 1000 tin thÃ¬ poll má»™t láº§n Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»› (nhanh hÆ¡n sleep tá»«ng tin)
#                 if total_sent % 1000 == 0:
#                     producer.poll(0)
            
#             # Flush nháº¹ sau má»—i ngÃ y Ä‘á»ƒ Ä‘áº£m báº£o data vÃ o Kafka theo thá»© tá»± tÆ°Æ¡ng Ä‘á»‘i
#             producer.flush()
#             print(f"   âœ… Xong ngÃ y {current_day.strftime('%Y-%m-%d')} ({args.msgs_per_day} msgs)")

#     except KeyboardInterrupt:
#         print("\nğŸ›‘ Dá»«ng simulation.")
#     except Exception as e:
#         print(f"âŒ Lá»—i: {e}")
#     finally:
#         producer.flush()
#         print(f"\nğŸ‰ HOÃ€N Táº¤T! Tá»•ng cá»™ng {total_sent} báº£n tin Ä‘Ã£ Ä‘Æ°á»£c gá»­i vÃ o Kafka.")

# if __name__ == "__main__":
#     main()

# import time
# import json
# import random
# import os
# import uuid
# import argparse
# from datetime import datetime, timedelta

# # ThÆ° viá»‡n Confluent Kafka
# from confluent_kafka import SerializingProducer
# from confluent_kafka.serialization import StringSerializer
# from confluent_kafka.schema_registry import SchemaRegistryClient
# from confluent_kafka.schema_registry.avro import AvroSerializer

# # ==========================================
# # Cáº¤U HÃŒNH (CONFIG)
# # ==========================================
# KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP', 'localhost:9092')
# SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:9081')
# TOPIC = 'raw_clicks_avro'

# # Path
# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))
# VALID_USERS_PATH = os.path.join(PROJECT_ROOT, 'src/simulation/users.json')
# ITEM_MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')

# # ==========================================
# # SCHEMA
# # ==========================================
# VALUE_SCHEMA_STR = """
# {
#   "namespace": "ecommerce.tracking",
#   "type": "record",
#   "name": "UserClick",
#   "fields": [
#     {"name": "event_id", "type": "string"},
#     {"name": "session_id", "type": "string"},
#     {"name": "user_id", "type": "string"},
#     {"name": "item_id", "type": "string"},
#     {"name": "event_type", "type": "string"},
#     {"name": "rating_original", "type": "float"},
#     {"name": "timestamp", "type": "string"},
#     {"name": "context", "type": {
#         "type": "record",
#         "name": "ContextData",
#         "fields": [
#             {"name": "device", "type": "string"},
#             {"name": "location", "type": "string"},
#             {"name": "ip", "type": "string"}
#         ]
#     }}
#   ]
# }
# """

# def load_valid_data():
#     if not os.path.exists(VALID_USERS_PATH):
#         print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file users.json, táº¡o user giáº£ láº­p táº¡m thá»i.")
#         return [f"User_{i}" for i in range(100)], [f"Item_{i}" for i in range(1000)]

#     with open(VALID_USERS_PATH, 'r') as f:
#         users = json.load(f)
#     with open(ITEM_MAP_PATH, 'r') as f:
#         item_map = json.load(f)
#         items = list(item_map.keys()) 
#     return users, items

# def delivery_report(err, msg):
#     if err is not None:
#         print(f"âŒ Message delivery failed: {err}")

# # ==========================================
# # MAIN SIMULATION LOGIC
# # ==========================================
# def main():
#     # 1. Cáº¥u hÃ¬nh tham sá»‘ cháº¡y
#     parser = argparse.ArgumentParser()
#     # ğŸ”¥ Sá»¬A: Máº·c Ä‘á»‹nh cháº¡y 14 ngÃ y (2 tuáº§n)
#     parser.add_argument("--days", type=int, default=14, help="Sá»‘ ngÃ y muá»‘n giáº£ láº­p")
#     parser.add_argument("--msgs-per-day", type=int, default=2000, help="Sá»‘ message má»—i ngÃ y")
#     args = parser.parse_args()

#     # 2. Setup Kafka
#     users, items = load_valid_data()
#     schema_registry_client = SchemaRegistryClient({'url': SCHEMA_REGISTRY_URL})
#     avro_serializer = AvroSerializer(schema_registry_client, VALUE_SCHEMA_STR)
    
#     producer_conf = {
#         'bootstrap.servers': KAFKA_BOOTSTRAP,
#         'key.serializer': StringSerializer('utf_8'),
#         'value.serializer': avro_serializer,
#         'queue.buffering.max.messages': 50000 
#     }
#     producer = SerializingProducer(producer_conf)

#     # 3. TÃ­nh toÃ¡n thá»i gian báº¯t Ä‘áº§u
#     # ğŸ”¥ Sá»¬A: Báº¯t Ä‘áº§u tá»« ngÃ y 23/12/2025
#     # (Khá»›p vá»›i thá»i Ä‘iá»ƒm káº¿t thÃºc cá»§a táº­p Train Data Ä‘Ã£ hack time)
#     start_date = datetime(2025, 12, 23)
    
#     end_date = start_date + timedelta(days=args.days)
    
#     print(f"ğŸš€ Báº®T Äáº¦U GIáº¢ Láº¬P TÆ¯Æ NG LAI: {args.days} ngÃ y (2 tuáº§n)")
#     print(f"ğŸ“… Thá»i gian dá»¯ liá»‡u (Event Time): Tá»« {start_date.strftime('%Y-%m-%d')} Ä‘áº¿n {end_date.strftime('%Y-%m-%d')}")
#     print("-" * 50)

#     total_sent = 0

#     try:
#         # --- VÃ’NG Láº¶P NGÃ€Y (Tá»« 23/12 -> TÆ°Æ¡ng lai) ---
#         for day_offset in range(args.days):
#             current_day = start_date + timedelta(days=day_offset)
#             print(f"ğŸ“… Äang sinh dá»¯ liá»‡u cho ngÃ y: {current_day.strftime('%Y-%m-%d')} ...")

#             # --- VÃ’NG Láº¶P MESSAGE TRONG NGÃ€Y ---
#             for _ in range(args.msgs_per_day):
#                 # Random thá»i gian trong vÃ²ng 24h cá»§a ngÃ y hÃ´m Ä‘Ã³
#                 random_second = random.randint(0, 86399)
#                 event_time = current_day.replace(hour=0, minute=0, second=0, microsecond=0) + timedelta(seconds=random_second)

#                 user_id = str(random.choice(users))
#                 item_id = str(random.choice(items))
#                 event_type = random.choice(['click', 'click', 'click', 'view', 'purchase']) 

#                 value_obj = {
#                     "event_id": str(uuid.uuid4()),
#                     "session_id": str(uuid.uuid4()),
#                     "user_id": user_id,
#                     "item_id": item_id,
#                     "event_type": event_type,
#                     "rating_original": round(random.uniform(1.0, 5.0), 1),
#                     "timestamp": event_time.isoformat(), # DÃ¹ng thá»i gian tÆ°Æ¡ng lai nÃ y
#                     "context": {
#                         "device": random.choice(['mobile', 'desktop']),
#                         "location": random.choice(['Hanoi', 'HCM']),
#                         "ip": "127.0.0.1"
#                     }
#                 }

#                 producer.produce(
#                     topic=TOPIC,
#                     key=user_id,
#                     value=value_obj,
#                     on_delivery=delivery_report
#                 )
                
#                 total_sent += 1
                
#                 if total_sent % 1000 == 0:
#                     producer.poll(0)
            
#             # Flush nháº¹ sau má»—i ngÃ y
#             producer.flush()
#             print(f"   âœ… Xong ngÃ y {current_day.strftime('%Y-%m-%d')} ({args.msgs_per_day} msgs)")

#     except KeyboardInterrupt:
#         print("\nğŸ›‘ Dá»«ng simulation.")
#     except Exception as e:
#         print(f"âŒ Lá»—i: {e}")
#     finally:
#         producer.flush()
#         print(f"\nğŸ‰ HOÃ€N Táº¤T! Tá»•ng cá»™ng {total_sent} báº£n tin Ä‘Ã£ Ä‘Æ°á»£c gá»­i vÃ o Kafka.")

# if __name__ == "__main__":
#     main()

import time
import json
import os
import uuid
import pandas as pd
import numpy as np
from datetime import datetime

# ThÆ° viá»‡n Kafka Client
from confluent_kafka import SerializingProducer
from confluent_kafka.serialization import StringSerializer
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroSerializer

# ==========================================
# 1. Cáº¤U HÃŒNH (CONFIGURATION)
# ==========================================
# Káº¿t ná»‘i Kafka & Schema Registry
KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP', 'localhost:9092')
SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:9081')
TOPIC = 'raw_clicks_avro'

# ÄÆ°á»ng dáº«n file (Tá»± Ä‘á»™ng tÃ­nh toÃ¡n tÆ°Æ¡ng Ä‘á»‘i)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))

# Input: File Parquet thÃ¡ng 12 & File Map Item
PARQUET_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/incremental_dec_2025')
MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')

# Tá»‘c Ä‘á»™ giáº£ láº­p (0 = Max Speed - Báº¯n nhanh nháº¥t cÃ³ thá»ƒ)
SIMULATION_SPEED = 0  

# ==========================================
# 2. AVRO SCHEMA (Äá»ŠNH NGHÄ¨A Cáº¤U TRÃšC TIN NHáº®N)
# ==========================================
VALUE_SCHEMA_STR = """
{
  "namespace": "ecommerce.tracking",
  "type": "record",
  "name": "UserClick",
  "fields": [
    {"name": "event_id", "type": "string"},
    {"name": "session_id", "type": "string"},
    {"name": "user_id", "type": "string"},
    {"name": "item_id", "type": "string"},
    {"name": "event_type", "type": "string"},
    {"name": "rating_original", "type": "float"},
    {"name": "timestamp", "type": "string"},
    {"name": "context", "type": {
        "type": "record",
        "name": "ContextData",
        "fields": [
            {"name": "device", "type": "string"},
            {"name": "location", "type": "string"},
            {"name": "ip", "type": "string"}
        ]
    }}
  ]
}
"""

# ==========================================
# 3. HÃ€M LOAD & Xá»¬ LÃ DATA (STRICT MODE)
# ==========================================
def load_and_prepare_data():
    """
    Äá»c Parquet, Map ID ngÆ°á»£c láº¡i thÃ nh String ASIN.
    Chá»‰ giá»¯ láº¡i Item cÃ³ trong Map. Táº¡o View + Purchase.
    """
    print(f"ğŸ“¥ Äang load dá»¯ liá»‡u nguá»“n tá»«: {PARQUET_PATH}")
    if not os.path.exists(PARQUET_PATH):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file data: {PARQUET_PATH}")

    # 1. Äá»c dá»¯ liá»‡u mÃ´ phá»ng (User thÃ¡ng 12)
    df = pd.read_parquet(PARQUET_PATH)
    
    # 2. Äá»c Map (Index -> Item ID tháº­t)
    print(f"ğŸ“– Äang Ä‘á»c Item Map tá»«: {MAP_PATH}")
    if not os.path.exists(MAP_PATH):
        raise FileNotFoundError(f"âŒ KhÃ´ng tÃ¬m tháº¥y file Map: {MAP_PATH}")

    with open(MAP_PATH, 'r') as f:
        item_map = json.load(f)
    
    # Chuyá»ƒn Key tá»« String sang Int Ä‘á»ƒ tra cá»©u (VÃ¬ sequence_ids trong Parquet lÃ  int)
    id_to_asin = {int(k): v for k, v in item_map.items()}

    print("ğŸ’¥ Äang xá»­ lÃ½: Kiá»ƒm tra Map & NhÃ¢n báº£n data (1 Purchase -> ThÃªm 1 View)...")
    
    all_events = []
    skipped_count = 0
    valid_items_count = 0
    
    # Duyá»‡t qua tá»«ng User trong file Parquet
    for _, row in df.iterrows():
        user_id = row['user_id']
        items = row['sequence_ids']          # List cÃ¡c item index [10, 20, 30]
        timestamps = row['sequence_timestamps'] # List timestamp tÆ°Æ¡ng á»©ng
        
        # Duyá»‡t qua tá»«ng hÃ nh Ä‘á»™ng trong chuá»—i cá»§a User Ä‘Ã³
        for item_idx, ts in zip(items, timestamps):
            
            # ğŸ”¥ [QUAN TRá»ŒNG]: STRICT CHECK
            # Náº¿u Item Index trong data má»›i KHÃ”NG CÃ“ trong Map cÅ© -> Bá» QUA LUÃ”N
            # Äá»ƒ trÃ¡nh lá»—i há»‡ thá»‘ng hoáº·c gá»£i Ã½ item rÃ¡c
            if item_idx not in id_to_asin:
                skipped_count += 1
                continue 
            
            # Láº¥y Item ID thá»±c (VÃ­ dá»¥: "B00123AB")
            real_item_id = id_to_asin[item_idx]
            valid_items_count += 1
            
            # --- Táº O Sá»° KIá»†N VIEW (Giáº£ láº­p: User xem hÃ ng trÆ°á»›c khi mua 10-120s) ---
            # Viá»‡c nÃ y giÃºp User cÅ© cÃ³ thÃªm dá»¯ liá»‡u tÆ°Æ¡ng tÃ¡c
            delay_seconds = np.random.randint(10, 120)
            all_events.append({
                "user_id": user_id,
                "item_id": real_item_id,
                "timestamp": ts - delay_seconds,
                "event_type": "view",       # Loáº¡i sá»± kiá»‡n View
                "rating": 0.0
            })
            
            # --- Táº O Sá»° KIá»†N PURCHASE (HÃ nh vi mua tháº­t trong Data) ---
            all_events.append({
                "user_id": user_id,
                "item_id": real_item_id,
                "timestamp": ts,
                "event_type": "purchase",   # Loáº¡i sá»± kiá»‡n Purchase
                "rating": 5.0
            })

    # Chuyá»ƒn List thÃ nh DataFrame Ä‘á»ƒ dá»… xá»­ lÃ½
    df_flat = pd.DataFrame(all_events)
    
    # Sáº¯p xáº¿p theo thá»i gian tÄƒng dáº§n (MÃ´ phá»ng dÃ²ng cháº£y thá»i gian thá»±c)
    if not df_flat.empty:
        df_flat = df_flat.sort_values(by="timestamp").reset_index(drop=True)
    
    print("-" * 60)
    print(f"âš ï¸  ÄÃ£ bá» qua (Skipped): {skipped_count} items (Do ID khÃ´ng cÃ³ trong Map)")
    print(f"âœ…  Há»£p lá»‡ (Valid):      {valid_items_count} items gá»‘c")
    print(f"ğŸš€  Tá»•ng Events sáº½ gá»­i:  {len(df_flat)} (Bao gá»“m cáº£ View & Purchase)")
    print("-" * 60)
    
    return df_flat

def delivery_report(err, msg):
    """Callback gá»i khi Kafka nháº­n tin thÃ nh cÃ´ng/tháº¥t báº¡i"""
    if err is not None:
        print(f"âŒ Gá»­i tháº¥t báº¡i: {err}")
    # else:
    #     print(f"âœ… Gá»­i thÃ nh cÃ´ng: {msg.key().decode('utf-8')}")

# ==========================================
# 4. MAIN LOOP (Gá»¬I TIN VÃ€O KAFKA)
# ==========================================
def main():
    # Cáº¥u hÃ¬nh Schema Registry
    schema_registry_client = SchemaRegistryClient({'url': SCHEMA_REGISTRY_URL})
    avro_serializer = AvroSerializer(schema_registry_client, VALUE_SCHEMA_STR)
    
    # Cáº¥u hÃ¬nh Producer (Tá»‘i Æ°u cho tá»‘c Ä‘á»™ cao)
    producer_conf = {
        'bootstrap.servers': KAFKA_BOOTSTRAP,
        'key.serializer': StringSerializer('utf_8'),
        'value.serializer': avro_serializer,
        'queue.buffering.max.messages': 500000, # Bá»™ Ä‘á»‡m lá»›n
        'linger.ms': 10,       # Gom batch má»—i 10ms (giÃºp gá»­i nhanh hÆ¡n gá»­i láº» táº»)
        'compression.type': 'snappy' # NÃ©n dá»¯ liá»‡u Ä‘á»ƒ tiáº¿t kiá»‡m bÄƒng thÃ´ng
    }
    producer = SerializingProducer(producer_conf)

    # 1. Load vÃ  Chuáº©n bá»‹ dá»¯ liá»‡u
    try:
        df_stream = load_and_prepare_data()
    except Exception as e:
        print(f"âŒ Lá»—i Data Preparation: {e}")
        return

    total_records = len(df_stream)
    if total_records == 0:
        print("âš ï¸ KhÃ´ng cÃ³ data há»£p lá»‡ Ä‘á»ƒ gá»­i. Kiá»ƒm tra láº¡i file Incremental!")
        return

    print(f"ğŸš€ Báº®T Äáº¦U SIMULATION: Äang gá»­i {total_records} events vÃ o topic '{TOPIC}'...")
    
    start_time = time.time()
    
    # 2. VÃ²ng láº·p gá»­i tin
    for i, row in df_stream.iterrows():
        
        # Chuyá»ƒn timestamp sá»‘ sang ISO String (cho Ä‘Ãºng schema Avro)
        ts_iso = datetime.fromtimestamp(row['timestamp']).isoformat()
        
        value_obj = {
            "event_id": str(uuid.uuid4()),
            "session_id": str(uuid.uuid4()),
            "user_id": str(row['user_id']),
            "item_id": str(row['item_id']), # Äáº£m báº£o lÃ  string
            "event_type": row['event_type'],
            "rating_original": float(row['rating']),
            "timestamp": ts_iso,
            "context": {
                "device": "desktop",
                "location": "Simulation_Dec2025",
                "ip": "127.0.0.1"
            }
        }

        # Gá»­i Async (KhÃ´ng chá» pháº£n há»“i tá»«ng tin)
        producer.produce(
            topic=TOPIC,
            key=str(row['user_id']),
            value=value_obj,
            on_delivery=delivery_report
        )
        
        # Poll nháº¹ Ä‘á»ƒ Kafka client xá»­ lÃ½ callback (trÃ¡nh trÃ n RAM local)
        # 5000 tin má»›i poll 1 láº§n Ä‘á»ƒ tÄƒng tá»‘c
        if i % 5000 == 0:
            producer.poll(0)
            elapsed = time.time() - start_time
            rate = i / elapsed if elapsed > 0 else 0
            print(f"ğŸ“¤ ÄÃ£ gá»­i {i}/{total_records} events... (Tá»‘c Ä‘á»™: {int(rate)} msg/s)")

        # Náº¿u muá»‘n giáº£ láº­p tá»‘c Ä‘á»™ cháº­m thÃ¬ uncomment dÃ²ng dÆ°á»›i
        if SIMULATION_SPEED > 0:
            time.sleep(SIMULATION_SPEED)

    # 3. Káº¿t thÃºc
    print("â³ Äang Ä‘áº©y ná»‘t cÃ¡c tin cÃ²n láº¡i trong hÃ ng Ä‘á»£i (Flushing)...")
    producer.flush()
    
    duration = time.time() - start_time
    print(f"ğŸ‰ HOÃ€N Táº¤T! ÄÃ£ gá»­i {total_records} events trong {duration:.2f}s.")

if __name__ == "__main__":
    main()
# import time
# import json
# import os
# import uuid
# import pandas as pd
# import numpy as np
# from datetime import datetime

# # Kafka
# from confluent_kafka import SerializingProducer
# from confluent_kafka.serialization import StringSerializer
# from confluent_kafka.schema_registry import SchemaRegistryClient
# from confluent_kafka.schema_registry.avro import AvroSerializer

# # ======================================================
# # 1. CONFIG
# # ======================================================
# KAFKA_BOOTSTRAP = os.getenv('KAFKA_BOOTSTRAP', 'localhost:9092')
# SCHEMA_REGISTRY_URL = os.getenv('SCHEMA_REGISTRY_URL', 'http://localhost:9081')
# TOPIC = 'raw_clicks_avro'

# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))

# PARQUET_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/incremental_dec_2025')
# MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')

# # ğŸ”¥ Simulation Control
# SIMULATION_SPEED = 0        # 0 = theo timestamp tháº­t | >0 = sleep cá»‘ Ä‘á»‹nh (giÃ¢y)
# ENABLE_NOISE = True

# # ======================================================
# # 2. AVRO SCHEMA
# # ======================================================
# VALUE_SCHEMA_STR = """
# {
#   "namespace": "ecommerce.tracking",
#   "type": "record",
#   "name": "UserClick",
#   "fields": [
#     {"name": "event_id", "type": "string"},
#     {"name": "session_id", "type": "string"},
#     {"name": "user_id", "type": "string"},
#     {"name": "item_id", "type": "string"},
#     {"name": "event_type", "type": "string"},
#     {"name": "rating_original", "type": "float"},
#     {"name": "timestamp", "type": "string"},
#     {"name": "context", "type": {
#       "type": "record",
#       "name": "ContextData",
#       "fields": [
#         {"name": "device", "type": "string"},
#         {"name": "location", "type": "string"},
#         {"name": "ip", "type": "string"}
#       ]
#     }}
#   ]
# }
# """

# # ======================================================
# # 3. LOAD & PREPARE DATA
# # ======================================================
# def load_and_prepare_data():
#     print(f"ğŸ“¥ Load parquet: {PARQUET_PATH}")
#     df = pd.read_parquet(PARQUET_PATH)

#     print(f"ğŸ“– Load item map: {MAP_PATH}")
#     with open(MAP_PATH, 'r') as f:
#         item_map = json.load(f)

#     # index -> ASIN
#     id_to_asin = {int(k): v for k, v in item_map.items()}
#     all_valid_item_ids = list(id_to_asin.values())

#     all_events = []
#     skipped = 0

#     for _, row in df.iterrows():
#         user_id = row['user_id']
#         items = row['sequence_ids']
#         timestamps = row['sequence_timestamps']

#         for item_idx, ts in zip(items, timestamps):
#             if item_idx not in id_to_asin:
#                 skipped += 1
#                 continue

#             real_item = id_to_asin[item_idx]

#             # ğŸŒª NOISE
#             if ENABLE_NOISE:
#                 for _ in range(np.random.randint(2, 5)):
#                     noise_item = np.random.choice(all_valid_item_ids)
#                     if noise_item == real_item:
#                         continue

#                     noise_ts = ts - np.random.randint(120, 600)
#                     all_events.append({
#                         "user_id": user_id,
#                         "item_id": noise_item,
#                         "timestamp": noise_ts,
#                         "event_type": "view",
#                         "rating": 0.0
#                     })

#             # ğŸ¯ REAL EVENTS
#             all_events.append({
#                 "user_id": user_id,
#                 "item_id": real_item,
#                 "timestamp": ts - np.random.randint(10, 120),
#                 "event_type": "view",
#                 "rating": 0.0
#             })

#             all_events.append({
#                 "user_id": user_id,
#                 "item_id": real_item,
#                 "timestamp": ts,
#                 "event_type": "purchase",
#                 "rating": 5.0
#             })

#     df_events = pd.DataFrame(all_events)
#     df_events = df_events.sort_values("timestamp").reset_index(drop=True)

#     print("-" * 60)
#     print(f"âš ï¸ Skipped (no map): {skipped}")
#     print(f"ğŸš€ Total events: {len(df_events)}")
#     print("-" * 60)

#     return df_events

# # ======================================================
# # 4. DELIVERY CALLBACK
# # ======================================================
# def delivery_report(err, msg):
#     if err is not None:
#         print(f"âŒ Delivery failed: {err}")

# # ======================================================
# # 5. MAIN
# # ======================================================
# def main():
#     schema_registry_client = SchemaRegistryClient({'url': SCHEMA_REGISTRY_URL})
#     avro_serializer = AvroSerializer(schema_registry_client, VALUE_SCHEMA_STR)

#     # ğŸ”¥ PRODUCER Ã‰P REALTIME
#     producer_conf = {
#         'bootstrap.servers': KAFKA_BOOTSTRAP,
#         'key.serializer': StringSerializer('utf_8'),
#         'value.serializer': avro_serializer,
#         'linger.ms': 0,
#         'batch.num.messages': 1,
#         'queue.buffering.max.messages': 10,
#         'acks': 'all'
#     }

#     producer = SerializingProducer(producer_conf)

#     df_stream = load_and_prepare_data()
#     if df_stream.empty:
#         print("âš ï¸ No data to stream")
#         return

#     print(f"ğŸš€ START STREAMING TO TOPIC: {TOPIC}")
#     start_time = time.time()
#     prev_ts = None

#     for i, row in df_stream.iterrows():
#         # â±ï¸ REAL TIME DELAY
#         if SIMULATION_SPEED > 0:
#             time.sleep(SIMULATION_SPEED)
#         elif prev_ts is not None:
#             delta = row['timestamp'] - prev_ts
#             if delta > 0:
#                 time.sleep(min(delta, 2))  # cap 2s

#         prev_ts = row['timestamp']

#         ts_iso = datetime.fromtimestamp(row['timestamp']).isoformat()

#         value_obj = {
#             "event_id": str(uuid.uuid4()),
#             "session_id": str(uuid.uuid4()),
#             "user_id": str(row['user_id']),
#             "item_id": str(row['item_id']),
#             "event_type": row['event_type'],
#             "rating_original": float(row['rating']),
#             "timestamp": ts_iso,
#             "context": {
#                 "device": "desktop",
#                 "location": "VN_Simulation",
#                 "ip": "192.168.1.1"
#             }
#         }

#         producer.produce(
#             topic=TOPIC,
#             key=str(row['user_id']),
#             value=value_obj,
#             on_delivery=delivery_report
#         )

#         # ğŸ”¥ Gá»¬I NGAY Tá»ªNG EVENT
#         producer.poll(0)
#         producer.flush(timeout=1.0)

#         if i % 500 == 0:
#             elapsed = time.time() - start_time
#             print(f"ğŸ“¤ Sent {i}/{len(df_stream)} | {int(i / max(elapsed,1))} msg/s")

#     producer.flush()
#     print(f"ğŸ‰ DONE! Sent {len(df_stream)} events")

# # ======================================================
# if __name__ == "__main__":
#     main()
