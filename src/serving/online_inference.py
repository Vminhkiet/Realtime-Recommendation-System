import json
import requests
import time
import sys
import traceback
import redis # <--- TH∆Ø VI·ªÜN QUAN TR·ªåNG
from confluent_kafka import Consumer, KafkaError
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroDeserializer
from confluent_kafka.serialization import SerializationContext, MessageField

# ==========================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# ==========================================
KAFKA_BROKER = "localhost:9092"
SCHEMA_REGISTRY_URL = "http://localhost:9081"
TF_SERVING_URL = "http://localhost:8501/v1/models/sasrec:predict"
TOPIC_NAME = "processed_clicks"
GROUP_ID = "recommendation_inference_redis_final"
ITEM_MAP_PATH = "./data/model_registry/item_map.json"

# --- C·∫§U H√åNH REDIS ---
# N·∫øu ch·∫°y code n√†y TRONG DOCKER th√¨ host l√† "redis"
# N·∫øu ch·∫°y code n√†y TR√äN M√ÅY TH·∫¨T (nh∆∞ b·∫°n ƒëang l√†m) th√¨ host l√† "localhost"
REDIS_HOST = "localhost" 
REDIS_PORT = 6379
REDIS_DB = 0

# ==========================================
# 2. KH·ªûI T·∫†O K·∫æT N·ªêI
# ==========================================
print("‚è≥ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...")

# 2.1. Load Item Map
try:
    with open(ITEM_MAP_PATH, 'r') as f:
        item_map = json.load(f)
    reverse_item_map = {int(k): v for k, v in item_map.items()}
    print(f"‚úÖ ƒê√£ load Item Map ({len(item_map)} items).")
except FileNotFoundError:
    print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file {ITEM_MAP_PATH}. K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã d·∫°ng 'Index_XXX'.")
    reverse_item_map = {}

# 2.2. K·∫øt n·ªëi Redis
try:
    redis_client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB)
    redis_client.ping() # Test k·∫øt n·ªëi
    print(f"‚úÖ K·∫øt n·ªëi Redis th√†nh c√¥ng t·∫°i {REDIS_HOST}:{REDIS_PORT}")
except Exception as e:
    print(f"‚ùå L·ªói k·∫øt n·ªëi Redis: {e}")
    # sys.exit(1) # T√πy ch·ªçn: C√≥ th·ªÉ t·∫Øt app lu√¥n n·∫øu Redis ch·∫øt

# 2.3. K·∫øt n·ªëi Kafka
schema_registry_conf = {'url': SCHEMA_REGISTRY_URL}
try:
    schema_registry_client = SchemaRegistryClient(schema_registry_conf)
    avro_deserializer = AvroDeserializer(schema_registry_client)
except Exception as e:
    print(f"‚ùå L·ªói Schema Registry: {e}")
    sys.exit(1)

consumer_conf = {
    'bootstrap.servers': KAFKA_BROKER,
    'group.id': GROUP_ID,
    'auto.offset.reset': 'latest'
}
consumer = Consumer(consumer_conf)
consumer.subscribe([TOPIC_NAME])

print(f"‚úÖ ƒêang l·∫Øng nghe topic: {TOPIC_NAME}")
print("---------------------------------------------------------")

# ==========================================
# 3. H√ÄM CHU·∫®N B·ªä PAYLOAD
# ==========================================
def prepare_tf_serving_payload(history_list):
    MAX_LEN = 50
    if len(history_list) > MAX_LEN:
        processed_history = history_list[-MAX_LEN:]
    else:
        padding = [0.0] * (MAX_LEN - len(history_list))
        processed_history = history_list + padding

    real_item_ids = [float(x) for x in processed_history]
    dummy_safe_sequence = [1.0] * MAX_LEN 

    payload = {
        "signature_name": "serving_default",
        "inputs": {
            "args_0":   [dummy_safe_sequence], 
            "args_0_1": [real_item_ids],       
            "args_0_2": [real_item_ids]        
        }
    }
    return payload

# ==========================================
# 4. MAIN LOOP (INFERENCE & SAVE TO REDIS)
# ==========================================
try:
    while True:
        msg = consumer.poll(1.0)
        if msg is None: continue
        if msg.error():
            print(f"Kafka Error: {msg.error()}")
            continue

        try:
            data = avro_deserializer(msg.value(), SerializationContext(msg.topic(), MessageField.VALUE))
            if data is None: continue
            
            current_item_idx = data.get('item_idx')
            user_id = data.get('user_id', 'Unknown')

            if current_item_idx is None: continue

            # =========================================================
            # üß† PH·∫¶N QUAN TR·ªåNG: QU·∫¢N L√ù L·ªäCH S·ª¨ (SLIDING WINDOW)
            # =========================================================
            history_key = f"hist:{user_id}"  # Key l∆∞u l·ªãch s·ª≠ xem
            
            # 1. Th√™m item m·ªõi v√†o ƒëu√¥i danh s√°ch (Right Push)
            redis_client.rpush(history_key, current_item_idx)
            
            # 2. C·∫Øt danh s√°ch (Sliding Window): Ch·ªâ gi·ªØ l·∫°i 50 item m·ªõi nh·∫•t
            # LTRIM gi·ªØ l·∫°i c√°c ph·∫ßn t·ª≠ trong kho·∫£ng index (start, stop)
            # -50 nghƒ©a l√† l·∫•y t·ª´ c√°i th·ª© 50 ƒë·∫øm t·ª´ d∆∞·ªõi l√™n
            redis_client.ltrim(history_key, -50, -1)
            redis_client.expire(history_key, 30)
            # 3. L·∫•y to√†n b·ªô l·ªãch s·ª≠ ra ƒë·ªÉ ƒë∆∞a v√†o Model
            # LRANGE tr·∫£ v·ªÅ list c√°c byte (b'123'), c·∫ßn √©p ki·ªÉu v·ªÅ int
            raw_history = redis_client.lrange(history_key, 0, -1)
            history_input = [int(x) for x in raw_history]

            print(f"‚ö° User [{user_id}] - V·ª´a click: {current_item_idx}")
            print(f"üìö Context L·ªãch s·ª≠ ({len(history_input)} items): {history_input}")

            # =========================================================

            # --- G·ª≠i Request AI (Gi·ªù input ƒë√£ l√† list d√†i ƒë·∫ßy ƒë·ªß) ---
            payload = prepare_tf_serving_payload(history_input)
            
            start_time = time.time()
            response = requests.post(TF_SERVING_URL, json=payload)
            latency = (time.time() - start_time) * 1000

            if response.status_code == 200:
                result = response.json()
                outputs = result.get('outputs') or result.get('predictions')
                
                if isinstance(outputs, dict):
                    final_result = outputs.get('predictions') or list(outputs.values())[0]
                else:
                    final_result = outputs

                if final_result and len(final_result) > 0:
                    top_indices = final_result[0]
                    
                    top_product_ids = []
                    for idx in top_indices:
                        p_id = reverse_item_map.get(int(idx), f"Index_{int(idx)}")
                        top_product_ids.append(p_id)

                    # L∆∞u k·∫øt qu·∫£ g·ª£i √Ω v√†o Redis (Key rec:...)
                    rec_key = f"rec:{user_id}"
                    redis_client.setex(rec_key, 3600, json.dumps(top_product_ids))

                    print(f"üíé G·ª£i √Ω m·ªõi nh·∫•t: {top_product_ids[:3]}...")
                    print("-" * 50)
            else:
                print(f"‚ùå L·ªói TF Serving: {response.text}")

        except Exception:
            traceback.print_exc()

except KeyboardInterrupt:
    print("\nüõë D·ª´ng ch∆∞∆°ng tr√¨nh.")
finally:
    consumer.close()