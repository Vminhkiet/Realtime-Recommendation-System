from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType, LongType
import json
import time
# Import file utils v·ª´a t·∫°o
import utils 
# Import Kafka Producer ƒë·ªÉ b·∫Øn k·∫øt qu·∫£ ra
from kafka import KafkaProducer

# C·∫•u h√¨nh Kafka
KAFKA_SERVER = "kafka:29092"
TOPIC_INPUT = "user_clicks"
TOPIC_OUTPUT = "recommendations"

def process_batch(df, batch_id):
    """
    H√†m n√†y ch·∫°y m·ªói khi c√≥ m·ªôt l√¥ d·ªØ li·ªáu m·ªõi t·ª´ Kafka (Micro-batch)
    """
    # N·∫øu batch r·ªóng th√¨ b·ªè qua
    if df.count() == 0: return
    
    # Chuy·ªÉn Spark DataFrame th√†nh List Python ƒë·ªÉ d·ªÖ x·ª≠ l√Ω logic ph·ª©c t·∫°p
    rows = df.collect()
    print(f"\n‚ö° [Batch {batch_id}] ƒêang x·ª≠ l√Ω {len(rows)} clicks...")
    
    # T·∫°o k·∫øt n·ªëi Kafka Producer (ƒë·ªÉ g·ª≠i k·∫øt qu·∫£ g·ª£i √Ω)
    producer = KafkaProducer(bootstrap_servers=KAFKA_SERVER, 
                             value_serializer=lambda v: json.dumps(v).encode('utf-8'))
    
    for row in rows:
        user = row['user_id']
        item = row['item_id']
        
        # --- G·ªåI TR√ç TU·ªÜ NH√ÇN T·∫†O ---
        # 1. Update Redis & 2. Predict
        recommendations = utils.AIInferenceService.predict(user, item)
        
        if recommendations:
            # ƒê√≥ng g√≥i k·∫øt qu·∫£
            result = {
                "user_id": user,
                "trigger_item": item,
                "recommendations": recommendations, # List c√°c m√≥n g·ª£i √Ω
                "timestamp": int(time.time())
            }
            
            # 3. B·∫Øn ra Kafka (Topic: recommendations)
            producer.send(TOPIC_OUTPUT, result)
            
            # In ra m√†n h√¨nh ƒë·ªÉ Demo cho th·∫ßy xem
            print(f"‚úÖ G·ª£i √Ω cho {user[:5]}...: {recommendations[:2]}...")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ g·ª£i √Ω cho {user} (L·ªói ho·∫∑c Cold Start)")
            
    producer.flush()

def main():
    # Kh·ªüi t·∫°o Spark
    spark = SparkSession.builder \
        .appName("SpeedLayer_SASRec") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")

    # ƒê·ªãnh nghƒ©a c·∫•u tr√∫c d·ªØ li·ªáu t·ª´ Producer g·ª≠i sang
    schema = StructType() \
        .add("user_id", StringType()) \
        .add("item_id", StringType()) \
        .add("timestamp", LongType())

    # ƒê·ªçc lu·ªìng t·ª´ Kafka
    df_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_SERVER) \
        .option("subscribe", TOPIC_INPUT) \
        .load()
    
    # Parse JSON t·ª´ Kafka
    df_parsed = df_stream.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")
    
    # L·ªçc b·ªè d·ªØ li·ªáu l·ªói (null user)
    df_clean = df_parsed.filter(col("user_id").isNotNull())

    # Ch·∫°y Streaming v·ªõi h√†m process_batch
    query = df_clean.writeStream \
        .foreachBatch(process_batch) \
        .trigger(processingTime='2 seconds') \
        .start()

    print("üöÄ Speed Layer ƒëang ch·∫°y... ƒêang ch·ªù d·ªØ li·ªáu t·ª´ Kafka...")
    query.awaitTermination()

if __name__ == "__main__":
    main()