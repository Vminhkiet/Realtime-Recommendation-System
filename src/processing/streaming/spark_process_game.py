import os
import json
import shutil
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField,
    StringType, FloatType, LongType, IntegerType
)
from pyspark.ml.feature import StringIndexer

# ================== CONFIG ==================
# ƒê∆∞·ªùng d·∫´n g·ªëc
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# Nh·∫£y ra 3 c·∫•p th∆∞ m·ª•c ƒë·ªÉ v·ªÅ root project
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(BASE_DIR)))

# Input Files
RAW_DATA = os.path.join(PROJECT_ROOT, 'data/raw_source/Video_Games.jsonl')
META_DATA = os.path.join(PROJECT_ROOT, 'data/raw_source/meta_Video_Games.jsonl')

# Output Ch√≠nh (L·ªãch s·ª≠ ƒë·ªÉ Train Model)
OUTPUT_PARQUET = os.path.join(PROJECT_ROOT, 'data/model_registry/processed_parquet')

# Output Ph·ª• (D·ªØ li·ªáu T∆∞∆°ng lai gi·∫£ ƒë·ªãnh - Th√°ng 12/2025)
OUTPUT_INCREMENTAL = os.path.join(PROJECT_ROOT, 'data/model_registry/incremental_dec_2025')

# Output Maps
MAP_OUTPUT = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')
CAT_MAP_OUTPUT = os.path.join(PROJECT_ROOT, 'data/model_registry/category_map.json')
ITEM_CAT_MAP_OUTPUT = os.path.join(PROJECT_ROOT, 'data/model_registry/item_category.json')

# M·ªëc th·ªùi gian
MIN_TS = 1514764800  # 2018-01-01
# M·ªëc c·∫Øt d·ªØ li·ªáu: 01/12/2025 (ƒê·ªÉ gi·∫£ l·∫≠p d·ªØ li·ªáu m·ªõi ph√°t sinh)
DEC_2025_START_TS = 1761955200  # 2025-11-01 

# ================== MAIN ==================
def main():
    spark = SparkSession.builder \
        .appName("RecSys_VideoGames_Split_History_vs_Incremental") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()

    print("üöÄ START: Processing Amazon Video Games dataset...")

    # ========== 1. ƒê·ªäNH NGHƒ®A SCHEMA ==========
    review_schema = StructType([
        StructField("rating", FloatType(), True),
        StructField("parent_asin", StringType(), True),
        StructField("user_id", StringType(), True),
        StructField("timestamp", LongType(), True)
    ])

    meta_schema = StructType([
        StructField("parent_asin", StringType(), True),
        StructField("title", StringType(), True),
        StructField("main_category", StringType(), True)
    ])

    # ========== 2. ƒê·ªåC REVIEWS ==========
    if not os.path.exists(RAW_DATA):
        print(f"‚ùå Missing file: {RAW_DATA}")
        return

    df_reviews = spark.read.schema(review_schema).json(RAW_DATA)

    # L·ªçc r√°c
    df_reviews = (
        df_reviews
        .withColumnRenamed("parent_asin", "item_id")
        .dropna(subset=["item_id", "user_id", "timestamp"])
        .filter(F.col("rating") >= 3.0)      # Ch·ªâ l·∫•y rating t·ªët
        .filter(F.col("timestamp") >= MIN_TS) # Ch·ªâ l·∫•y t·ª´ 2018
    )

    print(f"‚úÖ Reviews loaded: {df_reviews.count():,}")

    # ========== 3. ƒê·ªåC & JOIN METADATA ==========
    if os.path.exists(META_DATA):
        print("üìò Reading metadata & Extracting Categories...")
        df_meta = spark.read.schema(meta_schema).json(META_DATA)
        df_meta = df_meta.withColumnRenamed("parent_asin", "item_id")
        df_meta = df_meta.withColumn("t_lower", F.lower(F.col("title")))

        # Logic g√°n Category d·ª±a tr√™n Title
        df_meta = df_meta.withColumn(
            "category_final",
            F.when(F.col("t_lower").contains("ps5"), "PS5")
            .when(F.col("t_lower").contains("ps4"), "PS4")
            .when(F.col("t_lower").contains("xbox"), "Xbox")
            .when((F.col("t_lower").contains("nintendo") & F.col("t_lower").contains("switch")), "NintendoSwitch")
            .when(F.col("t_lower").contains("pc") | F.col("t_lower").contains("steam"), "PC_Gaming")
            .when(F.col("t_lower").contains("controller"), "Controller")
            .when(F.col("t_lower").contains("headset") | F.col("t_lower").contains("headphone"), "Audio")
            .when(F.col("t_lower").contains("keyboard"), "Keyboard")
            .when(F.col("t_lower").contains("mouse"), "Mouse")
            .otherwise("Other_Gaming")
        )
        
        # Ch·ªâ l·∫•y item_id v√† category ƒë·ªÉ join
        df_meta_clean = df_meta.select("item_id", "category_final").dropDuplicates(["item_id"])
        
        df_joined = df_reviews.join(df_meta_clean, on="item_id", how="left")
        df_joined = df_joined.fillna({"category_final": "Unknown"})
    else:
        print("‚ö†Ô∏è Metadata not found. Setting all categories to Unknown.")
        df_joined = df_reviews.withColumn("category_final", F.lit("Unknown"))

    # ========== 4. INDEXING (QUAN TR·ªåNG: L√ÄM TR√äN TO√ÄN B·ªò DATA) ==========
    print("üî¢ Indexing Items & Categories (Global)...")
    
    # StringIndexer c·∫ßn qu√©t to√†n b·ªô data ƒë·ªÉ g√°n ID nh·∫•t qu√°n
    item_indexer = StringIndexer(inputCol="item_id", outputCol="item_idx_raw", handleInvalid="skip").fit(df_joined)
    df_indexed = item_indexer.transform(df_joined)

    cat_indexer = StringIndexer(inputCol="category_final", outputCol="cat_idx_raw", handleInvalid="keep").fit(df_indexed)
    df_indexed = cat_indexer.transform(df_indexed)

    # Shift index + 1 (ƒë·ªÉ d√†nh s·ªë 0 cho padding sau n√†y)
    df_indexed = (
        df_indexed
        .withColumn("item_idx", (F.col("item_idx_raw") + 1).cast(IntegerType()))
        .withColumn("category_ids", (F.col("cat_idx_raw") + 1).cast(IntegerType()))
    )

    # ==============================================================================
    # B∆Ø·ªöC 5: T√ÅCH DATA (SPLIT) - HISTORY (TRAIN) vs INCREMENTAL (SIMULATION)
    # ==============================================================================
    print(f"‚úÇÔ∏è  Splitting Data at Timestamp: {DEC_2025_START_TS} (Dec 1, 2025)")

    # --- A. T·∫¨P L·ªäCH S·ª¨ (HISTORY) ---
    # L·∫•y d·ªØ li·ªáu TR∆Ø·ªöC th√°ng 12/2025
    df_history_flat = df_indexed.filter(F.col("timestamp") < DEC_2025_START_TS)
    
    # Group l·∫°i th√†nh chu·ªói h√†nh vi
    df_history_grouped = df_history_flat.groupBy("user_id").agg(
        F.sort_array(F.collect_list(F.struct("timestamp", "item_idx", "category_ids"))).alias("events")
    )

    df_train_final = df_history_grouped.select(
        F.col("user_id"),
        F.col("events.item_idx").alias("sequence_ids"),
        F.col("events.category_ids").alias("category_ids"),
        F.col("events.timestamp").alias("sequence_timestamps"),
        F.element_at(F.col("events.timestamp"), -1).alias("last_timestamp")
    )

    # L·ªçc K-Core >= 5 (Ch·ªâ gi·ªØ User ch·∫•t l∆∞·ª£ng ƒë·ªÉ Train Model)
    df_train_final = df_train_final.filter(F.size(F.col("sequence_ids")) >= 3)
    
    # üî• Cache t·∫≠p Train l·∫°i ƒë·ªÉ d√πng cho b∆∞·ªõc Th·ªëng k√™ b√™n d∆∞·ªõi
    df_train_final.cache()
    
    print(f"‚úÖ [History] Valid Train Users (K-Core >= 3): {df_train_final.count():,}")

    # --- B. T·∫¨P INCREMENTAL (SIMULATION) ---
    # L·∫•y d·ªØ li·ªáu T·ª™ th√°ng 12/2025 tr·ªü ƒëi
    df_incremental_flat = df_indexed.filter(F.col("timestamp") >= DEC_2025_START_TS)
    
    # Group l·∫°i theo User (Gi·ªØ nguy√™n, kh√¥ng l·ªçc K-core, ƒë·ªÉ m√¥ ph·ªèng th·ª±c t·∫ø c√≥ c·∫£ user m·ªõi)
    df_test_final = df_incremental_flat.groupBy("user_id").agg(
        F.sort_array(F.collect_list(F.struct("timestamp", "item_idx", "category_ids"))).alias("events")
    ).select(
        "user_id",
        F.col("events.item_idx").alias("sequence_ids"),
        F.col("events.category_ids").alias("category_ids"),
        F.col("events.timestamp").alias("sequence_timestamps")
    )
    
    # Cache t·∫≠p Test
    df_test_final.cache()

    # ==============================================================================
    # B∆Ø·ªöC 6: L∆ØU FILES PARQUET
    # ==============================================================================
    
    # 1. L∆∞u History (Base 5 Years)
    print(f"üíæ Saving HISTORY Parquet -> {OUTPUT_PARQUET}")
    if os.path.exists(OUTPUT_PARQUET): shutil.rmtree(OUTPUT_PARQUET)
    df_train_final.write.parquet(OUTPUT_PARQUET)

    # 2. L∆∞u Incremental (Dec 2025)
    print(f"üíæ Saving INCREMENTAL Parquet -> {OUTPUT_INCREMENTAL}")
    if os.path.exists(OUTPUT_INCREMENTAL): shutil.rmtree(OUTPUT_INCREMENTAL)
    
    if not df_test_final.rdd.isEmpty():
        df_test_final.write.parquet(OUTPUT_INCREMENTAL)
        
        # ==============================================================================
        # üìä [NEW] TH·ªêNG K√ä USER C≈® vs M·ªöI TRONG T·∫¨P INCREMENTAL
        # ==============================================================================
        print("\n" + "="*50)
        print("üìä PH√ÇN T√çCH NG∆Ø·ªúI D√ôNG TH√ÅNG 12 (SIMULATION DATA)")
        print("="*50)
        
        total_inc_users = df_test_final.count()

        # ƒê·∫øm User C≈©: Join v·ªõi t·∫≠p Train (History) ƒë·ªÉ xem ai ƒë√£ t·ª´ng xu·∫•t hi·ªán
        # left_semi: Ch·ªâ l·∫•y nh·ªØng d√≤ng b√™n tr√°i c√≥ kh·ªõp v·ªõi b√™n ph·∫£i
        returning_users_count = df_test_final.join(df_train_final, on="user_id", how="left_semi").count()

        # ƒê·∫øm User M·ªõi
        new_users_count = total_inc_users - returning_users_count

        print(f"üëâ T·ªïng User Active (T12/2025):  {total_inc_users:,}")
        print(f"   ‚úÖ Kh√°ch Quen (Returning):    {returning_users_count:,}  (ƒê√£ c√≥ trong History Train)")
        print(f"   üÜï Kh√°ch M·ªõi (New/Cold):      {new_users_count:,}  (L·∫ßn ƒë·∫ßu th·∫•y ho·∫∑c history qu√° √≠t)")
        
        if total_inc_users > 0:
            print(f"   üìà T·ª∑ l·ªá Retention:           {(returning_users_count/total_inc_users)*100:.2f}%")
        print("="*50 + "\n")

    else:
        print("‚ö†Ô∏è No data found in Dec 2025.")

    # ========== 7. L∆ØU MAPPING JSON ==========
    print("üìù Saving ID Mappings...")
    
    # Item Map (Index -> ASIN)
    item_map = {int(i + 1): label for i, label in enumerate(item_indexer.labels)}
    with open(MAP_OUTPUT, "w") as f: json.dump(item_map, f)

    # Category Map (Index -> Name)
    cat_map = {int(i + 1): label for i, label in enumerate(cat_indexer.labels)}
    with open(CAT_MAP_OUTPUT, "w") as f: json.dump(cat_map, f)

    # Item -> Category Map (D√πng ƒë·ªÉ tra c·ª©u nhanh khi streaming)
    # L·∫•y t·ª´ df_indexed (distinct item_id) ƒë·ªÉ nhanh h∆°n
    print("üó∫Ô∏è  Generating Item-Category Map...")
    df_map = df_indexed.select("item_idx", "category_ids").distinct()
    rows = df_map.collect()
    item_cat_map = {str(row["item_idx"]): int(row["category_ids"]) for row in rows}
    
    with open(ITEM_CAT_MAP_OUTPUT, "w") as f: json.dump(item_cat_map, f)

    print(f"‚úÖ Maps saved. Total mapped items: {len(item_cat_map)}")
    print("üéâ DONE ‚Äì ALL DATASETS READY FOR PIPELINE.")

    # Uncache
    df_train_final.unpersist()
    df_test_final.unpersist()
    spark.stop()

if __name__ == "__main__":
    main()