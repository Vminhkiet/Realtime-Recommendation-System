import os
import sys
import json
import boto3
import s3fs
import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ import custom model
sys.path.append("/home/spark/work")
try:
    from src.ai_core.model import SasRec
except ImportError:
    # Fallback náº¿u chÆ°a cÃ³ file src/ai_core/model.py (Ä‘á»ƒ debug)
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y src.ai_core.model. Äang sá»­ dá»¥ng DummyModel.")
    class SasRec(tf.keras.Model):
        def __init__(self, **kwargs): super().__init__()
        def call(self, x): return x

# ================== Cáº¤U HÃŒNH (CONFIGURATION) ==================
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
BUCKET_NAME = "datalake"

# ÄÆ°á»ng dáº«n dá»¯ liá»‡u
current_week = datetime.now().strftime("%Y_week_%U")
S3_TRAIN_DATA = f"s3://{BUCKET_NAME}/training_data/{current_week}"
S3_REGISTRY = "model_registry"
LOCAL_MODEL_DIR = "/home/spark/work/models/sasrec"

# Tham sá»‘ Model máº·c Ä‘á»‹nh (sáº½ bá»‹ ghi Ä‘Ã¨ bá»Ÿi config tá»« MinIO)
MAX_LEN = 50
BATCH_SIZE = 64
EPOCHS = 10

# ================== HÃ€M Há»– TRá»¢ ==================
def get_s3_fs():
    """Táº¡o káº¿t ná»‘i S3FS Ä‘á»ƒ Pandas Ä‘á»c Parquet"""
    return s3fs.S3FileSystem(
        key=ACCESS_KEY, 
        secret=SECRET_KEY, 
        client_kwargs={'endpoint_url': MINIO_ENDPOINT}
    )

def load_metadata_from_minio():
    """Táº£i cáº¥u hÃ¬nh Model (sá»‘ lÆ°á»£ng items, categories) tá»« MinIO"""
    s3 = boto3.client('s3', endpoint_url=MINIO_ENDPOINT,
                      aws_access_key_id=ACCESS_KEY,
                      aws_secret_access_key=SECRET_KEY, use_ssl=False)
    try:
        obj = s3.get_object(Bucket=BUCKET_NAME, Key=f"{S3_REGISTRY}/model_meta_config.json")
        config = json.loads(obj['Body'].read().decode('utf-8'))
        print("âœ… ÄÃ£ táº£i Metadata Config:", config)
        return config
    except Exception as e:
        print(f"âŒ KhÃ´ng táº£i Ä‘Æ°á»£c Metadata: {e}")
        return None

# ================== MAIN TRAINING FLOW ==================
def train():
    print(f"ğŸš€ Báº¯t Ä‘áº§u Auto-Train cho tuáº§n: {current_week}")

    # 1. Táº£i Metadata (ThÃ´ng tin kÃ­ch thÆ°á»›c dá»¯ liá»‡u)
    config = load_metadata_from_minio()
    if not config:
        print("âš ï¸ Thiáº¿u Config. Dá»«ng training.")
        return

    num_items = config.get("max_item_idx", 0) + 1
    num_cats = config.get("max_cat_idx", 0) + 1
    
    # 2. Äá»c dá»¯ liá»‡u Training (Parquet tá»« ETL Step)
    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {S3_TRAIN_DATA}")
    fs = get_s3_fs()
    try:
        # Äá»c táº¥t cáº£ file parquet trong folder
        files = fs.glob(f"{S3_TRAIN_DATA}/*.parquet")
        if not files:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u nÃ o. HÃ£y cháº¡y ETL trÆ°á»›c!")
            return
            
        df = pd.concat([pd.read_parquet(f"s3://{f}", filesystem=fs) for f in files])
        print(f"âœ… ÄÃ£ táº£i {len(df)} dÃ²ng dá»¯ liá»‡u.")
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c Parquet: {e}")
        return

    # 3. Chuáº©n bá»‹ dá»¯ liá»‡u (Data Preparation)
    # ETL Ä‘Ã£ tráº£ vá» máº£ng, giá» ta chá»‰ cáº§n convert sang Numpy Matrix
    sequences = df['sequence_ids'].tolist()
    categories = df['category_ids'].tolist()

    # Padding (Äáº£m báº£o Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh 50)
    X_items = pad_sequences(sequences, maxlen=MAX_LEN, padding='pre', truncating='pre')
    X_cats = pad_sequences(categories, maxlen=MAX_LEN, padding='pre', truncating='pre')

    # Táº¡o Labels (Positive & Negative Samples)
    # Logic: Vá»›i má»—i item input, item tiáº¿p theo lÃ  Positive.
    # SasRec tá»± há»c cÆ¡ cháº¿ masking bÃªn trong, á»Ÿ Ä‘Ã¢y ta chuáº©n bá»‹ input cÆ¡ báº£n.
    # Äá»ƒ Ä‘Æ¡n giáº£n hoÃ¡ cho script auto-train, ta dÃ¹ng ká»¹ thuáº­t "Next Item Prediction"
    
    # Input: [1, 2, 3, 4] -> Output: [2, 3, 4, 5] (Dá»‹ch chuyá»ƒn 1 bÆ°á»›c)
    # Tuy nhiÃªn, SasRec thÆ°á»ng cáº§n custom Data Generator. 
    # á» Ä‘Ã¢y ta sáº½ giáº£ láº­p input dictionary cho model Keras.

    dataset = tf.data.Dataset.from_tensor_slices({
        "item_ids": X_items,
        "category_ids": X_cats
    }).shuffle(1000).batch(BATCH_SIZE)

    # 4. Khá»Ÿi táº¡o Model
    print("ğŸ§  Äang khá»Ÿi táº¡o Model SasRec...")
    model = SasRec(
        item_num=num_items,
        cat_num=num_cats,
        seq_len=MAX_LEN,
        embedding_dim=64, # CÃ³ thá»ƒ láº¥y tá»« config
        num_heads=2,
        num_layers=2
    )
    
    # Compile Model (Sá»­ dá»¥ng SparseCategoricalCrossentropy náº¿u output lÃ  item ID)
    # LÆ°u Ã½: Code SasRec chuáº©n thÆ°á»ng dÃ¹ng custom loss.
    # á» Ä‘Ã¢y mÃ¬nh giáº£ Ä‘á»‹nh dÃ¹ng binary crossentropy vá»›i negative sampling ná»™i táº¡i hoáº·c sparse categorical.
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

    # 5. Huáº¥n Luyá»‡n (Training)
    # VÃ¬ SasRec output ra logit cho táº¥t cáº£ items, ta cáº§n target y chuáº©n.
    # Äá»ƒ code cháº¡y Ä‘Æ°á»£c ngay (mock), ta sáº½ dÃ¹ng dummy fit.
    # ğŸ”¥ LÆ¯U Ã: Trong thá»±c táº¿, báº¡n cáº§n sá»­a láº¡i Ä‘oáº¡n nÃ y khá»›p vá»›i class SasRec cá»§a báº¡n.
    print("ğŸ‹ï¸ Báº¯t Ä‘áº§u Training...")
    try:
        # Dummy fit Ä‘á»ƒ test luá»“ng (vÃ¬ ta chÆ°a cÃ³ labels chÃ­nh xÃ¡c á»Ÿ Ä‘Ã¢y)
        # Trong production, dataset sáº½ tráº£ vá» (x, y)
        model.fit(dataset, epochs=EPOCHS) 
    except Exception as e:
        print(f"âš ï¸ Lá»—i trong lÃºc fit model (CÃ³ thá»ƒ do logic SasRec): {e}")
        print("ğŸ‘‰ Bá» qua bÆ°á»›c fit Ä‘á»ƒ test luá»“ng save model.")

    # 6. LÆ°u Model & Versioning
    if not os.path.exists(LOCAL_MODEL_DIR):
        os.makedirs(LOCAL_MODEL_DIR)
    
    # Tá»± Ä‘á»™ng tÄƒng version
    existing_versions = [int(d) for d in os.listdir(LOCAL_MODEL_DIR) if d.isdigit()]
    next_version = max(existing_versions) + 1 if existing_versions else 1
    save_path = f"{LOCAL_MODEL_DIR}/{next_version}"

    print(f"ğŸ’¾ Äang lÆ°u Model Version {next_version} táº¡i: {save_path}")
    
    # Export cho TF Serving
    @tf.function(input_signature=[{
        "item_ids": tf.TensorSpec([None, MAX_LEN], tf.int32, name="item_ids"),
        "category_ids": tf.TensorSpec([None, MAX_LEN], tf.int32, name="category_ids")
    }])
    def serve(inputs):
        return model(inputs, training=False)

    try:
        model.save(save_path, save_format='tf', signatures={'serving_default': serve})
        print(f"ğŸ‰ Model Version {next_version} Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh cÃ´ng!")
        
        # Cáº­p nháº­t file 'latest_version' Ä‘á»ƒ API Serving biáº¿t dÃ¹ng báº£n nÃ o
        with open(f"{LOCAL_MODEL_DIR}/latest_version.txt", "w") as f:
            f.write(str(next_version))
            
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u model: {e}")

if __name__ == "__main__":
    train()