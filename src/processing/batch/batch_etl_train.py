import os
import sys
import json
import boto3
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType
from datetime import datetime, timedelta

# ================== C·∫§U H√åNH (CONFIGURATION) ==================
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
BUCKET_NAME = "datalake"

# ƒê∆∞·ªùng d·∫´n Input/Output
S3_ROOT_INPUT = f"s3a://{BUCKET_NAME}/topics/processed_clicks"
current_week = datetime.now().strftime("%Y_week_%U")
S3_OUTPUT_TRAIN = f"s3a://{BUCKET_NAME}/training_data/{current_week}"
S3_MODEL_REGISTRY = "model_registry"

# üî• C·∫§U H√åNH L·ªåC NG∆Ø·ªúI D√ôNG (QUALITY FILTER) üî•
# Ch·ªâ nh·ªØng user c√≥ √≠t nh·∫•t bao nhi√™u h√†nh ƒë·ªông m·ªõi ƒë∆∞·ª£c ƒë∆∞a v√†o Train?
MIN_INTERACTIONS = 5 
MAX_SEQUENCE_LENGTH = 50

# ================== H√ÄM H·ªñ TR·ª¢ (HELPER) ==================
def upload_json_to_minio(data_dict, filename):
    """Upload file c·∫•u h√¨nh JSON l√™n MinIO (Model Registry)"""
    s3 = boto3.client('s3', endpoint_url=MINIO_ENDPOINT,
                      aws_access_key_id=ACCESS_KEY,
                      aws_secret_access_key=SECRET_KEY, 
                      use_ssl=False)
    try:
        key = f"{S3_MODEL_REGISTRY}/{filename}"
        s3.put_object(Bucket=BUCKET_NAME, Key=key,
                      Body=json.dumps(data_dict), ContentType='application/json')
        print(f"‚úÖ ƒê√£ upload Config: {key}")
    except Exception as e:
        print(f"‚ùå L·ªói upload {filename}: {e}")

# ================== MAIN PIPELINE ==================
def main():
    # 1. KH·ªûI T·∫†O SPARK
    spark = SparkSession.builder \
        .appName(f"Batch_ETL_{current_week}_PROD") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("ERROR")
    print(f"üöÄ B·∫Øt ƒë·∫ßu Job ETL (Quality Filter >= {MIN_INTERACTIONS}): {current_week}")

    # 2. ƒê·ªåC D·ªÆ LI·ªÜU
    print(f"üìÇ ƒêang qu√©t d·ªØ li·ªáu t·ª´: {S3_ROOT_INPUT}")
    try:
        df_raw = spark.read \
            .option("basePath", S3_ROOT_INPUT) \
            .option("mergeSchema", "true") \
            .option("recursiveFileLookup", "true") \
            .parquet(S3_ROOT_INPUT)
        print(f"üëâ T·ªïng s·ªë file Parquet t√¨m th·∫•y: {len(df_raw.inputFiles())}")
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc d·ªØ li·ªáu: {e}")
        spark.stop()
        return

    # 3. L√ÄM S·∫†CH & CHU·∫®N H√ìA
    if "category_ids" not in df_raw.columns:
        df_raw = df_raw.withColumn("category_ids", F.lit(0).cast(IntegerType()))
    else:
        # ƒê·∫£m b·∫£o t√™n c·ªôt nh·∫•t qu√°n
        if "category_id" in df_raw.columns:
            df_raw = df_raw.withColumnRenamed("category_id", "category_ids")

    # L·ªçc th·ªùi gian (30 ng√†y g·∫ßn nh·∫•t)
    print("üßπ ƒêang l·ªçc r√°c v√† th·ªùi gian...")
    df_clean = df_raw.withColumn("ts_obj", F.to_timestamp(F.col("timestamp")))
    df_filtered = df_clean.filter(
        (F.col("ts_obj").isNotNull()) & 
        (F.col("ts_obj") >= F.date_sub(F.current_timestamp(), 30))
    )

    count = df_filtered.count()
    print(f"‚úÖ S·ªë d√≤ng b·∫£n ghi h·ª£p l·ªá: {count}")
    
    if count == 0:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu.")
        spark.stop()
        return

    # 4. GOM NH√ìM (GROUP BY USER)
    print("üîÑ ƒêang gom nh√≥m h√†nh vi theo User...")
    df_grouped = df_filtered.withColumn("ts_long", F.col("ts_obj").cast("long")).groupBy("user_id").agg(
        F.sort_array(
            F.collect_list(
                F.struct(
                    F.col("ts_long").alias("ts"), 
                    F.col("item_idx").alias("item"), 
                    F.col("category_ids").alias("cat")
                )
            )
        ).alias("events")
    )

    # --- DEBUG: HI·ªÇN TH·ªä TH·ªêNG K√ä TR∆Ø·ªöC KHI L·ªåC ---
    print("\nüìä --- TH·ªêNG K√ä D·ªÆ LI·ªÜU G·ªêC (Tr∆∞·ªõc khi c·∫Øt/l·ªçc) ---")
    df_grouped.select(
        F.col("user_id"), 
        F.size(F.col("events")).alias("total_interactions")
    ).orderBy(F.col("total_interactions").desc()).show(5, truncate=False)
    # ------------------------------------------------

    # 5. T·∫†O DATASET & L·ªåC CH·∫§T L∆Ø·ª¢NG (QUALITY FILTER)
    df_final = df_grouped.select(
        F.col("user_id"),
        F.slice(F.col("events.item"), -MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH).alias("sequence_ids"),
        F.slice(F.col("events.cat"), -MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH).alias("category_ids"),
        F.element_at(F.col("events.ts"), -1).alias("last_timestamp")
    )

    # üî• QUAN TR·ªåNG: Ch·ªâ gi·ªØ l·∫°i User c√≥ s·ªë l∆∞·ª£ng t∆∞∆°ng t√°c >= MIN_INTERACTIONS
    df_final = df_final.filter(F.size(F.col("sequence_ids")) >= MIN_INTERACTIONS)

    final_count = df_final.count()
    print(f"üìâ K·∫øt qu·∫£ sau khi l·ªçc (User >= {MIN_INTERACTIONS} items): {final_count} User Sequences.")

    if final_count > 0:
        # 6. L∆ØU FILE PARQUET
        print(f"üíæ ƒêang ghi d·ªØ li·ªáu v√†o: {S3_OUTPUT_TRAIN}")
        df_final.coalesce(1).write.mode("overwrite").parquet(S3_OUTPUT_TRAIN)

        # 7. METADATA
        print("üó∫Ô∏è ƒêang t·∫°o Metadata Config...")
        max_item = df_filtered.agg(F.max("item_idx")).collect()[0][0]
        max_cat = df_filtered.agg(F.max("category_ids")).collect()[0][0]
        
        df_maps = df_filtered.select("item_idx", "category_ids").distinct()
        item_cat_map = {str(r["item_idx"]): int(r["category_ids"]) for r in df_maps.collect()}

        meta_config = {
            "max_item_idx": int(max_item) if max_item else 0,
            "max_cat_idx": int(max_cat) if max_cat else 0,
            "train_path": S3_OUTPUT_TRAIN,
            "updated_at": datetime.now().isoformat(),
            "min_interactions": MIN_INTERACTIONS
        }

        upload_json_to_minio(item_cat_map, "item_category.json")
        upload_json_to_minio(meta_config, "model_meta_config.json")
        
        print("üéâ Batch ETL Ho√†n t·∫•t th√†nh c√¥ng!")
    else:
        print(f"‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng c√≥ User n√†o c√≥ ƒë·ªß {MIN_INTERACTIONS} h√†nh ƒë·ªông. H√£y ch·∫°y th√™m 'make sim'!")

    spark.stop()

if __name__ == "__main__":
    main()