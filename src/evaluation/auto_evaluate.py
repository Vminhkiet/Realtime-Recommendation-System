import os
import sys
import json
import s3fs
import pickle
import numpy as np
import pandas as pd
import tensorflow as tf
from tqdm import tqdm
from datetime import datetime

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ load custom model class
sys.path.append("/home/spark/work")

# =======================
# Cáº¤U HÃŒNH Há»† THá»NG
# =======================
MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
BUCKET_NAME = "datalake"

# ÄÆ°á»ng dáº«n Model & Data
MODEL_BASE_DIR = "/home/spark/work/models/sasrec"
S3_MODEL_REGISTRY = "model_registry"
current_week = datetime.now().strftime("%Y_week_%U")
S3_DATA_PATH = f"s3://{BUCKET_NAME}/training_data/{current_week}"

# Cáº¥u hÃ¬nh Ä‘Ã¡nh giÃ¡
MAX_LEN = 50
NUM_NEG_TEST = 99  # ÄÃ¡nh giÃ¡ 1 Positive vs 99 Negatives
SEED = 42

np.random.seed(SEED)
tf.random.set_seed(SEED)

# =======================
# HÃ€M Há»– TRá»¢
# =======================
def get_latest_model_version():
    """TÃ¬m phiÃªn báº£n model má»›i nháº¥t"""
    if not os.path.exists(MODEL_BASE_DIR):
        return None
    versions = [int(d) for d in os.listdir(MODEL_BASE_DIR) if d.isdigit()]
    return max(versions) if versions else None

def get_metadata():
    """Táº£i sá»‘ lÆ°á»£ng Item tá»« MinIO Ä‘á»ƒ biáº¿t khoáº£ng random negative"""
    s3 = boto3.client('s3', endpoint_url=MINIO_ENDPOINT,
                      aws_access_key_id=ACCESS_KEY,
                      aws_secret_access_key=SECRET_KEY, use_ssl=False)
    try:
        obj = s3.get_object(Bucket=BUCKET_NAME, Key=f"{S3_MODEL_REGISTRY}/model_meta_config.json")
        return json.loads(obj['Body'].read().decode('utf-8'))
    except:
        return {"max_item_idx": 1000} # Fallback

def load_data_from_minio():
    """Äá»c trá»±c tiáº¿p file Parquet tá»« MinIO"""
    print(f"ğŸ“‚ Äang táº£i dá»¯ liá»‡u Test tá»«: {S3_DATA_PATH}")
    fs = s3fs.S3FileSystem(key=ACCESS_KEY, secret=SECRET_KEY, client_kwargs={'endpoint_url': MINIO_ENDPOINT})
    try:
        files = fs.glob(f"{S3_DATA_PATH}/*.parquet")
        if not files: return None
        return pd.concat([pd.read_parquet(f"s3://{f}", filesystem=fs) for f in files])
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c data: {e}")
        return None

def pad_sequence(seq, max_len):
    """Padding sequence cho Ä‘Ãºng chuáº©n input model"""
    seq = list(seq)[-max_len:] # Cáº¯t náº¿u dÃ i quÃ¡
    pad_len = max_len - len(seq)
    return list(seq) + [0] * pad_len # Pad sau (hoáº·c trÆ°á»›c tuá»³ config train)

# =======================
# MAIN EVALUATION LOOP
# =======================
def main():
    print("\nğŸ“Š Báº®T Äáº¦U ÄÃNH GIÃ (AUTO EVALUATE) - LEAVE ONE OUT")
    
    # 1. Load Model
    ver = get_latest_model_version()
    if not ver:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y model nÃ o.")
        return
    model_path = f"{MODEL_BASE_DIR}/{ver}"
    print(f"ğŸ”„ Load model version {ver} tá»« {model_path}...")
    
    try:
        # Load trá»n váº¹n Keras Model Ä‘á»ƒ truy cáº­p layer embedding bÃªn trong
        model = tf.keras.models.load_model(model_path)
    except:
        print("âš ï¸ KhÃ´ng load Ä‘Æ°á»£c dáº¡ng Keras Model (cÃ³ thá»ƒ do format SavedModel).")
        print("â„¹ï¸ Chuyá»ƒn sang cháº¿ Ä‘á»™ Serving Signature (cháº­m hÆ¡n nhÆ°ng an toÃ n).")
        model = tf.saved_model.load(model_path)
    
    # 2. Load Data & Metadata
    df = load_data_from_minio()
    if df is None or df.empty:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ test.")
        return
        
    meta = get_metadata()
    vocab_size = meta.get("max_item_idx", 1000)
    
    hits_10, ndcgs_10 = 0, 0
    num_users = len(df)
    
    print(f"ğŸ‘¥ Tá»•ng sá»‘ User test: {num_users}")
    print(f"ğŸ“¦ Vocab Size: {vocab_size}")

    # 3. VÃ²ng láº·p Ä‘Ã¡nh giÃ¡
    # Chiáº¿n thuáº­t: Láº¥y list item cá»§a user. 
    # - Input: [Item 1, ..., Item N-1]
    # - Label: Item N (CÃ¡i cuá»‘i cÃ¹ng user Ä‘Ã£ click)
    # - Negative: 99 cÃ¡i user chÆ°a xem (hoáº·c random)
    
    for _, row in tqdm(df.iterrows(), total=num_users, desc="Evaluating"):
        full_seq = row['sequence_ids']
        if len(full_seq) < 2: continue # KhÃ´ng Ä‘á»§ Ä‘á»ƒ test
        
        # TÃ¡ch Train/Test (Leave-one-out)
        target_item = full_seq[-1]      # Ground Truth
        input_seq = full_seq[:-1]       # History Ä‘á»ƒ dá»± Ä‘oÃ¡n
        
        # Padding
        padded_input = pad_sequence(input_seq, MAX_LEN)
        
        # Negative Sampling
        test_items = [target_item]
        while len(test_items) < NUM_NEG_TEST + 1:
            neg = np.random.randint(1, vocab_size + 1)
            if neg not in full_seq: # Äáº£m báº£o khÃ´ng trÃ¹ng lá»‹ch sá»­
                test_items.append(neg)
        
        # Chuyá»ƒn thÃ nh Tensor
        input_tensor = tf.constant([padded_input] * 100, dtype=tf.int32) # Batch size 100 (1 pos + 99 neg)
        
        # Giáº£ láº­p category (náº¿u cÃ³)
        cat_tensor = tf.zeros_like(input_tensor) 
        
        # --- Dá»° ÄOÃN ---
        # CÃ¡ch nÃ y hÆ¡i "cá»¥c sÃºc" (predict 100 láº§n) nhÆ°ng tÆ°Æ¡ng thÃ­ch má»i model
        # Äá»ƒ tá»‘i Æ°u, nÃªn láº¥y User Embedding 1 láº§n rá»“i nhÃ¢n vá»›i 100 Item Embedding
        # NhÆ°ng á»Ÿ Ä‘Ã¢y ta dÃ¹ng hÃ m predict() cho Ä‘Æ¡n giáº£n logic
        
        # Dá»± Ä‘oÃ¡n cho 100 trÆ°á»ng há»£p: (User History + Candidate Item)
        # VÃ¬ model SasRec thÆ°á»ng chá»‰ nháº­n input sequence, ta cáº§n trick nháº¹:
        # Append candidate item vÃ o cuá»‘i chuá»—i input Ä‘á»ƒ xem score cá»§a nÃ³
        
        # TRICK: Ta dÃ¹ng User Embedding tá»« model Ä‘á»ƒ tÃ­nh Dot Product vá»›i 100 Items
        # YÃªu cáº§u model pháº£i expose Ä‘Æ°á»£c Embedding Layer. 
        # Náº¿u dÃ¹ng SavedModel, ta gá»i hÃ m serve() Ä‘á»ƒ láº¥y logits
        
        try:
            # Gá»i model Ä‘á»ƒ láº¥y Logits cho Táº¤T Cáº¢ items (cÃ¡ch nhanh nháº¥t)
            # Input: (1, 50)
            single_input = tf.constant([padded_input], dtype=tf.int32)
            single_cat = tf.constant([pad_sequence([1]*len(input_seq), MAX_LEN)], dtype=tf.int32)
            
            logits = model(
                {"item_ids": single_input, "category_ids": single_cat}, 
                training=False
            ) 
            # Logits shape: (1, 50, Vocab_Size) -> Láº¥y bÆ°á»›c cuá»‘i cÃ¹ng
            # Náº¿u model tráº£ vá» dict
            if isinstance(logits, dict):
                # Tuá»³ vÃ o output model cá»§a báº¡n (thÆ°á»ng lÃ  'output_1' hoáº·c tÃªn layer)
                # Giáº£ sá»­ model tráº£ vá» sequence output
                seq_output = list(logits.values())[0] # (1, 50, Hidden)
                scores = seq_output[0, -1, :] # Vector Ä‘iá»ƒm sá»‘ cho toÃ n bá»™ Vocab
            else:
                # Náº¿u model tráº£ tháº³ng logit (1, 50, Vocab)
                scores = logits[0, -1, :] 

            # Láº¥y Ä‘iá»ƒm cá»§a 100 item test
            test_scores = tf.gather(scores, test_items).numpy()
            
            # Ranking
            # Item Ä‘áº§u tiÃªn (index 0) lÃ  Target. Äáº¿m xem cÃ³ bao nhiÃªu tháº±ng Ä‘iá»ƒm cao hÆ¡n nÃ³
            rank = np.sum(test_scores > test_scores[0]) + 1
            
            if rank <= 10:
                hits_10 += 1
                ndcgs_10 += 1.0 / np.log2(rank + 1)
                
        except Exception as e:
            # Skip náº¿u lá»—i dimension (do config chÆ°a khá»›p)
            pass

    # =======================
    # BÃO CÃO Káº¾T QUáº¢
    # =======================
    hr = hits_10 / num_users
    ndcg = ndcgs_10 / num_users
    
    print("\n" + "="*50)
    print(f"ğŸ† Káº¾T QUáº¢ TEST (Model v{ver})")
    print("="*50)
    print(f"ğŸ‘¥ Users Evaluated : {num_users}")
    print(f"ğŸ¯ Hit Rate @10    : {hr:.4f}  ({hr*100:.2f}%)")
    print(f"â­ NDCG @10        : {ndcg:.4f}")
    print("="*50 + "\n")

if __name__ == "__main__":
    main()