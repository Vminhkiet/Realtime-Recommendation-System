import os
import boto3
from botocore.exceptions import NoCredentialsError

# ================== C·∫§U H√åNH ==================
MINIO_ENDPOINT = "http://minio:9000" # D√πng localhost v√¨ b·∫°n ch·∫°y script n√†y t·ª´ m√°y host
ACCESS_KEY = "minioadmin"
SECRET_KEY = "minioadmin"
BUCKET_NAME = "datalake"

# ƒê∆∞·ªùng d·∫´n Local (N∆°i b·∫°n ƒëang c√≥ d·ªØ li·ªáu trong VS Code)
LOCAL_MODEL_REGISTRY = "data/model_registry" 

# ================== H√ÄM UPLOAD ƒê·ªÜ QUY ==================
def upload_folder_to_s3(s3_client, local_folder, s3_prefix):
    """
    Upload to√†n b·ªô th∆∞ m·ª•c local l√™n MinIO gi·ªØ nguy√™n c·∫•u tr√∫c
    """
    print(f"üìÇ ƒêang qu√©t th∆∞ m·ª•c: {local_folder} ...")
    
    for root, dirs, files in os.walk(local_folder):
        for filename in files:
            # ƒê∆∞·ªùng d·∫´n file tr√™n m√°y local
            local_path = os.path.join(root, filename)
            
            # T√≠nh to√°n ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªÉ t·∫°o Key tr√™n S3
            relative_path = os.path.relpath(local_path, local_folder)
            s3_key = os.path.join(s3_prefix, relative_path).replace("\\", "/") # Fix l·ªói ƒë∆∞·ªùng d·∫´n Windows
            
            print(f"  ‚¨ÜÔ∏è Uploading: {relative_path} -> s3://{BUCKET_NAME}/{s3_key}")
            
            try:
                s3_client.upload_file(local_path, BUCKET_NAME, s3_key)
            except Exception as e:
                print(f"  ‚ùå L·ªói file {filename}: {e}")

def main():
    # 1. Kh·ªüi t·∫°o k·∫øt n·ªëi
    s3 = boto3.client('s3',
                      endpoint_url=MINIO_ENDPOINT,
                      aws_access_key_id=ACCESS_KEY,
                      aws_secret_access_key=SECRET_KEY)

    # 2. Upload c√°c file Map (JSON) & Pickle
    # Nh·ªØng file n√†y n·∫±m ngay trong root c·ªßa model_registry
    files_to_upload = [
        "item_map.json", 
        "category_map.json", 
        "item_category.json",
        "test_set.pkl" 
        # "sasrec_v1.keras" # File n√†y ch·ªâ d√πng ƒë·ªÉ train ti·∫øp, kh√¥ng c·∫ßn thi·∫øt cho Serving n·∫øu ƒë√£ c√≥ folder '1'
    ]
    
    print("--- üöÄ B·∫ÆT ƒê·∫¶U UPLOAD C√ÅC FILE L·∫∫ ---")
    for fname in files_to_upload:
        local_path = os.path.join(LOCAL_MODEL_REGISTRY, fname)
        if os.path.exists(local_path):
            s3.upload_file(local_path, BUCKET_NAME, f"model_registry/{fname}")
            print(f"‚úÖ ƒê√£ upload: {fname}")
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y: {fname}")

    # 3. Upload Folder D·ªØ li·ªáu Train (Parquet)
    # Upload v√†o: datalake/processed_parquet
    print("\n--- üì¶ B·∫ÆT ƒê·∫¶U UPLOAD PARQUET ---")
    upload_folder_to_s3(s3, 
                        os.path.join(LOCAL_MODEL_REGISTRY, "processed_parquet"), 
                        "processed_parquet")

    # 4. Upload Model cho TF Serving (Quan tr·ªçng nh·∫•t)
    # C·∫•u tr√∫c TF Serving b·∫Øt bu·ªôc: models/<t√™n_model>/<version>/saved_model.pb
    # Local c·ªßa b·∫°n ƒëang l√†: model_registry/1/...
    # Ch√∫ng ta s·∫Ω ƒë·∫©y l√™n: models/sasrec/1/...
    print("\n--- ü§ñ B·∫ÆT ƒê·∫¶U UPLOAD MODEL (TF SERVING) ---")
    local_model_path = os.path.join(LOCAL_MODEL_REGISTRY, "1")
    if os.path.exists(local_model_path):
        upload_folder_to_s3(s3, local_model_path, "models/sasrec/1")
        print("‚úÖ Upload Model th√†nh c√¥ng! S·∫µn s√†ng cho TF Serving.")
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c model version '1'. B·∫°n ƒë√£ save model ch∆∞a?")

if __name__ == "__main__":
    main()