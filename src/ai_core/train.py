import os
import json
import random
import pickle
import pandas as pd
import tensorflow as tf
from collections import Counter
from model import SasRec

# --- C·∫§U H√åNH ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))

PARQUET_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/processed_parquet')
MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')
CAT_MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/category_map.json')
MODEL_SAVE_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/sasrec_v1.keras')
TEST_SET_SAVE_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/test_set.pkl')
LOG_PATH = os.path.join(PROJECT_ROOT, 'data/training_history.json')

# --- HYPERPARAMETERS T·ªêI ∆ØU (GRANDMASTER MODE) ---
MAX_LEN = 50
BATCH_SIZE = 32
EPOCHS = 100
EMBED_DIM = 128
STEP_SIZE = 1
MIN_INTERACTIONS = 5

# üî• H√†m l·∫•y h√†ng Hot (Hard Negative)
def get_popular_items(sequences, top_k=2000):
    print("üî• ƒêang t√≠nh to√°n ƒë·ªô ph·ªï bi·∫øn c·ªßa s·∫£n ph·∫©m (Hard Negatives)...")
    all_items = [item for seq in sequences for item in seq]
    counter = Counter(all_items)
    popular_items = [item for item, count in counter.most_common(top_k)]
    return popular_items

def load_data():
    print("üì• ƒêang load d·ªØ li·ªáu t·ª´ Parquet...")
    if not os.path.exists(PARQUET_PATH):
        raise FileNotFoundError("‚ùå Ch∆∞a ch·∫°y spark_process.py!")
    
    df = pd.read_parquet(PARQUET_PATH)
    item_seqs = df['sequence_ids'].tolist()

    # üî• Gi·∫£m dung l∆∞·ª£ng data cho vi·ªác ph√°t tri·ªÉn nhanh (Ch·ªâ d√πng 5000 users)
    df = df.sample(n=min(5000, len(df)), random_state=42) 
    print(f"üìâ ƒê√£ l·∫•y m·∫´u simulation: {len(df)} users")
    
    if 'category_ids' not in df.columns:
        raise ValueError("‚ùå Thi·∫øu c·ªôt category_ids. H√£y ch·∫°y l·∫°i spark_process.py!")
        
    cat_seqs = df['category_ids'].tolist()
    
    with open(MAP_PATH, 'r') as f:
        item_map = json.load(f)
        vocab_size = len(item_map)

    if os.path.exists(CAT_MAP_PATH):
        with open(CAT_MAP_PATH, 'r') as f:
            cat_map = json.load(f)
            num_categories = len(cat_map)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y category_map.json. T·ª± t√≠nh to√°n...")
        all_cats = set()
        for seq in cat_seqs: all_cats.update(seq)
        num_categories = max(all_cats)

    return item_seqs, cat_seqs, vocab_size, num_categories

def create_dataset(item_seqs, cat_seqs, max_len, num_items, num_cats, popular_items):
    def generator():
        for item_seq, cat_seq in zip(item_seqs, cat_seqs):
            seq_len = len(item_seq)
            
            if seq_len <= max_len + 1:
                item_windows = [item_seq]
                cat_windows = [cat_seq]
            else:
                item_windows = [item_seq[i : i + max_len + 1] for i in range(0, seq_len - max_len, STEP_SIZE)]
                cat_windows = [cat_seq[i : i + max_len + 1] for i in range(0, seq_len - max_len, STEP_SIZE)]

            for item_window, cat_window in zip(item_windows, cat_windows):
                if len(item_window) < 2: continue
                
                input_item = list(item_window[:-1])
                pos_item = list(item_window[1:])
                input_cat = list(cat_window[:-1])
                
                # üî• K·ª∏ THU·∫¨T DATA AUGMENTATION: MASKING (Che m·∫Øt) üî•
                # T·ª∑ l·ªá 20% s·ªë m·∫´u s·∫Ω b·ªã che m·∫•t 1 m√≥n ƒë·ªì ng·∫´u nhi√™n
                # Gi√∫p Model kh√¥ng b·ªã h·ªçc t·ªß, ph·∫£i suy lu·∫≠n t·ª´ ng·ªØ c·∫£nh
                if random.random() < 0.2: 
                    mask_idx = random.randint(0, len(input_item) - 1)
                    input_item[mask_idx] = 0 # G√°n v·ªÅ 0 (Xem nh∆∞ ch∆∞a t·ª´ng mua)
                    # Kh√¥ng mask category ƒë·ªÉ model v·∫´n c√≤n manh m·ªëi suy lu·∫≠n
                
                pad_len = max_len - len(input_item)
                
                input_ids = input_item + [0] * pad_len
                cat_ids = input_cat + [0] * pad_len
                pos_ids = pos_item + [0] * pad_len
                mask = [True] * len(input_item) + [False] * pad_len
                
                # --- Hard Negative Sampling ---
                neg_ids = []
                win_set = set(item_window)
                for _ in range(len(input_item)):
                    if random.random() < 0.5:
                        neg = random.choice(popular_items)
                    else:
                        neg = random.randint(1, num_items)
                    while neg in win_set: 
                        neg = random.randint(1, num_items)
                    neg_ids.append(neg)
                neg_ids = neg_ids + [0] * pad_len
                
                yield (
                    {
                        "item_ids": input_ids, 
                        "category_ids": cat_ids, 
                        "padding_mask": mask
                    },
                    {
                        "positive_sequence": pos_ids, 
                        "negative_sequence": neg_ids
                    }
                )

    return tf.data.Dataset.from_generator(
        generator,
        output_signature=(
            {
                "item_ids": tf.TensorSpec(shape=(max_len,), dtype=tf.int32),
                "category_ids": tf.TensorSpec(shape=(max_len,), dtype=tf.int32),
                "padding_mask": tf.TensorSpec(shape=(max_len,), dtype=tf.bool)
            },
            {
                "positive_sequence": tf.TensorSpec(shape=(max_len,), dtype=tf.int32), 
                "negative_sequence": tf.TensorSpec(shape=(max_len,), dtype=tf.int32)
            }
        )
    ).cache().shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

def main():
    # 1. Load Data
    item_seqs, cat_seqs, vocab_size, num_categories = load_data()
    print(f"üìä T·ªïng User: {len(item_seqs)}")
    print(f"üìä T·ªïng Item: {vocab_size} | T·ªïng Category: {num_categories}")

    # 2. L·ªçc D·ªØ Li·ªáu
    print(f"üßπ ƒêang l·ªçc user mua d∆∞·ªõi {MIN_INTERACTIONS} m√≥n...")
    filtered_data = [(s, c) for s, c in zip(item_seqs, cat_seqs) if len(s) >= MIN_INTERACTIONS]
    
    if not filtered_data: raise ValueError("‚ùå L·ªói: D·ªØ li·ªáu qu√° √≠t!")

    item_seqs = [x[0] for x in filtered_data]
    cat_seqs = [x[1] for x in filtered_data]
    print(f"üìâ User c√≤n l·∫°i: {len(item_seqs)}")

    # 3. T√≠nh Popular Items
    popular_items = get_popular_items(item_seqs, top_k=2000)

    # 4. Split Train/Test
    print("‚úÇÔ∏è Leave-One-Out Split...")
    train_items, train_cats = [], []
    test_set = []

    for i in range(len(item_seqs)):
        seq = item_seqs[i]
        cat = cat_seqs[i]
        
        target_item = seq[-1]
        train_items.append(seq[:-1])
        train_cats.append(cat[:-1]) 
        
        test_set.append({
            "input_items": seq[:-1],
            "input_cats": cat[:-1],
            "label": target_item
        })

    with open(TEST_SET_SAVE_PATH, 'wb') as f: pickle.dump(test_set, f)
    print(f"‚úÖ ƒê√£ l∆∞u {len(test_set)} m·∫´u Test.")

    # 5. T·∫°o Dataset
    train_ds = create_dataset(train_items, train_cats, MAX_LEN, vocab_size, num_categories, popular_items)

    # 6. Train Model (C·∫•u h√¨nh Grandmaster)
    print("üöÄ B·∫Øt ƒë·∫ßu Training (Augmentation + Cosine Decay)...")
    
    model = SasRec(
        vocabulary_size=vocab_size + 1, 
        category_size=num_categories + 1,
        num_layers=2, 
        num_heads=4,
        hidden_dim=EMBED_DIM, 
        dropout=0.4, # ‚¨ÜÔ∏è TƒÉng Dropout l√™n 0.4 ƒë·ªÉ √©p model h·ªçc kh√≥ h∆°n
        max_sequence_length=MAX_LEN
    )
    
    # üî• OPTIMIZER: D√πng CosineDecayRestarts thay v√¨ gi·∫£m ƒë·ªÅu
    # Gi√∫p model tho√°t kh·ªèi c√°c ƒëi·ªÉm c·ª±c ti·ªÉu c·ª•c b·ªô (Local Minima)
    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(
        initial_learning_rate=0.001,
        first_decay_steps=1000,
        t_mul=2.0,
        m_mul=0.9,
        alpha=1e-6
    )
    
    optimizer = tf.keras.optimizers.AdamW(
        learning_rate=lr_schedule, 
        weight_decay=0.05 # ‚¨ÜÔ∏è TƒÉng Weight Decay l√™n 0.05 ƒë·ªÉ ph·∫°t n·∫∑ng vi·ªác h·ªçc t·ªß
    )

    callbacks = [
        # B·ªè ReduceLROnPlateau v√¨ ƒë√£ c√≥ Scheduler x·ªãn ·ªü tr√™n
        tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_SAVE_PATH, save_best_only=True, monitor='loss', mode='min', verbose=1),
        tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True) # TƒÉng ki√™n nh·∫´n l√™n 15
    ]

    model.compile(optimizer=optimizer)
    history = model.fit(train_ds, epochs=EPOCHS, callbacks=callbacks)

    print(f"üìä ƒêang l∆∞u log training v√†o: {LOG_PATH}")
    try:
        with open(LOG_PATH, 'w') as f: json.dump(history.history, f)
    except Exception as e: print(f"‚ö†Ô∏è L·ªói l∆∞u log: {e}")

if __name__ == "__main__":
    main()