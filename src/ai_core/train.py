import pickle
import numpy as np
import tensorflow as tf
import random
import os
from model import SasRec  # Import class SasRec t·ª´ file tr√™n

# --- C·∫§U H√åNH ---
# S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·ªÉ tr√°nh l·ªói FileNotFoundError
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))

DATA_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/dataset.pkl')
MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.pkl')
MODEL_SAVE_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/sasrec_v1.keras')

# Hyperparameters
MAX_LEN = 50
BATCH_SIZE = 32  # Gi·∫£m xu·ªëng 64 cho nh·∫π m√°y
EMBED_DIM = 32
EPOCHS = 5      # C√≥ th·ªÉ tƒÉng l√™n 20-50 n·∫øu Loss v·∫´n cao

def load_data():
    print("üì• ƒêang load d·ªØ li·ªáu...")
    if not os.path.exists(DATA_PATH):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file {DATA_PATH}. H√£y ch·∫°y data_process.py tr∆∞·ªõc!")
        
    with open(DATA_PATH, 'rb') as f:
        sequences = pickle.load(f)
    with open(MAP_PATH, 'rb') as f:
        item2id, _ = pickle.load(f)
    return sequences, len(item2id)

def prepare_training_data(sequences, max_len, num_items):
    def generator():
        for seq in sequences:
            # 1. C·∫Øt chu·ªói
            seq = seq[-(max_len + 1):]
            if len(seq) < 2: continue

            # 2. T√°ch Input/Target
            input_seq = seq[:-1]
            pos_seq = seq[1:]
            
            # 3. Padding
            pad_len = max_len - len(input_seq)
            input_ids = [0] * pad_len + input_seq
            pos_ids = [0] * pad_len + pos_seq
            mask = [False] * pad_len + [True] * len(input_seq)
            
            # 4. Negative Sampling
            neg_ids = []
            seq_set = set(seq)
            for _ in range(len(input_seq)):
                neg = random.randint(1, num_items)
                while neg in seq_set: 
                    neg = random.randint(1, num_items)
                neg_ids.append(neg)
            neg_ids = [0] * pad_len + neg_ids

            yield (
                {"item_ids": input_ids, "padding_mask": mask},
                {"positive_sequence": pos_ids, "negative_sequence": neg_ids}
            )

    return tf.data.Dataset.from_generator(
        generator,
        output_signature=(
            {
                "item_ids": tf.TensorSpec(shape=(max_len,), dtype=tf.int32),
                "padding_mask": tf.TensorSpec(shape=(max_len,), dtype=tf.bool)
            },
            {
                "positive_sequence": tf.TensorSpec(shape=(max_len,), dtype=tf.int32),
                "negative_sequence": tf.TensorSpec(shape=(max_len,), dtype=tf.int32)
            }
        )
    ).shuffle(1000).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)

def main():
    # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
    sequences, num_items = load_data()
    print(f"‚úÖ T·ªïng s·ªë user sessions: {len(sequences)}")
    print(f"‚úÖ T·ªïng s·ªë s·∫£n ph·∫©m: {num_items}")
    
    train_ds = prepare_training_data(sequences, MAX_LEN, num_items)

    # 2. Kh·ªüi t·∫°o Model
    # +1 v√†o vocab size cho padding (0)
    model = SasRec(
        vocabulary_size=num_items + 1, 
        num_layers=2,
        num_heads=2,
        hidden_dim=EMBED_DIM,
        dropout=0.1,
        max_sequence_length=MAX_LEN
    )

    # 3. Compile
    # AdamW th∆∞·ªùng h·ªôi t·ª• t·ªët h∆°n Adam th∆∞·ªùng cho Transformer
    model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001))

    # 4. Train
    print("üöÄ B·∫Øt ƒë·∫ßu Training...")
    history = model.fit(train_ds, epochs=EPOCHS)

    # 5. L∆∞u Model (Gi·ªù s·∫Ω kh√¥ng c√≤n l·ªói NotImplementedError n·ªØa)
    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)
    model.save(MODEL_SAVE_PATH)
    print(f"üíæ ƒê√£ l∆∞u model th√†nh c√¥ng t·∫°i: {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    main()