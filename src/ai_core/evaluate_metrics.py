import os
import json
import s3fs
import numpy as np
import pandas as pd
import tensorflow as tf
from tqdm import tqdm
from datetime import datetime
import math
import re

# --- C·∫§U H√åNH ---
MINIO_CONF = {
    "key": os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
    "secret": os.getenv("MINIO_SECRET_KEY", "minioadmin"),
    "client_kwargs": {"endpoint_url": os.getenv("MINIO_ENDPOINT", "http://minio:9000")}
}
BUCKET = "datalake"
# Config path gi·ªØ nguy√™n s3a c≈©ng ƒë∆∞·ª£c, code x·ª≠ l√Ω b√™n d∆∞·ªõi
META_CONFIG_PATH = f"s3://{BUCKET}/model_registry/model_meta_config.json"

MAX_LEN = 50
TOP_K = 10
QUALITY_THRESHOLD = 0.01

try:
    from model import SasRec
except ImportError:
    import sys
    sys.path.append(os.getcwd())
    from src.ai_core.model import SasRec

def get_fs():
    return s3fs.S3FileSystem(**MINIO_CONF)

def force_s3_protocol(path):
    """
    [NUCLEAR OPTION] Ph∆∞∆°ng ph√°p 'C·∫Øt v√† Gh√©p'.
    B·∫•t ch·∫•p ƒë·∫ßu v√†o l√† s3a://, S3A://, hay d√≠nh r√°c,
    n√≥ s·∫Ω √©p v·ªÅ s3:// chu·∫©n.
    """
    if not path: return ""
    
    # 1. Clean c∆° b·∫£n
    s = str(path).strip().strip('"').strip("'")
    
    # 2. N·∫øu c√≥ ch·ª©a '://' -> C·∫Øt ƒë√¥i v√† gh√©p l·∫°i
    if "://" in s:
        # V√≠ d·ª•: "s3a://bucket/file" -> parts = ["s3a", "bucket/file"]
        parts = s.split("://", 1) 
        clean_suffix = parts[1]
        new_path = f"s3://{clean_suffix}"
        
        # Ch·ªâ in log n·∫øu c√≥ s·ª± thay ƒë·ªïi ƒë·ªÉ debug
        if new_path != s:
            print(f"üîß Force Convert: '{s}' \n             -> '{new_path}'")
        return new_path
    
    # N·∫øu kh√¥ng c√≥ protocol (ƒë∆∞·ªùng d·∫´n local), gi·ªØ nguy√™n
    return s

def load_config():
    fs = get_fs()
    # Config path c≈©ng c·∫ßn force s3
    config_path = force_s3_protocol(META_CONFIG_PATH)
    
    if fs.exists(config_path):
        with fs.open(config_path, 'r') as f:
            return json.load(f)
    return {}

def update_config_metrics(metrics, status):
    fs = get_fs()
    config_path = force_s3_protocol(META_CONFIG_PATH)
    try:
        with fs.open(config_path, 'r') as f:
            config = json.load(f)
    except:
        config = {}

    config["latest_eval_metrics"] = metrics
    config["eval_status"] = status
    config["last_eval_at"] = datetime.now().isoformat()
    
    with fs.open(config_path, 'w') as f:
        json.dump(config, f, indent=4)
    print(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t metrics v√†o Config.")

def get_item_embedding_matrix(model):
    try:
        return model.item_embedding.embeddings.numpy()
    except:
        try:
            return model.get_layer("item_embedding").get_weights()[0]
        except:
            return model.layers[0].get_weights()[0]

def pad_sequence(seq, max_len):
    seq = seq[-max_len:]
    pad_len = max_len - len(seq)
    seq_padded = seq + [0] * pad_len
    mask = [True] * len(seq) + [False] * pad_len
    return np.array(seq_padded, dtype=np.int32), np.array(mask, dtype=bool)

def calculate_metrics(ranked_ids, target_item, k=10):
    if target_item not in ranked_ids[:k]:
        return 0.0, 0.0
    hr = 1.0
    rank_index = np.where(ranked_ids[:k] == target_item)[0][0]
    ndcg = 1.0 / math.log2(rank_index + 2)
    return hr, ndcg

def main():
    print("="*60)
    print("üöÄ B·∫ÆT ƒê·∫¶U QUY TR√åNH ƒê√ÅNH GI√Å (NUCLEAR FIX)")
    print("="*60)

    # 1. ƒê·ªçc Config
    config = load_config()
    raw_model_path = config.get("latest_model_path")
    raw_test_path = config.get("test_path")
    
    if not raw_model_path or not raw_test_path:
        print("‚ùå Thi·∫øu path trong config. H·ªßy."); return

    # [√ÅP D·ª§NG H√ÄM FIX M·ªöI]
    model_path = raw_model_path
    test_path = force_s3_protocol(raw_test_path)

    print(f"üéØ Model (Final): {model_path}")
    print(f"üéØ Data  (Final): {test_path}")

    # 2. Load Model
    fs = get_fs()
    local_model_path = "/tmp/current_eval_model.keras"
    
    try:
        if fs.exists(model_path):
            print("‚¨áÔ∏è  ƒêang t·∫£i model...")
            fs.get(model_path, local_model_path)
            model = tf.keras.models.load_model(local_model_path, custom_objects={'SasRec': SasRec}, compile=False)
            print("‚úÖ Load Model th√†nh c√¥ng!")
        else:
            print(f"‚ùå KH√îNG T√åM TH·∫§Y FILE: {model_path}")
            print(f"   (Path g·ªëc trong config l√†: {repr(raw_model_path)})")
            return
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i/load model: {e}"); return

    # 3. Vocab Size
    try:
        all_item_embeddings = get_item_embedding_matrix(model)
        model_vocab_size = all_item_embeddings.shape[0]
        print(f"‚ÑπÔ∏è  Vocab Size: {model_vocab_size}")
    except Exception as e:
        print(f"‚ùå L·ªói embedding: {e}"); return

    # 4. Load Data
    try:
        df_test = pd.read_parquet(test_path, storage_options=MINIO_CONF)
        print(f"üìä Loaded {len(df_test)} users.")
    except Exception as e:
        print(f"‚ùå L·ªói load data: {e}"); return

    # 5. Eval Loop
    print("\n‚öôÔ∏è  ƒêang ch·∫•m ƒëi·ªÉm...")
    total_hr = 0
    total_ndcg = 0
    valid_users = 0
    debug_printed = False

    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):
        seq = row['sequence_ids']
        if len(seq) < 2: continue
        target = int(seq[-1])

        if target >= model_vocab_size: continue 

        safe_hist = [x if x < model_vocab_size else 0 for x in seq[:-1]]
        p_seq, p_mask = pad_sequence(safe_hist, MAX_LEN)
        
        inputs = {
            "item_ids": tf.expand_dims(p_seq, 0),
            "category_ids": tf.expand_dims(np.zeros_like(p_seq), 0),
            "padding_mask": tf.expand_dims(p_mask, 0)
        }

        try:
            out = model(inputs, training=False)
            if isinstance(out, dict):
                user_vec = out.get("item_sequence_embedding", list(out.values())[0])[0, -1, :]
            else:
                user_vec = out[0, -1, :]

            scores = np.matmul(all_item_embeddings, user_vec)
            scores[0] = -np.inf

            top_idx = np.argpartition(scores, -TOP_K)[-TOP_K:]
            sorted_top = top_idx[np.argsort(scores[top_idx])][::-1]

            hr, ndcg = calculate_metrics(sorted_top, target, TOP_K)
            total_hr += hr
            total_ndcg += ndcg
            valid_users += 1
        except Exception as e:
            if not debug_printed:
                print(f"‚ùå Debug Error: {e}")
                debug_printed = True
            continue

    # 6. Report
    print("\n" + "="*30)
    print(f"‚úÖ Valid Users: {valid_users}/{len(df_test)}")
    if valid_users > 0:
        avg_hr = total_hr / valid_users
        avg_ndcg = total_ndcg / valid_users
        print(f"üèÜ HR@{TOP_K}: {avg_hr:.4f}")
        print(f"üèÜ NDCG@{TOP_K}: {avg_ndcg:.4f}")
        update_config_metrics({"HR@10": avg_hr, "NDCG@10": avg_ndcg}, "PASSED" if avg_hr >= QUALITY_THRESHOLD else "FAILED")
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ k·∫øt qu·∫£.")
        update_config_metrics({"error": "no_valid_users"}, "FAILED")

if __name__ == "__main__":
    main()