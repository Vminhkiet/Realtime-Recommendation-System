# import os
# import json
# import s3fs
# import random
# import pandas as pd
# import tensorflow as tf
# import numpy as np
# from datetime import datetime

# # Import Custom Layer (B·∫Øt bu·ªôc)
# try:
#     from model import SasRec
# except ImportError:
#     from src.ai_core.model import SasRec

# # ================== C·∫§U H√åNH ==================
# MINIO_CONF = {
#     "key": os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
#     "secret": os.getenv("MINIO_SECRET_KEY", "minioadmin"),
#     "client_kwargs": {"endpoint_url": os.getenv("MINIO_ENDPOINT", "http://minio:9000")}
# }

# BUCKET = "datalake"
# META_CONFIG_PATH = f"s3://{BUCKET}/model_registry/model_meta_config.json"
# MODEL_REGISTRY_S3 = f"s3://{BUCKET}/model_registry"
# LOCAL_MODEL_DIR = "/home/spark/work/data/models"  # ƒê∆∞·ªùng d·∫´n trong Docker

# # Hyperparameters
# LEARNING_RATE = 1e-4
# EPOCHS = 5  # Fine-tune th√¨ s·ªë epoch th·∫•p th√¥i
# BATCH_SIZE = 32
# MAX_LEN = 50

# # ================== HELPER FUNCTIONS ==================

# def get_fs():
#     return s3fs.S3FileSystem(**MINIO_CONF)

# def get_latest_config():
#     fs = get_fs()
#     if fs.exists(META_CONFIG_PATH):
#         with fs.open(META_CONFIG_PATH, 'r') as f:
#             return json.load(f)
#     return {}

# def update_meta_config(new_model_s3_path):
#     fs = get_fs()
#     config = get_latest_config()
    
#     # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n model m·ªõi nh·∫•t
#     config["latest_model_path"] = new_model_s3_path
#     config["last_trained_at"] = datetime.now().isoformat()
    
#     with fs.open(META_CONFIG_PATH, 'w') as f:
#         json.dump(config, f, indent=4)
#     print(f"‚úÖ Config updated: Latest Model -> {new_model_s3_path}")

# def load_data_from_s3(s3_path):
#     print(f"üì• Loading Data: {s3_path}")
#     fs = get_fs()
#     # Chuy·ªÉn s3a:// th√†nh s3:// cho s3fs/pandas
#     path = s3_path.replace("s3a://", "s3://")
    
#     # ƒê·ªçc Parquet t·ª´ S3
#     with fs.open(path, 'rb') as f:
#         df = pd.read_parquet(f)
    
#     print(f"üìä Loaded {len(df)} rows.")
#     return df

# def download_model_from_s3(s3_path, local_path):
#     fs = get_fs()
#     print(f"‚¨áÔ∏è Downloading model from {s3_path}...")
#     fs.get(s3_path, local_path, recursive=True)

# def upload_model_to_s3(local_path, s3_path):
#     fs = get_fs()
#     print(f"‚¨ÜÔ∏è Uploading model to {s3_path}...")
#     fs.put(local_path, s3_path, recursive=True)

# # Dataset Generator (Gi·ªØ nguy√™n logic c·ªßa b·∫°n)
# def create_dataset(item_seqs, cat_seqs, vocab_size):
#     # ... (Gi·ªØ nguy√™n code h√†m generator c·ªßa b·∫°n) ...
#     # L∆∞u √Ω: Nh·ªõ import create_dataset logic v√†o ƒë√¢y
#     pass 

# # ================== MAIN JOB ==================
# def main():
#     print("üöÄ STARTING AUTO-TRAIN JOB...")
    
#     # 1. ƒê·ªçc Config ƒë·ªÉ bi·∫øt l·∫•y Data ·ªü ƒë√¢u v√† Model c≈© ·ªü ƒë√¢u
#     config = get_latest_config()
#     data_path = config.get("incremental_path")  # L·∫•y path tu·∫ßn n√†y
#     latest_model_s3 = config.get("latest_model_path") # L·∫•y model tu·∫ßn tr∆∞·ªõc
#     vocab_size = config.get("max_item_idx", 10000) # L·∫•y vocab size t·ª´ ETL

#     if not data_path:
#         print("‚ùå Kh√¥ng t√¨m th·∫•y 'incremental_path' trong config. H·ªßy train.")
#         return

#     # 2. Load Data
#     try:
#         df = load_data_from_s3(data_path)
#         item_seqs = df['sequence_ids'].tolist()
#         cat_seqs = df['category_ids'].tolist()
#     except Exception as e:
#         print(f"‚ö†Ô∏è L·ªói load data: {e}"); return

#     # 3. Chu·∫©n b·ªã Model (Download ho·∫∑c Load Base)
#     os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)
#     local_model_path = os.path.join(LOCAL_MODEL_DIR, "current_model.keras")
    
#     if latest_model_s3:
#         # N·∫øu ƒë√£ c√≥ model tr√™n S3, t·∫£i v·ªÅ ƒë·ªÉ train ti·∫øp
#         download_model_from_s3(latest_model_s3, local_model_path)
#         print("üîÑ Loading Existing Model for Fine-tuning...")
#         model = tf.keras.models.load_model(local_model_path, custom_objects={'SasRec': SasRec}, compile=False)
#     else:
#         print("üÜï Creating NEW Model (First Run)...")
#         # Logic kh·ªüi t·∫°o model m·ªõi n·∫øu ch∆∞a c√≥ (c·∫ßn import class SasRec v√† kh·ªüi t·∫°o)
#         # model = SasRec(...) 
#         # model.build(...)
#         print("‚ö†Ô∏è Ch∆∞a c√≥ model g·ªëc. H√£y train Base Model tr∆∞·ªõc!"); return

#     # 4. Compile & Train
#     # Logic dataset (c·∫ßn copy h√†m create_dataset v√†o)
#     train_ds = create_dataset(item_seqs, cat_seqs, vocab_size) 
    
#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))
#     model.fit(train_ds, epochs=EPOCHS)

#     # 5. L∆∞u & Upload Model M·ªõi
#     # T·∫°o t√™n version theo ng√†y: sasrec_20251229.keras
#     version = datetime.now().strftime("%Y%m%d_%H%M")
#     new_model_name = f"sasrec_{version}.keras"
    
#     local_save_path = os.path.join(LOCAL_MODEL_DIR, new_model_name)
#     model.save(local_save_path)
#     print(f"üíæ Saved local: {local_save_path}")
    
#     s3_save_path = f"{MODEL_REGISTRY_S3}/{new_model_name}"
#     upload_model_to_s3(local_save_path, s3_save_path)
    
#     # 6. C·∫≠p nh·∫≠t Config ƒë·ªÉ tu·∫ßn sau bi·∫øt ƒë∆∞·ªùng d√πng
#     update_meta_config(s3_save_path)
    
#     print("üéâ FINE-TUNING COMPLETE.")

# if __name__ == "__main__":
#     main()


# import os
# import json
# import s3fs
# import random
# import pandas as pd
# import tensorflow as tf
# import numpy as np
# from datetime import datetime
# import mlflow
# import mlflow.tensorflow

# MLFLOW_TRACKING_URI = os.getenv(
#     "MLFLOW_TRACKING_URI",
#     "http://mlflow:5000"
# )

# mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
# mlflow.set_experiment("sasrec_weekly_training")

# # --- IMPORT MODEL CLASS ---
# try:
#     from model import SasRec
# except ImportError:
#     import sys
#     sys.path.append(os.getcwd())
#     from src.ai_core.model import SasRec

# # ================== 1. C·∫§U H√åNH WEEKLY (NH·∫∏ NH√ÄNG) ==================
# MINIO_CONF = {
#     "key": os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
#     "secret": os.getenv("MINIO_SECRET_KEY", "minioadmin"),
#     "client_kwargs": {"endpoint_url": os.getenv("MINIO_ENDPOINT", "http://minio:9000")}
# }

# BUCKET = "datalake"
# META_CONFIG_PATH = f"s3://{BUCKET}/model_registry/model_meta_config.json"
# MODEL_REGISTRY_S3 = f"s3://{BUCKET}/model_registry"
# LOCAL_MODEL_DIR = "/home/spark/work/data/models"

# # --- HYPERPARAMETERS ---
# MAX_LEN = 10       # Kh·ªõp v·ªõi ETL Spark
# BATCH_SIZE = 64
# EPOCHS = 5         # S·ªë epoch √≠t v√¨ ch·ªâ c·∫ßn h·ªçc data tu·∫ßn n√†y
# LEARNING_RATE = 5e-5 # C·ª∞C K·ª≤ QUAN TR·ªåNG: LR nh·ªè ƒë·ªÉ Fine-tune

# # ================== 2. HELPER FUNCTIONS ==================

# def get_fs():
#     return s3fs.S3FileSystem(**MINIO_CONF)

# def get_latest_config():
#     fs = get_fs()
#     if fs.exists(META_CONFIG_PATH):
#         with fs.open(META_CONFIG_PATH, 'r') as f:
#             return json.load(f)
#     return {}

# def update_meta_config(new_model_s3_path, metrics=None):
#     fs = get_fs()
#     config = get_latest_config()
    
#     config["latest_model_path"] = new_model_s3_path
#     config["last_trained_at"] = datetime.now().isoformat()
#     config["train_mode"] = "WEEKLY_INCREMENTAL"

#     if metrics:
#         if "history" not in config: config["history"] = []
#         record = metrics.copy()
#         record["date"] = config["last_trained_at"]
#         record["model"] = new_model_s3_path.split("/")[-1]
#         config["history"].append(record)
#         if len(config["history"]) > 20: config["history"] = config["history"][-20:]

#     with fs.open(META_CONFIG_PATH, 'w') as f:
#         json.dump(config, f, indent=4)
#     print(f"‚úÖ [Config] Updated Latest Model -> {new_model_s3_path}")

# def load_data_from_s3(s3_path):
#     path = s3_path.replace("s3a://", "s3://")
#     print(f"üì• Loading Data: {path}")
#     fs = get_fs()
#     if not fs.exists(path):
#         print(f"‚ö†Ô∏è Path not found: {path}")
#         return pd.DataFrame()
#     return pd.read_parquet(path, storage_options=MINIO_CONF)

# def create_dataset(item_seqs, cat_seqs, vocab_size):
#     """Dataset Generator cho Weekly Data"""
#     def generator():
#         for items, cats in zip(item_seqs, cat_seqs):
#             in_items = items[:-1]
#             target_items = items[1:]
#             in_cats = cats[:-1]
            
#             # Logic Padding (Cho user m·ªõi) & C·∫Øt (Cho user c≈©)
#             curr_len = len(in_items)
#             pad_len = MAX_LEN - curr_len
            
#             if pad_len < 0:
#                 in_items = in_items[-MAX_LEN:]
#                 target_items = target_items[-MAX_LEN:]
#                 in_cats = in_cats[-MAX_LEN:]
#                 pad_len = 0
#                 curr_len = MAX_LEN

#             padded_items = list(in_items) + [0] * pad_len
#             padded_cats = list(in_cats) + [0] * pad_len
#             padded_targets = list(target_items) + [0] * pad_len
#             padding_mask = [True] * curr_len + [False] * pad_len
            
#             # Negative Sampling
#             neg_items = [random.randint(1, vocab_size - 1) for _ in range(MAX_LEN)]

#             yield (
#                 {
#                     "item_ids": np.array(padded_items, dtype=np.int32),
#                     "category_ids": np.array(padded_cats, dtype=np.int32),
#                     "padding_mask": np.array(padding_mask, dtype=np.bool_)
#                 },
#                 {
#                     "positive_sequence": np.array(padded_targets, dtype=np.int32),
#                     "negative_sequence": np.array(neg_items, dtype=np.int32)
#                 }
#             )

#     return tf.data.Dataset.from_generator(
#         generator,
#         output_signature=(
#             {
#                 "item_ids": tf.TensorSpec((MAX_LEN,), tf.int32),
#                 "category_ids": tf.TensorSpec((MAX_LEN,), tf.int32),
#                 "padding_mask": tf.TensorSpec((MAX_LEN,), tf.bool),
#             },
#             {
#                 "positive_sequence": tf.TensorSpec((MAX_LEN,), tf.int32),
#                 "negative_sequence": tf.TensorSpec((MAX_LEN,), tf.int32),
#             }
#         )
#     ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# # ================== 3. MAIN WEEKLY JOB ==================
# def main():
#     print(f"üöÄ STARTING WEEKLY FINE-TUNING (MAX_LEN={MAX_LEN})...")
    
#     # 1. Ki·ªÉm tra Config
#     config = get_latest_config()
#     train_path = config.get("train_path")
#     test_path = config.get("test_path")
#     latest_model_s3 = config.get("latest_model_path")
#     data_max_idx = config.get("max_item_idx", 0)

#     if not latest_model_s3:
#         print("‚ùå CRITICAL: Kh√¥ng t√¨m th·∫•y model g·ªëc (Base Model).")
#         print("üëâ Vui l√≤ng ch·∫°y script 'train_cold_start.py' tr∆∞·ªõc ƒë·ªÉ t·∫°o model n·ªÅn t·∫£ng.")
#         return

#     if not train_path:
#         print("‚ùå Thi·∫øu path data tu·∫ßn n√†y. H·ªßy job.")
#         return

#     # 2. Load Data Tu·∫ßn
#     df_train = load_data_from_s3(train_path)
#     if df_train.empty:
#         print("‚ö†Ô∏è Data tu·∫ßn n√†y r·ªóng. Kh√¥ng c√≥ g√¨ ƒë·ªÉ h·ªçc.")
#         return
#     print(f"üìä New Data Rows: {len(df_train)}")

#     # 3. Load Model C≈©
#     os.makedirs(LOCAL_MODEL_DIR, exist_ok=True)
#     local_model_path = os.path.join(LOCAL_MODEL_DIR, "base_model.keras")
#     fs = get_fs()

#     print(f"üîÑ Downloading Base Model: {latest_model_s3}")
#     try:
#         fs.get(latest_model_s3.replace("s3a://", "s3://"), local_model_path)
#         model = tf.keras.models.load_model(local_model_path, custom_objects={'SasRec': SasRec}, compile=False)
#     except Exception as e:
#         print(f"‚ùå L·ªói load model c≈©: {e}")
#         return

#     # 4. SAFETY CHECK (Quan tr·ªçng nh·∫•t c·ªßa Weekly Train)
#     # Ki·ªÉm tra xem Model c≈© c√≥ ƒë·ªß ch·ªó ch·ª©a Item m·ªõi kh√¥ng
#     current_vocab_capacity = config.get("max_item_idx") + 1
#     print(f"‚ÑπÔ∏è Model Capacity: {current_vocab_capacity} items | Max New Item ID: {data_max_idx}")

#     if data_max_idx >= current_vocab_capacity:
#         print("\n" + "="*50)
#         print("üõë STOPPING JOB: TR√ÄN B·ªò NH·ªö ITEM (VOCAB OVERFLOW)")
#         print(f"   - Model c≈© ch·ªâ h·ªó tr·ª£ ID ƒë·∫øn: {current_vocab_capacity}")
#         print(f"   - Data m·ªõi xu·∫•t hi·ªán ID: {data_max_idx}")
#         print("üëâ GI·∫¢I PH√ÅP: H√£y ch·∫°y l·∫°i 'train_cold_start.py' ƒë·ªÉ Resize l·∫°i Embedding Matrix.")
#         print("="*50 + "\n")
#         return # D·ª´ng lu√¥n, kh√¥ng c·ªë train v√¨ s·∫Ω crash

#     # 5. Prepare Dataset
#     # L∆∞u √Ω: D√πng current_vocab_capacity ƒë·ªÉ l√†m vocab_size cho generator
#     train_ds = create_dataset(df_train['sequence_ids'], df_train['category_ids'], current_vocab_capacity)

#     # 6. Fine-tuning
#     # print(f"üî• Fine-tuning start (LR: {LEARNING_RATE} - Epochs: {EPOCHS})...")
    
#     # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=None, metrics=['accuracy'])
#     # history = model.fit(train_ds, epochs=EPOCHS)

#     # # 7. Evaluate
#     # metrics = {"loss": history.history['loss'][-1], "accuracy": history.history['accuracy'][-1]}

#     import mlflow
#     import mlflow.tensorflow

#     mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000"))
#     mlflow.set_experiment("sasrec_weekly_training")

#     print(f"üî• Fine-tuning start (LR: {LEARNING_RATE} - Epochs: {EPOCHS})...")

#     with mlflow.start_run(run_name=f"weekly_{datetime.now().strftime('%Y%m%d_%H%M')}"):

#         # ===== LOG PARAMS =====
#         mlflow.log_param("max_len", MAX_LEN)
#         mlflow.log_param("batch_size", BATCH_SIZE)
#         mlflow.log_param("epochs", EPOCHS)
#         mlflow.log_param("learning_rate", LEARNING_RATE)
#         mlflow.log_param("train_mode", "weekly_incremental")

#         model.compile(
#             optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
#         )

#         history = model.fit(train_ds, epochs=EPOCHS)

#         # ===== TRAIN METRICS =====
#         train_loss = history.history["loss"][-1]

#         mlflow.log_metric("train_loss", train_loss)

#         metrics = {
#             "loss": train_loss
#         }

#         # ===== EVALUATE =====
#         if test_path:
#             try:
#                 df_test = load_data_from_s3(test_path)
#                 if not df_test.empty:
#                     test_ds = create_dataset(
#                         df_test['sequence_ids'],
#                         df_test['category_ids'],
#                         current_vocab_capacity
#                     )
#                     eval_res = model.evaluate(test_ds, return_dict=True)
#                     for k, v in eval_res.items():
#                         mlflow.log_metric(f"test_{k}", v)
#                     metrics.update(eval_res)
#             except Exception as e:
#                 print(f"‚ö†Ô∏è Eval skipped: {e}")

#         # ===== LOG MODEL TO MLFLOW =====
#         timestamp = datetime.now().strftime("%Y%m%d_%H%M")
#         MODEL_DIR = f"/tmp/sasrec_weekly_{timestamp}.keras"
#         os.makedirs(os.path.dirname(MODEL_DIR), exist_ok=True)

#         model.save(MODEL_DIR)
#         mlflow.log_artifacts(MODEL_DIR, artifact_path="model")



#     if test_path:
#         try:
#             df_test = load_data_from_s3(test_path)
#             if not df_test.empty:
#                 test_ds = create_dataset(df_test['sequence_ids'], df_test['category_ids'], current_vocab_capacity)
#                 eval_res = model.evaluate(test_ds, return_dict=True)
#                 metrics.update(eval_res)
#                 print(f"üìä Test Metrics: {metrics}")
#         except: pass

#     # 8. Save Version M·ªõi
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M")
#     new_model_name = f"sasrec_weekly_{timestamp}.keras"
#     local_save_path = os.path.join(LOCAL_MODEL_DIR, new_model_name)
    
#     print(f"üíæ Saving Version: {new_model_name}")
#     model.save(local_save_path)
    
#     s3_save_path = f"s3://{BUCKET}/model_registry/{new_model_name}"
#     print(f"‚¨ÜÔ∏è Uploading to S3...")
#     fs.put(local_save_path, s3_save_path)

#     # 9. Update Config
#     update_meta_config(s3_save_path, metrics)
#     print("üéâ WEEKLY UPDATE COMPLETE.")

# if __name__ == "__main__":
#     main()
# import os
# import sys
# import json
# import random
# import pandas as pd
# import tensorflow as tf
# import numpy as np
# import mlflow
# import mlflow.tensorflow
# import boto3
# from collections import Counter
# from datetime import datetime

# # ==========================================
# # 1. SETUP CREDENTIALS
# # ==========================================
# os.environ["AWS_ACCESS_KEY_ID"] = "minioadmin"
# os.environ["AWS_SECRET_ACCESS_KEY"] = "minioadmin"
# os.environ["AWS_DEFAULT_REGION"] = "us-east-1"
# MINIO_URL = os.getenv("MINIO_ENDPOINT", "http://minio:9000")
# os.environ["MLFLOW_S3_ENDPOINT_URL"] = MINIO_URL
# os.environ["AWS_ENDPOINT_URL"] = MINIO_URL
# os.environ["AWS_S3_ENDPOINT_URL"] = MINIO_URL
# os.environ["MLFLOW_TRACKING_URI"] = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")

# # ==========================================
# # 2. IMPORTS & CONFIG
# # ==========================================
# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# try:
#     from model import SasRec
# except ImportError:
#     try:
#         from src.ai_core.model import SasRec
#     except ImportError:
#         print("‚ùå CRITICAL: Cannot import SasRec model class.")
#         sys.exit(1)

# BUCKET_NAME = "datalake"
# META_CONFIG_KEY = "model_registry/model_meta_config.json"
# MAP_PATH = f"s3a://{BUCKET_NAME}/model_registry/item_map.json"

# # Local temp paths
# LOCAL_MODEL_PATH = "/tmp/sasrec_daily.keras"
# LOCAL_PREV_MODEL = "data/model_registry/sasrec_v1.keras"

# # Hyperparameters
# MAX_USERS = 20000
# MIN_INTERACTIONS = 5
# MAX_LEN = 50
# EMBED_DIM = 64
# NUM_BLOCKS = 2
# NUM_HEADS = 4
# DROPOUT_RATE = 0.1
# BATCH_SIZE = 64
# EPOCHS = 5      
# LEARNING_RATE = 5e-5

# # ==========================================
# # 3. HELPER FUNCTIONS (MINIO)
# # ==========================================
# def get_s3_client():
#     return boto3.client("s3", endpoint_url=MINIO_URL,
#                         aws_access_key_id="minioadmin",
#                         aws_secret_access_key="minioadmin")

# def parse_s3_path(s3_path):
#     """Chuy·ªÉn s3a://bucket/key th√†nh (bucket, key)"""
#     clean_path = s3_path.replace("s3a://", "").replace("s3://", "")
#     parts = clean_path.split("/", 1)
#     return parts[0], parts[1]

# def get_meta_config():
#     """ƒê·ªçc file config t·ª´ MinIO"""
#     s3 = get_s3_client()
#     try:
#         obj = s3.get_object(Bucket=BUCKET_NAME, Key=META_CONFIG_KEY)
#         return json.loads(obj['Body'].read().decode('utf-8'))
#     except Exception as e:
#         print(f"‚ö†Ô∏è Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c Config ({e}). S·∫Ω d√πng tham s·ªë m·∫∑c ƒë·ªãnh.")
#         return {}

# def update_meta_config(new_model_path):
#     """C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n model m·ªõi v√†o Config tr√™n MinIO"""
#     s3 = get_s3_client()
#     try:
#         # 1. ƒê·ªçc l·∫°i config m·ªõi nh·∫•t
#         current_config = get_meta_config()
        
#         # 2. Update tr∆∞·ªùng latest_model_path
#         current_config["latest_model_path"] = new_model_path
#         current_config["last_trained_at"] = datetime.now().isoformat()
        
#         # 3. Ghi ƒë√® l·∫°i l√™n MinIO
#         s3.put_object(
#             Bucket=BUCKET_NAME, 
#             Key=META_CONFIG_KEY, 
#             Body=json.dumps(current_config, indent=4),
#             ContentType='application/json'
#         )
#         print(f"‚úÖ [METADATA] ƒê√£ c·∫≠p nh·∫≠t latest_model_path: {new_model_path}")
#     except Exception as e:
#         print(f"‚ùå [METADATA] L·ªói c·∫≠p nh·∫≠t Config: {e}")

# # ==========================================
# # 4. DATA LOADING (Updated to use Config Paths)
# # ==========================================
# def load_data(train_path_s3):
#     print(f"üì• Loading Training Data from: {train_path_s3}")
    
#     # Pandas read_parquet h·ªó tr·ª£ s3 n·∫øu c√†i s3fs, nh∆∞ng ta d√πng boto3 t·∫£i v·ªÅ cho ch·∫Øc
#     local_parquet = "/tmp/train_data.parquet"
#     bucket, key = parse_s3_path(train_path_s3)
    
#     s3 = get_s3_client()
#     # Logic download folder parquet (Spark l∆∞u folder)
#     # ƒê·ªÉ ƒë∆°n gi·∫£n, gi·∫£ s·ª≠ pandas ƒë·ªçc th·∫≥ng qua s3 storage options
#     try:
#         df = pd.read_parquet(
#             train_path_s3.replace("s3a://", "s3://"),
#             storage_options={
#                 "key": "minioadmin", "secret": "minioadmin", 
#                 "client_kwargs": {"endpoint_url": MINIO_URL}
#             }
#         )
#     except:
#         print("‚ö†Ô∏è L·ªói ƒë·ªçc S3 tr·ª±c ti·∫øp, ki·ªÉm tra l·∫°i path.")
#         return [], [], [], 50000, 100

#     # X·ª≠ l√Ω Timestamp
#     time_col = 'last_ts' if 'last_ts' in df.columns else 'last_timestamp'
#     df['last_timestamp'] = df[time_col].apply(lambda x: x/1000 if x > 32503680000 else x)

#     # L·∫•y d·ªØ li·ªáu
#     item_seqs = df['sequence_ids'].tolist()
#     cat_seqs = df['category_ids'].tolist()
    
#     # Load Vocab Size t·ª´ Map
#     try:
#         map_bucket, map_key = parse_s3_path(MAP_PATH)
#         obj = s3.get_object(Bucket=map_bucket, Key=map_key)
#         vocab_size = len(json.loads(obj['Body'].read().decode('utf-8')))
#     except:
#         vocab_size = 106000 # Fallback

#     return item_seqs, cat_seqs, vocab_size, 100

# # ==========================================
# # 5. DATASET GENERATOR
# # ==========================================
# def create_dataset(item_seqs, cat_seqs, max_len, num_items, is_training=True):
#     def generator():
#         data = list(zip(item_seqs, cat_seqs))
#         if is_training: random.shuffle(data)
#         for item_seq, cat_seq in data:
#             # Code c≈© c·ªßa b·∫°n...
#             # ƒê∆°n gi·∫£n h√≥a cho ng·∫Øn g·ªçn (Logic sliding window gi·ªØ nguy√™n)
#             # Gi·∫£ s·ª≠ seq ƒë√£ ƒë∆∞·ª£c c·∫Øt s·∫µn t·ª´ ETL (ETL b·∫°n l√†m kh√° k·ªπ r·ªìi)
            
#             # Pad sequence
#             seq_len = len(item_seq)
#             pad_len = max_len - seq_len
            
#             if pad_len > 0:
#                 input_ids = list(item_seq) + [0] * pad_len
#                 cat_ids = list(cat_seq) + [0] * pad_len
#                 mask = [True] * seq_len + [False] * pad_len
#             else:
#                 input_ids = list(item_seq)[-max_len:]
#                 cat_ids = list(cat_seq)[-max_len:]
#                 mask = [True] * max_len
            
#             # Negative Sampling ƒë∆°n gi·∫£n
#             pos_ids = input_ids[1:] + [0] # Shift left
#             neg_ids = [random.randint(1, num_items) for _ in range(max_len)]
            
#             yield (
#                 {"item_ids": input_ids, "category_ids": cat_ids, "padding_mask": mask},
#                 {"positive_sequence": pos_ids, "negative_sequence": neg_ids}
#             )

#     return tf.data.Dataset.from_generator(
#         generator,
#         output_signature=(
#             {"item_ids": tf.TensorSpec((max_len,), tf.int32), "category_ids": tf.TensorSpec((max_len,), tf.int32), "padding_mask": tf.TensorSpec((max_len,), tf.bool)},
#             {"positive_sequence": tf.TensorSpec((max_len,), tf.int32), "negative_sequence": tf.TensorSpec((max_len,), tf.int32)}
#         )
#     ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# # ==========================================
# # 6. MAIN PIPELINE
# # ==========================================
# def main():
#     print(f"üöÄ DAILY TRAIN STARTED | URI: {os.environ['MLFLOW_TRACKING_URI']}")
    
#     # 1. ƒê·ªåC CONFIG T·ª™ MINIO
#     config = get_meta_config()
#     if not config:
#         print("‚ùå CRITICAL: Kh√¥ng l·∫•y ƒë∆∞·ª£c Config. D·ª´ng.")
#         return

#     # L·∫•y c√°c ƒë∆∞·ªùng d·∫´n quan tr·ªçng
#     latest_model_s3 = config.get("latest_model_path")
#     train_data_s3 = config.get("train_path") # L·∫•y path data m·ªõi nh·∫•t t·ª´ ETL h√¥m nay
    
#     print(f"üìã Config Loaded:")
#     print(f"   - Train Data: {train_data_s3}")
#     print(f"   - Prev Model: {latest_model_s3}")

#     # 2. LOAD DATA
#     item_seqs, cat_seqs, vocab_size, num_categories = load_data(train_data_s3)
    
#     # Popular items (Dummy logic cho nhanh)
#     popular_items = [i for i in range(1, 1000)] 

#     # Split 90/10
#     val_size = max(1, int(len(item_seqs) * 0.1))
#     train_ds = create_dataset(item_seqs[val_size:], cat_seqs[val_size:], MAX_LEN, vocab_size, is_training=True)
#     val_ds = create_dataset(item_seqs[:val_size], cat_seqs[:val_size], MAX_LEN, vocab_size, is_training=False)

#     # 3. MLFLOW RUN
#     mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
#     mlflow.set_experiment("sasrec_daily_update")
    
#     with mlflow.start_run() as run:
#         # Build Model
#         model = SasRec(
#             vocabulary_size=vocab_size + 1,
#             category_size=num_categories + 1,
#             num_layers=NUM_BLOCKS, num_heads=NUM_HEADS,
#             hidden_dim=EMBED_DIM, dropout=DROPOUT_RATE,
#             max_sequence_length=MAX_LEN
#         )
#         # Dummy call to build variables
#         model({"item_ids": tf.zeros((1, MAX_LEN), dtype=tf.int32), "category_ids": tf.zeros((1, MAX_LEN), dtype=tf.int32), "padding_mask": tf.zeros((1, MAX_LEN), dtype=tf.bool)})

#         # ---------------------------------------------------------
#         # üî• LOAD MODEL C≈® T·ª™ MINIO (INCREMENTAL)
#         # ---------------------------------------------------------
#         if latest_model_s3 and "sasrec" in latest_model_s3:
#             print(f"‚¨áÔ∏è Downloading previous model from: {latest_model_s3}")
#             try:
#                 print(latest_model_s3)
#                 bucket, key = parse_s3_path(latest_model_s3)
#                 print(latest_model_s3)
#                 s3 = get_s3_client()
#                 s3.download_file(bucket, key, LOCAL_PREV_MODEL)
#                 print("11")
#                 model.load_weights(LOCAL_PREV_MODEL)
#                 print("‚úÖ [INCREMENTAL] Loaded weights successfully.")
#             except Exception as e:
#                 print(f"‚ö†Ô∏è Failed to load previous model ({e}). Training from scratch.")
#         else:
#             print("‚ö†Ô∏è No previous model path in config. Training from scratch.")

#         # ---------------------------------------------------------
#         # TRAIN
#         # ---------------------------------------------------------
#         model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE))
        
#         print(f"üî• Fine-tuning for {EPOCHS} epochs...")
#         model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)

#         # ---------------------------------------------------------
#         # SAVE & UPDATE CONFIG
#         # ---------------------------------------------------------
#         print("üíæ Saving model locally...")
#         model.save(LOCAL_MODEL_PATH)
        
#         print("‚¨ÜÔ∏è Uploading to MLflow...")
#         mlflow.log_artifact(LOCAL_MODEL_PATH, artifact_path="model_keras")
        
#         # L·∫•y S3 URI t·ª´ MLflow Run hi·ªán t·∫°i
#         # MLflow th∆∞·ªùng l∆∞u d·∫°ng: s3://mlflow/run_id/artifacts/model_keras/sasrec_daily.keras
#         artifact_uri = mlflow.get_artifact_uri("model_keras/sasrec_daily.keras")
        
#         # Chuy·ªÉn ƒë·ªïi sang s3a:// ƒë·ªÉ Spark d√πng ƒë∆∞·ª£c (n·∫øu c·∫ßn) ho·∫∑c gi·ªØ s3://
#         # Th·ªëng nh·∫•t d√πng s3a:// cho h·ªá sinh th√°i Hadoop/Spark
#         new_s3_path = artifact_uri.replace("s3://", "s3a://")
        
#         print(f"üîó New Model Path: {new_s3_path}")
        
#         # C·∫¨P NH·∫¨T JSON CONFIG TR√äN MINIO
#         update_meta_config(new_s3_path)

#     print("üéâ DAILY UPDATE COMPLETE.")

# if __name__ == "__main__":
#     main()
import os
import sys
import json
import random
import pickle
import pandas as pd
import tensorflow as tf
import numpy as np
import mlflow
import mlflow.tensorflow
import boto3
from collections import Counter
from datetime import datetime

# ==========================================
# 1. SETUP CREDENTIALS & ENV
# ==========================================
os.environ["AWS_ACCESS_KEY_ID"] = "minioadmin"
os.environ["AWS_SECRET_ACCESS_KEY"] = "minioadmin"
os.environ["AWS_DEFAULT_REGION"] = "us-east-1"
MINIO_URL = os.getenv("MINIO_ENDPOINT", "http://minio:9000")

# C·∫•u h√¨nh MLflow & S3
os.environ["MLFLOW_S3_ENDPOINT_URL"] = MINIO_URL
os.environ["AWS_ENDPOINT_URL"] = MINIO_URL
os.environ["AWS_S3_ENDPOINT_URL"] = MINIO_URL
os.environ["MLFLOW_TRACKING_URI"] = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")

# B·∫≠t Mixed Precision (T√πy ch·ªçn)
try:
    if tf.config.list_physical_devices('GPU'):
        tf.keras.mixed_precision.set_global_policy('mixed_float16')
        print("‚ö° Mixed Precision Enabled (Float16).")
except: pass

# ==========================================
# 2. IMPORTS & CONFIG
# ==========================================
# Import SasRec Model
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    from model import SasRec
except ImportError:
    try:
        from src.ai_core.model import SasRec
    except ImportError:
        print("‚ùå CRITICAL: Cannot import SasRec model class.")
        sys.exit(1)

# Paths & Constants
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))

# Input Paths
PARQUET_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/processed_parquet')
MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')
CAT_MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/category_map.json')

# Output Paths
MODEL_SAVE_PATH = "data/model_registry/sasrec_v1.keras"
TEST_SET_SAVE_PATH = "data/model_registry/test_set.pkl"
LOG_PATH = "data/model_registry/training_history.json"
VALID_USERS_PATH = os.path.join(PROJECT_ROOT, 'src/simulation/valid_users.json')

# Hyperparameters (Fine-tuning settings)
MAX_USERS = 20000       # L·∫•y 20k user t·ªët nh·∫•t ƒë·ªÉ fine-tune
MAX_LEN = 50
BATCH_SIZE = 64
EPOCHS = 5              # S·ªë epoch th·∫•p cho retrain
STEPS_PER_EPOCH = 200   
STEP_SIZE = 10          
FINE_TUNE_LR = 0.0001   # Learning rate nh·ªè h∆°n b√¨nh th∆∞·ªùng (1e-4)

# ==========================================
# 3. UTILS
# ==========================================
def ensure_bucket_exists(bucket_name="mlflow"):
    s3 = boto3.client("s3", endpoint_url=MINIO_URL)
    try: s3.head_bucket(Bucket=bucket_name)
    except: 
        try: s3.create_bucket(Bucket=bucket_name)
        except: pass

def normalize_ts(ts):
    return ts / 1000 if ts > 32503680000 else ts

def get_popular_items(sequences, top_k=2000):
    print("üî• ƒêang t√≠nh Popular Items (ƒë·ªÉ l√†m Hard Negatives)...")
    all_items = [i for seq in sequences for i in seq]
    counter = Counter(all_items)
    if 0 in counter: del counter[0]
    return [item for item, _ in counter.most_common(top_k)]

# ==========================================
# 4. LOAD DATA
# ==========================================
def load_data():
    print("üì• ƒêang load d·ªØ li·ªáu Parquet...")
    if not os.path.exists(PARQUET_PATH):
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {PARQUET_PATH}")

    df = pd.read_parquet(PARQUET_PATH)
    
    # X·ª≠ l√Ω Timestamp
    if 'last_timestamp' in df.columns:
        TIME_COL = 'last_timestamp'
    elif 'sequence_timestamps' in df.columns:
        df['last_timestamp'] = df['sequence_timestamps'].apply(lambda x: x[-1] if len(x) > 0 else 0)
        TIME_COL = 'last_timestamp'
    else:
        df['last_timestamp'] = 0
        TIME_COL = 'last_timestamp'

    df[TIME_COL] = df[TIME_COL].apply(normalize_ts)
    if 'user_id' not in df.columns: df['user_id'] = df.index.astype(str)

    # L·ªçc User t·ªët nh·∫•t
    print(f"üîç S√†ng l·ªçc Top {MAX_USERS} Users c√≥ l·ªãch s·ª≠ d√†y nh·∫•t...")
    df['seq_len'] = df['sequence_ids'].apply(len)
    df_sorted = df.sort_values(by='seq_len', ascending=False).head(MAX_USERS)
    df_final = df_sorted.sample(frac=1).reset_index(drop=True)

    print(f"‚úÖ ƒê√£ ch·ªçn: {len(df_final)} users.")

    item_seqs = df_final['sequence_ids'].tolist()
    cat_seqs = df_final['category_ids'].tolist()
    last_times = df_final[TIME_COL].tolist()
    valid_user_ids = df_final['user_id'].tolist()

    # Save Metadata
    os.makedirs(os.path.dirname(VALID_USERS_PATH), exist_ok=True)
    with open(VALID_USERS_PATH, 'w') as f:
        json.dump(valid_user_ids, f)

    with open(MAP_PATH, 'r') as f:
        vocab_size = len(json.load(f))
    
    if os.path.exists(CAT_MAP_PATH):
        with open(CAT_MAP_PATH, 'r') as f:
            num_categories = len(json.load(f))
    else:
        num_categories = 100

    return item_seqs, cat_seqs, last_times, vocab_size, num_categories

# ==========================================
# 5. DATASET GENERATOR
# ==========================================
def create_dataset(item_seqs, cat_seqs, max_len, num_items, popular_items, is_training=True):
    def generator():
        data = list(zip(item_seqs, cat_seqs))
        if is_training: random.shuffle(data)
        
        for item_seq, cat_seq in data:
            item_seq = list(item_seq)
            cat_seq = list(cat_seq)
            seq_len = len(item_seq)

            if is_training:
                if seq_len <= max_len + 1: starts = [0]
                else: starts = range(0, seq_len - max_len, STEP_SIZE)
            else:
                starts = [max(0, seq_len - max_len - 1)]

            for i in starts:
                end = min(i + max_len + 1, seq_len)
                item_win = item_seq[i:end]
                cat_win = cat_seq[i:end]
                
                if len(item_win) < 2: continue

                curr_item = item_win[:-1]
                curr_pos = item_win[1:]
                curr_cat = cat_win[:-1]

                # C·∫Øt ƒëu√¥i
                curr_item = curr_item[-max_len:]
                curr_pos = curr_pos[-max_len:]
                curr_cat = curr_cat[-max_len:]

                # Padding
                input_ids = list(curr_item)
                pos_ids = list(curr_pos)
                cat_ids = list(curr_cat)
                pad_len = max_len - len(input_ids)

                input_ids = input_ids + [0] * pad_len
                pos_ids = pos_ids + [0] * pad_len
                cat_ids = cat_ids + [0] * pad_len
                mask = [True] * len(curr_item) + [False] * pad_len

                # Negative Sampling
                win_set = set(item_win)
                neg_ids = []
                for _ in range(len(curr_item)):
                    if is_training and random.random() < 0.3 and popular_items:
                        neg = random.choice(popular_items)
                    else:
                        neg = random.randint(1, num_items)
                    while neg in win_set:
                        neg = random.randint(1, num_items)
                    neg_ids.append(neg)
                
                neg_ids += [0] * pad_len

                yield (
                    {"item_ids": input_ids, "category_ids": cat_ids, "padding_mask": mask},
                    {"positive_sequence": pos_ids, "negative_sequence": neg_ids}
                )

    ds = tf.data.Dataset.from_generator(
        generator,
        output_signature=(
            {"item_ids": tf.TensorSpec((max_len,), tf.int32), "category_ids": tf.TensorSpec((max_len,), tf.int32), "padding_mask": tf.TensorSpec((max_len,), tf.bool)},
            {"positive_sequence": tf.TensorSpec((max_len,), tf.int32), "negative_sequence": tf.TensorSpec((max_len,), tf.int32)}
        )
    )
    
    if is_training:
        ds = ds.repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    else:
        ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    
    return ds

# ==========================================
# 6. MAIN FUNCTION (STRICT RETRAIN ONLY)
# ==========================================
def main():
    print("üöÄ STRICT RETRAINING PIPELINE STARTED")
    ensure_bucket_exists("mlflow")

    # 1. SETUP MLFLOW
    mlflow.set_tracking_uri(os.environ["MLFLOW_TRACKING_URI"])
    mlflow.set_experiment("sasrec_strict_retrain")
    mlflow.tensorflow.autolog(log_models=True, log_datasets=False)

    with mlflow.start_run() as run:
        
        # 2. CHECK MODEL EXISTENCE (CRITICAL STEP)
        print(f"üîç Checking for existing model at: {MODEL_SAVE_PATH}")
        if not os.path.exists(MODEL_SAVE_PATH):
            print(f"‚õî STOPPING: Kh√¥ng t√¨m th·∫•y file model c≈© t·∫°i {MODEL_SAVE_PATH}")
            print("‚ö†Ô∏è Config: 'Never create new'. Ch∆∞∆°ng tr√¨nh s·∫Ω k·∫øt th√∫c ngay l·∫≠p t·ª©c.")
            sys.exit(0) # <--- D·ª™NG CH∆Ø∆†NG TR√åNH

        # 3. LOAD DATA (Ch·ªâ load khi bi·∫øt ch·∫Øc ch·∫Øn c√≥ model)
        item_seqs, cat_seqs, last_times, vocab_size, num_categories = load_data()
        popular_items = get_popular_items(item_seqs)

        # 4. SPLIT DATA
        train_items, train_cats = [], []
        val_items, val_cats = [], []
        test_set = []

        print("‚úÇÔ∏è Splitting Data...")
        for seq, cat, ts in zip(item_seqs, cat_seqs, last_times):
            if len(seq) < 3: continue 
            test_set.append({"input_items": seq[:-1], "input_cats": cat[:-1], "label": seq[-1], "test_time": ts})
            val_items.append(seq[:-1])
            val_cats.append(cat[:-1])
            train_items.append(seq[:-2])
            train_cats.append(cat[:-2])

        # 5. CREATE DATASETS
        train_ds = create_dataset(train_items, train_cats, MAX_LEN, vocab_size, popular_items, is_training=True)
        val_ds = create_dataset(val_items, val_cats, MAX_LEN, vocab_size, popular_items, is_training=False)

        # 6. LOAD MODEL (WARM START)
        try:
            print("üîÑ Found model. Attempting to load for Fine-tuning...")
            # Load v·ªõi compile=False ƒë·ªÉ ta t·ª± set l·∫°i Learning Rate
            model = tf.keras.models.load_model(
                MODEL_SAVE_PATH, 
                custom_objects={'SasRec': SasRec},
                compile=False 
            )
            print("‚úÖ Model loaded successfully! Ready for fine-tuning.")
        except Exception as e:
            print(f"‚ùå CRITICAL ERROR: File model t·ªìn t·∫°i nh∆∞ng b·ªã l·ªói.")
            print(f"   Details: {e}")
            sys.exit(1) # <--- D·ª™NG CH∆Ø∆†NG TR√åNH (B√°o l·ªói)

        # 7. COMPILE (FINE-TUNE LR)
        print(f"‚öôÔ∏è Compiling with Fine-tuning LR: {FINE_TUNE_LR}")
        
        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(
            initial_learning_rate=FINE_TUNE_LR,
            first_decay_steps=STEPS_PER_EPOCH * 5,
            t_mul=2.0, m_mul=0.9, alpha=1e-6
        )
        
        model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=0.01))

        # Log Params
        mlflow.log_params({
            "mode": "strict_retrain",
            "base_model": MODEL_SAVE_PATH,
            "learning_rate": FINE_TUNE_LR
        })

        # 8. START TRAINING
        print(f"üî• B·∫Øt ƒë·∫ßu Retrain (Fine-tune)...")
        history = model.fit(
            train_ds,
            validation_data=val_ds,
            epochs=EPOCHS,
            steps_per_epoch=STEPS_PER_EPOCH,
            validation_steps=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
                # L∆∞u ƒë√® l√™n file c≈© ƒë·ªÉ c·∫≠p nh·∫≠t model
                tf.keras.callbacks.ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor='val_loss')
            ]
        )

        # 9. SAVE & UPLOAD
        print("üíæ Saving artifacts...")
        with open(TEST_SET_SAVE_PATH, 'wb') as f:
            pickle.dump(test_set, f)
        
        # ModelCheckpoint ƒë√£ l∆∞u b·∫£n t·ªët nh·∫•t, nh∆∞ng ta l∆∞u th√™m l·∫ßn cu·ªëi cho ch·∫Øc
        model.save(MODEL_SAVE_PATH)

        try:
            # Upload model m·ªõi l√™n MLflow
            mlflow.log_artifact(TEST_SET_SAVE_PATH, artifact_path="data_splits")
            mlflow.log_artifact(MODEL_SAVE_PATH, artifact_path="model_keras")
            
            if os.path.exists(LOG_PATH):
                with open(LOG_PATH, 'w') as f: json.dump(history.history, f)
                mlflow.log_artifact(LOG_PATH, artifact_path="logs")
            
            print("‚úÖ Strict Retrain Completed & Uploaded!")
        except Exception as e:
            print(f"‚ö†Ô∏è Upload Failed: {e}")

if __name__ == "__main__":
    main()