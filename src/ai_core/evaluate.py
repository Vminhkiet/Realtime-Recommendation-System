import os
import json
import pickle
import numpy as np
import tensorflow as tf
from tqdm import tqdm

try:
    from .model import SasRec
except ImportError:
    from model import SasRec

# --- C·∫§U H√åNH ---
MAX_LEN = 50 
NUM_NEG_TEST = 99 # 1 m√≥n ƒë√∫ng + 99 m√≥n sai = 100 m√≥n ƒë·ªÉ x·∫øp h·∫°ng

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))
MODEL_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/sasrec_v1.keras')
TEST_SET_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/test_set.pkl')
MAP_PATH = os.path.join(PROJECT_ROOT, 'data/model_registry/item_map.json')

def main():
    print("üìä ƒê√ÅNH GI√Å V·ªöI CHI·∫æN THU·∫¨T SAMPLED METRICS (1 vs 99)...")
    
    # 1. Load Resources
    model = tf.keras.models.load_model(MODEL_PATH)
    with open(TEST_SET_PATH, 'rb') as f:
        test_set = pickle.load(f)
    with open(MAP_PATH, 'r') as f:
        item_map = json.load(f)
        vocab_size = len(item_map)

    hits_10 = 0
    ndcgs_10 = 0
    
    # 2. ƒê√°nh gi√° t·ª´ng User (Sampled evaluation kh√¥ng d√πng batch predict ƒë∆∞·ª£c d·ªÖ d√†ng)
    print(f"üöÄ ƒêang ch·∫•m ƒëi·ªÉm cho {len(test_set)} users...")
    
    for sample in tqdm(test_set):
        seq = sample['input_items'][-MAX_LEN:]
        cat = sample['input_cats'][-MAX_LEN:]
        target = sample['label']
        
        # T·∫°o danh s√°ch 100 m√≥n c·∫ßn x·∫øp h·∫°ng
        test_items = [target]
        while len(test_items) < 100:
            neg = np.random.randint(1, vocab_size + 1)
            if neg != target:
                test_items.append(neg)
        
        # Chu·∫©n b·ªã Input cho Model
        pad_len = MAX_LEN - len(seq)
        in_item = np.array([list(seq) + [0] * pad_len])
        in_cat = np.array([list(cat) + [0] * pad_len])
        in_mask = np.array([[True] * len(seq) + [False] * pad_len])
        
        # L·∫•y Embedding c·ªßa chu·ªói h√†nh vi t·ª´ Model
        outputs = model.predict({
            "item_ids": in_item,
            "category_ids": in_cat,
            "padding_mask": in_mask
        }, verbose=0)
        
        # Thay v√¨ l·∫•y Top 10 to√†n c·ª•c, ta ch·ªâ l·∫•y ƒëi·ªÉm c·ªßa 100 m√≥n n√†y
        # L·∫•y vector ƒë·∫∑c tr∆∞ng cu·ªëi c√πng c·ªßa sequence
        seq_emb = outputs['item_sequence_embedding'][0] # [Seq, Dim]
        # L·∫•y embedding t·∫°i v·ªã tr√≠ cu·ªëi c√πng c√≥ data
        last_idx = len(seq) - 1
        last_emb = seq_emb[last_idx] # [Dim]
        
        # L·∫•y tr·ªçng s·ªë Embedding c·ªßa 100 m√≥n test
        all_item_weights = model.item_embedding.embeddings # [Vocab, Dim]
        test_items_idx = np.array(test_items)
        # L·∫•y embedding c·ªßa 100 m√≥n
        test_embs = tf.gather(all_item_weights, test_items_idx) # [100, Dim]
        
        # T√≠nh ƒëi·ªÉm Score = Dot Product
        scores = tf.matmul(test_embs, tf.expand_dims(last_emb, -1))
        scores = tf.squeeze(scores).numpy() # [100]
        
        # X·∫øp h·∫°ng: M√≥n ƒë√∫ng (index 0) ƒë·ª©ng th·ª© m·∫•y trong 100 m√≥n?
        # ƒêi·ªÉm c√†ng cao h·∫°ng c√†ng nh·ªè (h·∫°ng 0 l√† cao nh·∫•t)
        rank = (scores > scores[0]).sum()
        
        if rank < 10:
            hits_10 += 1
            ndcgs_10 += 1 / np.log2(rank + 2)

    avg_hr = hits_10 / len(test_set)
    avg_ndcg = ndcgs_10 / len(test_set)
    
    print("\n-------------------------------------------------")
    print(f"üèÜ K·∫æT QU·∫¢ SAMPLED (Hit@10 trong 100 m√≥n):")
    print(f"   ‚úÖ Hit Rate @ 10: {avg_hr:.4f} ({avg_hr*100:.2f}%)")
    print(f"   ‚≠ê NDCG @ 10    : {avg_ndcg:.4f}")
    print("-------------------------------------------------")

if __name__ == "__main__":
    main()